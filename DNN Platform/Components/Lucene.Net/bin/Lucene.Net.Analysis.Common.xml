<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Lucene.Net.Analysis.Common</name>
    </assembly>
    <members>
        <member name="T:Lucene.Net.Analysis.Ar.ArabicAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Arabic. 
            <para/>
            This analyzer implements light-stemming as specified by:
            <c>
            Light Stemming for Arabic Information Retrieval
            </c>    
            http://www.mtholyoke.edu/~lballest/Pubs/arab_stem05.pdf
            <para/>
            The analysis package contains three primary components:
            <list type="bullet">
                <item><description><see cref="T:Lucene.Net.Analysis.Ar.ArabicNormalizationFilter"/>: Arabic orthographic normalization.</description></item>
                <item><description><see cref="T:Lucene.Net.Analysis.Ar.ArabicStemFilter"/>: Arabic light stemming</description></item>
                <item><description>Arabic stop words file: a set of default Arabic stop words.</description></item>
            </list>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Ar.ArabicAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Arabic stopwords.
            
            Default stopword list is from http://members.unine.ch/jacques.savoy/clef/index.html
            The stopword list is BSD-Licensed.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Ar.ArabicAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop-words set. </summary>
            <returns> an unmodifiable instance of the default stop-words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Ar.ArabicAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the DEFAULT_STOP_SET in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ar.ArabicAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.Ar.ArabicAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ar.ArabicAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words
            </summary>
            <param name="matchVersion">
                     lucene compatibility version </param>
            <param name="stopwords">
                     a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Ar.ArabicAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop word. If a none-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            <see cref="T:Lucene.Net.Analysis.Ar.ArabicStemFilter"/>.
            </summary>
            <param name="matchVersion">
                     lucene compatibility version </param>
            <param name="stopwords">
                     a stopword set </param>
            <param name="stemExclusionSet">
                     a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Ar.ArabicAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            used to tokenize all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Ar.ArabicNormalizationFilter"/>, <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/>
                    if a stem exclusion set is provided and <see cref="T:Lucene.Net.Analysis.Ar.ArabicStemFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Ar.ArabicLetterTokenizer">
            <summary>
            Tokenizer that breaks text into runs of letters and diacritics.
            <para>
            The problem with the standard Letter tokenizer is that it fails on diacritics.
            Handling similar to this is necessary for Indic Scripts, Hebrew, Thaana, etc.
            </para>
            <para>
            You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/> compatibility when creating
            <see cref="T:Lucene.Net.Analysis.Ar.ArabicLetterTokenizer"/>:
            <list type="bullet">
            <item><description>As of 3.1, <see cref="T:Lucene.Net.Analysis.Util.CharTokenizer"/> uses an int based API to normalize and
            detect token characters. See <see cref="M:Lucene.Net.Analysis.Ar.ArabicLetterTokenizer.IsTokenChar(System.Int32)"/> and
            <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.Normalize(System.Int32)"/> for details.</description></item>
            </list>
            </para>
            </summary>
            @deprecated (3.1) Use <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> instead. 
        </member>
        <member name="M:Lucene.Net.Analysis.Ar.ArabicLetterTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader)">
            <summary>
            Construct a new ArabicLetterTokenizer. </summary>
            <param name="matchVersion"> <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            to match 
            </param>
            <param name="in">
                     the input to split up into tokens </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Ar.ArabicLetterTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader)">
            <summary>
            Construct a new <see cref="T:Lucene.Net.Analysis.Ar.ArabicLetterTokenizer"/> using a given
            <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/>. 
            </summary>
            <param name="matchVersion">
                    Lucene version to match - See
                    <see cref="T:Lucene.Net.Util.LuceneVersion"/>.
            </param>
            <param name="factory">
                     the attribute factory to use for this Tokenizer </param>
            <param name="in">
                     the input to split up into tokens </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Ar.ArabicLetterTokenizer.IsTokenChar(System.Int32)">
            <summary>
            Allows for Letter category or NonspacingMark category </summary>
            <see cref="M:Lucene.Net.Analysis.Core.LetterTokenizer.IsTokenChar(System.Int32)"/>
        </member>
        <member name="T:Lucene.Net.Analysis.Ar.ArabicLetterTokenizerFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Ar.ArabicLetterTokenizer"/> </summary>
            @deprecated (3.1) Use StandardTokenizerFactory instead.
             
        </member>
        <member name="M:Lucene.Net.Analysis.Ar.ArabicLetterTokenizerFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Ar.ArabicNormalizationFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Ar.ArabicNormalizationFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Ar.ArabicNormalizer"/> to normalize the orthography.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Ar.ArabicNormalizationFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Ar.ArabicNormalizationFilter"/>.
            <code>
            &lt;fieldType name="text_arnormal" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.ArabicNormalizationFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ar.ArabicNormalizationFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Ar.ArabicNormalizationFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Ar.ArabicNormalizer">
            <summary>
            Normalizer for Arabic.
            <para/>
            Normalization is done in-place for efficiency, operating on a termbuffer.
            <para/>
            Normalization is defined as:
            <list type="bullet">
                <item><description> Normalization of hamza with alef seat to a bare alef.</description></item>
                <item><description> Normalization of teh marbuta to heh</description></item>
                <item><description> Normalization of dotless yeh (alef maksura) to yeh.</description></item>
                <item><description> Removal of Arabic diacritics (the harakat)</description></item>
                <item><description> Removal of tatweel (stretching character).</description></item>
            </list>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ar.ArabicNormalizer.Normalize(System.Char[],System.Int32)">
            <summary>
            Normalize an input buffer of Arabic text
            </summary>
            <param name="s"> input buffer </param>
            <param name="len"> length of input buffer </param>
            <returns> length of input buffer after normalization </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Ar.ArabicStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Ar.ArabicStemmer"/> to stem Arabic words..
            <para/>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <seealso cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/>
        </member>
        <member name="T:Lucene.Net.Analysis.Ar.ArabicStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Ar.ArabicStemFilter"/>.
            <code>
            &lt;fieldType name="text_arstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.ArabicNormalizationFilterFactory"/&gt;
                &lt;filter class="solr.ArabicStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ar.ArabicStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Ar.ArabicStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Ar.ArabicStemmer">
            <summary>
            Stemmer for Arabic.
            <para/>
            Stemming is done in-place for efficiency, operating on a termbuffer.
            <para/>
            Stemming is defined as:
            <list type="bullet">
                <item><description> Removal of attached definite article, conjunction, and prepositions.</description></item>
                <item><description> Stemming of common suffixes.</description></item>
            </list>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ar.ArabicStemmer.Stem(System.Char[],System.Int32)">
            <summary>
            Stem an input buffer of Arabic text.
            </summary>
            <param name="s"> input buffer </param>
            <param name="len"> length of input buffer </param>
            <returns> length of input buffer after normalization </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Ar.ArabicStemmer.StemPrefix(System.Char[],System.Int32)">
            <summary>
            Stem a prefix off an Arabic word. </summary>
            <param name="s"> input buffer </param>
            <param name="len"> length of input buffer </param>
            <returns> new length of input buffer after stemming. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Ar.ArabicStemmer.StemSuffix(System.Char[],System.Int32)">
            <summary>
            Stem suffix(es) off an Arabic word. </summary>
            <param name="s"> input buffer </param>
            <param name="len"> length of input buffer </param>
            <returns> new length of input buffer after stemming </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Ar.ArabicStemmer.StartsWithCheckLength(System.Char[],System.Int32,System.Char[])">
            <summary>
            Returns true if the prefix matches and can be stemmed </summary>
            <param name="s"> input buffer </param>
            <param name="len"> length of input buffer </param>
            <param name="prefix"> prefix to check </param>
            <returns> true if the prefix matches and can be stemmed </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Ar.ArabicStemmer.EndsWithCheckLength(System.Char[],System.Int32,System.Char[])">
            <summary>
            Returns true if the suffix matches and can be stemmed </summary>
            <param name="s"> input buffer </param>
            <param name="len"> length of input buffer </param>
            <param name="suffix"> suffix to check </param>
            <returns> true if the suffix matches and can be stemmed </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Bg.BulgarianAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Bulgarian.
            <para>
            This analyzer implements light-stemming as specified by: <i> Searching
            Strategies for the Bulgarian Language </i>
            http://members.unine.ch/jacques.savoy/Papers/BUIR.pdf
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Bg.BulgarianAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Bulgarian stopwords.
            
            Default stopword list is from
            http://members.unine.ch/jacques.savoy/clef/index.html The stopword list is
            BSD-Licensed.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Bg.BulgarianAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop-words set.
            </summary>
            <returns> an unmodifiable instance of the default stop-words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Bg.BulgarianAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the DEFAULT_STOP_SET in a lazy fashion once the outer
            class accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Bg.BulgarianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words:
            <see cref="F:Lucene.Net.Analysis.Bg.BulgarianAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Bg.BulgarianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Bg.BulgarianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words and a stem exclusion set.
            If a stem exclusion set is provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> 
            before <see cref="T:Lucene.Net.Analysis.Bg.BulgarianStemFilter"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Bg.BulgarianAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>, 
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.Bg.BulgarianStemFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Bg.BulgarianStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Bg.BulgarianStemmer"/> to stem Bulgarian
            words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Bg.BulgarianStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Bg.BulgarianStemFilter"/>.
            <code>
            &lt;fieldType name="text_bgstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.BulgarianStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Bg.BulgarianStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Bg.BulgarianStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Bg.BulgarianStemmer">
            <summary>
            Light Stemmer for Bulgarian.
            <para>
            Implements the algorithm described in:  
            <c>
            Searching Strategies for the Bulgarian Language
            </c>
            http://members.unine.ch/jacques.savoy/Papers/BUIR.pdf
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Bg.BulgarianStemmer.Stem(System.Char[],System.Int32)">
            <summary>
            Stem an input buffer of Bulgarian text.
            </summary>
            <param name="s"> input buffer </param>
            <param name="len"> length of input buffer </param>
            <returns> length of input buffer after normalization </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Bg.BulgarianStemmer.RemoveArticle(System.Char[],System.Int32)">
            <summary>
            Mainly remove the definite article </summary>
            <param name="s"> input buffer </param>
            <param name="len"> length of input buffer </param>
            <returns> new stemmed length </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Br.BrazilianAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Brazilian Portuguese language. 
            <para>
            Supports an external list of stopwords (words that
            will not be indexed at all) and an external list of exclusions (words that will
            not be stemmed, but indexed).
            </para>
            
            <para><b>NOTE</b>: This class uses the same <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            dependent settings as <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/>.</para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Br.BrazilianAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Brazilian Portuguese stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Br.BrazilianAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop-words set. </summary>
            <returns> an unmodifiable instance of the default stop-words set. </returns>
        </member>
        <member name="F:Lucene.Net.Analysis.Br.BrazilianAnalyzer.excltable">
            <summary>
            Contains words that should be indexed but not stemmed.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words (<see cref="P:Lucene.Net.Analysis.Br.BrazilianAnalyzer.DefaultStopSet"/>).
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words
            </summary>
            <param name="matchVersion">
                     lucene compatibility version </param>
            <param name="stopwords">
                     a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words and stemming exclusion words
            </summary>
            <param name="matchVersion">
                     lucene compatibility version </param>
            <param name="stopwords">
                     a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            used to tokenize all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from a <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    and <see cref="T:Lucene.Net.Analysis.Br.BrazilianStemFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Br.BrazilianStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Br.BrazilianStemmer"/>.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/>
        </member>
        <member name="F:Lucene.Net.Analysis.Br.BrazilianStemFilter.stemmer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Br.BrazilianStemmer"/> in use by this filter.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Br.BrazilianStemFilter"/> 
            </summary>
            <param name="in"> the source <see cref="T:Lucene.Net.Analysis.TokenStream"/>  </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Br.BrazilianStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Br.BrazilianStemFilter"/>.
            <code>
            &lt;fieldType name="text_brstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.BrazilianStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Br.BrazilianStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Br.BrazilianStemmer">
            <summary>
            A stemmer for Brazilian Portuguese words.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Br.BrazilianStemmer.TERM">
            <summary>
            Changed term
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.Stem(System.String)">
            <summary>
            Stems the given term to an unique <c>discriminator</c>.
            </summary>
            <param name="term">  The term that should be stemmed. </param>
            <returns>Discriminator for <paramref name="term"/></returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.IsStemmable(System.String)">
            <summary>
            Checks a term if it can be processed correctly.
            </summary>
            <returns>  true if, and only if, the given term consists in letters. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.IsIndexable(System.String)">
            <summary>
            Checks a term if it can be processed indexed.
            </summary>
            <returns> true if it can be indexed </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.IsVowel(System.Char)">
            <summary>
            See if string is 'a','e','i','o','u'
            </summary>
            <returns> true if is vowel </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.GetR1(System.String)">
            <summary>
            Gets R1
            
            R1 - is the region after the first non-vowel following a vowel,
                 or is the null region at the end of the word if there is
                 no such non-vowel.
            </summary>
            <returns> null or a string representing R1 </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.GetRV(System.String)">
            <summary>
            Gets RV
            
            RV - IF the second letter is a consonant, RV is the region after
                 the next following vowel,
            
                 OR if the first two letters are vowels, RV is the region
                 after the next consonant,
            
                 AND otherwise (consonant-vowel case) RV is the region after
                 the third letter.
            
                 BUT RV is the end of the word if this positions cannot be
                 found.
            </summary>
            <returns> null or a string representing RV </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.ChangeTerm(System.String)">
            <summary>
            1) Turn to lowercase
            2) Remove accents
            3) ã -> a ; õ -> o
            4) ç -> c
            </summary>
            <returns> null or a string transformed </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.Suffix(System.String,System.String)">
            <summary>
            Check if a string ends with a suffix
            </summary>
            <returns> true if the string ends with the specified suffix </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.ReplaceSuffix(System.String,System.String,System.String)">
            <summary>
            Replace a <see cref="T:System.String"/> suffix by another
            </summary>
            <returns> the replaced <see cref="T:System.String"/> </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.RemoveSuffix(System.String,System.String)">
            <summary>
            Remove a <see cref="T:System.String"/> suffix
            </summary>
            <returns> the <see cref="T:System.String"/> without the suffix </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.SuffixPreceded(System.String,System.String,System.String)">
            <summary>
            See if a suffix is preceded by a <see cref="T:System.String"/>
            </summary>
            <returns> true if the suffix is preceded </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.CreateCT(System.String)">
            <summary>
            Creates CT (changed term) , substituting * 'ã' and 'õ' for 'a~' and 'o~'.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.Step1">
            <summary>
            Standard suffix removal.
            Search for the longest among the following suffixes, and perform
            the following actions:
            </summary>
            <returns> false if no ending was removed </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.Step2">
            <summary>
            Verb suffixes.
            
            Search for the longest among the following suffixes in RV,
            and if found, delete.
            </summary>
            <returns> false if no ending was removed </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.Step3">
            <summary>
            Delete suffix 'i' if in RV and preceded by 'c'
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.Step4">
            <summary>
            Residual suffix
            
            If the word ends with one of the suffixes (os a i o á í ó)
            in RV, delete it
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.Step5">
            <summary>
            If the word ends with one of ( e é ê) in RV,delete it,
            and if preceded by 'gu' (or 'ci') with the 'u' (or 'i') in RV,
            delete the 'u' (or 'i')
            
            Or if the word ends ç remove the cedilha
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Br.BrazilianStemmer.Log">
            <summary>
            For log and debug purpose
            </summary>
            <returns> TERM, CT, RV, R1 and R2 </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Ca.CatalanAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Catalan.
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating CatalanAnalyzer:
            <list>
              <item><description> As of 3.6, <see cref="T:Lucene.Net.Analysis.Util.ElisionFilter"/> with a set of Catalan 
                   contractions is used by default.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Ca.CatalanAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Catalan stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Ca.CatalanAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Ca.CatalanAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Ca.CatalanAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ca.CatalanAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.Ca.CatalanAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ca.CatalanAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Ca.CatalanAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Ca.CatalanAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Util.ElisionFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, 
                    <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>, <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.CharFilters.BaseCharFilter">
            <summary>
            Base utility class for implementing a <see cref="T:Lucene.Net.Analysis.CharFilter"/>.
            You subclass this, and then record mappings by calling
            <see cref="M:Lucene.Net.Analysis.CharFilters.BaseCharFilter.AddOffCorrectMap(System.Int32,System.Int32)"/>, and then invoke the correct
            method to correct an offset.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.BaseCharFilter.Correct(System.Int32)">
            <summary>
            Retrieve the corrected offset. </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.BaseCharFilter.AddOffCorrectMap(System.Int32,System.Int32)">
            <summary>
            <para>
              Adds an offset correction mapping at the given output stream offset.
            </para>
            <para>
              Assumption: the offset given with each successive call to this method
              will not be smaller than the offset given at the previous invocation.
            </para>
            </summary>
            <param name="off"> The output stream offset at which to apply the correction </param>
            <param name="cumulativeDiff"> The input offset is given by adding this
                                  to the output offset </param>
        </member>
        <member name="T:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.CharFilter"/> that wraps another <see cref="T:System.IO.TextReader"/> and attempts to strip out HTML constructs.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.YYEOF">
            <summary>This character denotes the end of file</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.ZZ_BUFFERSIZE">
            <summary>initial size of the lookahead buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.ZZ_LEXSTATE">
            <summary>
            ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
            ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
                at the beginning of a line
            l is of the form l = 2*k, k a non negative integer
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.ZZ_CMAP_PACKED">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.ZZ_CMAP">
            Translates characters to character classes
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.ZZ_ACTION">
            Translates DFA states to action switch labels.
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.ZZ_ROWMAP">
            Translates a state to a row index in the transition table
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.ZZ_TRANS">
            <summary>
            The transition table of the DFA
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.ZZ_UNKNOWN_ERROR">
            <summary>error codes</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.ZZ_ERROR_MSG">
            <summary>error messages for the codes above</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.ZZ_ATTRIBUTE">
            <summary>
            ZZ_ATTRIBUTE[aState] contains the attributes of state <c>aState</c>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.zzReader">
            <summary>the input device</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.zzState">
            <summary>the current state of the DFA</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.zzLexicalState">
            <summary>the current lexical state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.zzBuffer">
            <summary>
            this buffer contains the current text to be matched and is the source of the YyText() string
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.zzMarkedPos">
            <summary>the textposition at the last accepting state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.zzCurrentPos">
            <summary>the current text position in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.zzStartRead">
            <summary>startRead marks the beginning of the YyText() string in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.zzEndRead">
            <summary>
            endRead marks the last character in the buffer, that has been read from input
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.yyline">
            <summary>number of newlines encountered up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.yychar">
            <summary>the number of characters up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.yycolumn">
            <summary>
            the number of characters from the last newline up to the start of the matched text
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.zzAtBOL">
            <summary>
            zzAtBOL == true &lt;=&gt; the scanner is currently at the beginning of a line
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.zzAtEOF">
            <summary>zzAtEOF == true &lt;=&gt; the scanner is at the EOF</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.zzEOFDone">
            <summary>denotes if the user-EOF-code has already been executed</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.upperCaseVariantsAccepted">
            <summary>
            user code:
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.#ctor(System.IO.TextReader)">
            <summary>
            Creates a new HTMLStripCharFilter over the provided TextReader.
            </summary>
            <param name="source"><see cref="T:System.IO.TextReader"/> to strip html tags from.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.#ctor(System.IO.TextReader,System.Collections.Generic.ICollection{System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter"/> over the provided <see cref="T:System.IO.TextReader"/>
            with the specified start and end tags.
            </summary>
            <param name="source"><see cref="T:System.IO.TextReader"/> to strip html tags from.</param>
            <param name="escapedTags">Tags in this set (both start and end tags) will not be filtered out.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.GetBufferedReader(System.IO.TextReader)">
            <summary>
            LUCENENET: Copied this method from the WordlistLoader class - this class requires readers
            with a Reset() method (which .NET readers don't support). So, we use the Java BufferedReader
            as a wrapper for whatever reader the user passes (unless it is already a BufferedReader).
            </summary>
            <param name="reader"></param>
            <returns></returns>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.TextSegment.pos">
            <summary>
            The position from which the next char will be read.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.TextSegment.#ctor(System.Char[],System.Int32)">
            <summary>
            Wraps the given <paramref name="buffer"/> and sets this.len to the given <paramref name="length"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.TextSegment.#ctor(System.Int32)">
            <summary>
            Allocates an internal buffer of the given size.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.TextSegment.Clear">
            <summary>
            Sets len = 0 and pos = 0.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.TextSegment.Restart">
            <summary>
            Sets pos = 0
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.TextSegment.NextChar">
            <summary>
            Returns the next char in the segment.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.TextSegment.IsRead">
            <summary>
            Returns true when all characters in the text segment have been read
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.ZzUnpackCMap(System.String)">
            <summary>
            Unpacks the compressed character translation table.
            </summary>
            <param name="packed">the packed character translation table</param>
            <returns>the unpacked character translation table</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.ZzRefill">
            <summary>
            Refills the input buffer.
            </summary>
            <returns><c>false</c>, iff there was new input.</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.YyClose">
            <summary>
            Disposes the input stream.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.YyReset(Lucene.Net.Analysis.Util.BufferedCharFilter)">
            <summary>
            Resets the scanner to read from a new input stream.
            Does not close the old reader.
            <para/>
            All internal variables are reset, the old input stream
            <b>cannot</b> be reused (internal buffer is discarded and lost).
            Lexical state is set to <see cref="F:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.YYINITIAL"/>.
            <para/>
            Internal scan buffer is resized down to its initial length, if it has grown.
            </summary>
            <param name="reader">the new input stream</param>
        </member>
        <member name="P:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.YyState">
            <summary>
            Returns the current lexical state.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.YyBegin(System.Int32)">
            <summary>
            Enters a new lexical state
            </summary>
            <param name="newState">the new lexical state</param>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.YyText">
            <summary>
            Returns the text matched by the current regular expression.
            </summary>
            <returns>Returns the text matched by the current regular expression.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.YyCharAt(System.Int32)">
            <summary>
            Returns the character at position <tt>pos</tt> from the 
            matched text. It is equivalent to YyText[pos], but faster
            </summary>
            <param name="pos">the position of the character to fetch. A value from 0 to YyLength()-1.</param>
            <returns>the character at position pos</returns>
        </member>
        <member name="P:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.YyLength">
            <summary>
            Returns the length of the matched text region.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.ZzScanError(System.Int32)">
            <summary>
            Reports an error that occured while scanning.
            <para/>
            In a wellformed scanner (no or only correct usage of
            YyPushBack(int) and a match-all fallback rule) this method 
            will only be called with things that "Can't Possibly Happen".
            If this method is called, something is seriously wrong
            (e.g. a JFlex bug producing a faulty scanner etc.).
            <para/>
            Usual syntax/scanner level error handling should be done
            in error fallback rules.
            </summary>
            <param name="errorCode">the code of the errormessage to display</param>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.YyPushBack(System.Int32)">
            <summary>
            Pushes the specified amount of characters back into the input stream.
            
            They will be read again by then next call of the scanning method
            </summary>
            <param name="number">the number of characters to be read again.
            This number must not be greater than YyLength()!</param>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.ZzDoEOF">
            <summary>
            Contains user EOF-code, which will be executed exactly once,
            when the end of file is reached
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter.NextChar">
            <summary>
            Resumes scanning until the next regular expression is matched,
            the end of input is encountered or an I/O-Error occurs.
            </summary>
            <returns>the next token</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="T:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilter"/>. 
            <code>
            &lt;fieldType name="text_html" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;charFilter class="solr.HTMLStripCharFilterFactory" escapedTags="a, title" /&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.CharFilters.HTMLStripCharFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.CharFilters.MappingCharFilter">
            <summary>
            Simplistic <see cref="T:Lucene.Net.Analysis.CharFilter"/> that applies the mappings
            contained in a <see cref="T:Lucene.Net.Analysis.CharFilters.NormalizeCharMap"/> to the character
            stream, and correcting the resulting changes to the
            offsets.  Matching is greedy (longest pattern matching at
            a given point wins).  Replacement is allowed to be the
            empty string.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.CharFilters.MappingCharFilter._input">
            <summary>
            LUCENENET specific support to buffer the reader.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.MappingCharFilter.#ctor(Lucene.Net.Analysis.CharFilters.NormalizeCharMap,System.IO.TextReader)">
            <summary>
            Default constructor that takes a <see cref="T:System.IO.TextReader"/>. </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.MappingCharFilter.GetBufferedReader(System.IO.TextReader)">
            <summary>
            LUCENENET: Copied this method from the <see cref="T:Lucene.Net.Analysis.Util.WordlistLoader"/> class - this class requires readers
            with a Reset() method (which .NET readers don't support). So, we use the <see cref="T:Lucene.Net.Analysis.Util.BufferedCharFilter"/> 
            (which is similar to Java BufferedReader) as a wrapper for whatever reader the user passes 
            (unless it is already a <see cref="T:Lucene.Net.Analysis.Util.BufferedCharFilter"/>).
            </summary>
            <param name="reader"></param>
            <returns></returns>
        </member>
        <member name="T:Lucene.Net.Analysis.CharFilters.MappingCharFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.CharFilters.MappingCharFilter"/>. 
            <code>
            &lt;fieldType name="text_map" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;charFilter class="solr.MappingCharFilterFactory" mapping="mapping.txt"/&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            
            @since Solr 1.4
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.MappingCharFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.CharFilters.MappingCharFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.CharFilters.NormalizeCharMap">
            <summary>
            Holds a map of <see cref="T:System.String"/> input to <see cref="T:System.String"/> output, to be used
            with <see cref="T:Lucene.Net.Analysis.CharFilters.NormalizeCharMap.Builder"/>.  Use the <see cref="T:Lucene.Net.Analysis.CharFilters.MappingCharFilter"/>
            to create this.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.CharFilters.NormalizeCharMap.Builder">
            <summary>
            Builds an NormalizeCharMap.
            <para>
            Call add() until you have added all the mappings, then call build() to get a NormalizeCharMap
            @lucene.experimental
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.NormalizeCharMap.Builder.Add(System.String,System.String)">
            <summary>
            Records a replacement to be applied to the input
             stream.  Whenever <code>singleMatch</code> occurs in
             the input, it will be replaced with
             <code>replacement</code>.
            </summary>
            <param name="match"> input String to be replaced </param>
            <param name="replacement"> output String </param>
            <exception cref="T:System.ArgumentException"> if
            <code>match</code> is the empty string, or was
            already previously added </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.CharFilters.NormalizeCharMap.Builder.Build">
            <summary>
            Builds the <see cref="T:Lucene.Net.Analysis.CharFilters.NormalizeCharMap"/>; call this once you
            are done calling <see cref="M:Lucene.Net.Analysis.CharFilters.NormalizeCharMap.Builder.Add(System.String,System.String)"/>. 
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Cjk.CJKAnalyzer">
            <summary>
            An <see cref="T:Lucene.Net.Analysis.Analyzer"/> that tokenizes text with <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/>,
            normalizes content with <see cref="T:Lucene.Net.Analysis.Cjk.CJKWidthFilter"/>, folds case with
            <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, forms bigrams of CJK with <see cref="T:Lucene.Net.Analysis.Cjk.CJKBigramFilter"/>,
            and filters stopwords with <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default CJK stopwords.
            <para/>
            Currently it contains some common English words that are not usually
            useful for searching and some double-byte interpunctions.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Cjk.CJKAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop-words set. </summary>
            <returns> an unmodifiable instance of the default stop-words set. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Cjk.CJKAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer which removes words in <see cref="P:Lucene.Net.Analysis.Cjk.CJKAnalyzer.DefaultStopSet"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Cjk.CJKAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words
            </summary>
            <param name="matchVersion">
                     lucene compatibility version </param>
            <param name="stopwords">
                     a stopword set </param>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKScript.HAN">
            <summary>
            bigram flag for Han Ideographs </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKScript.HIRAGANA">
            <summary>
            bigram flag for Hiragana </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKScript.KATAKANA">
            <summary>
            bigram flag for Katakana </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKScript.HANGUL">
            <summary>
            bigram flag for Hangul </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKScript.ALL">
            <summary>
            bigram flag for all scripts </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Cjk.CJKBigramFilter">
            <summary>
            Forms bigrams of CJK terms that are generated from <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/>
            or ICUTokenizer.
            <para>
            CJK types are set by these tokenizers, but you can also use 
            <see cref="M:Lucene.Net.Analysis.Cjk.CJKBigramFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Cjk.CJKScript)"/> to explicitly control which
            of the CJK scripts are turned into bigrams.
            </para>
            <para>
            By default, when a CJK character has no adjacent characters to form
            a bigram, it is output in unigram form. If you want to always output
            both unigrams and bigrams, set the <code>outputUnigrams</code>
            flag in <see cref="M:Lucene.Net.Analysis.Cjk.CJKBigramFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Cjk.CJKScript,System.Boolean)"/>.
            This can be used for a combined unigram+bigram approach.
            </para>
            <para>
            In all cases, all non-CJK input is passed thru unmodified.
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKBigramFilter.DOUBLE_TYPE">
            <summary>
            when we emit a bigram, its then marked as this type </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKBigramFilter.SINGLE_TYPE">
            <summary>
            when we emit a unigram, its then marked as this type </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Cjk.CJKBigramFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Calls <see cref="M:Lucene.Net.Analysis.Cjk.CJKBigramFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Cjk.CJKScript)">
                  CJKBigramFilter(@in, CJKScript.HAN | CJKScript.HIRAGANA | CJKScript.KATAKANA | CJKScript.HANGUL)</see>
            </summary>
            <param name="in">
                     Input <see cref="T:Lucene.Net.Analysis.TokenStream"/> </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Cjk.CJKBigramFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Cjk.CJKScript)">
            <summary>
            Calls <see cref="M:Lucene.Net.Analysis.Cjk.CJKBigramFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Cjk.CJKScript,System.Boolean)">
                  CJKBigramFilter(in, flags, false)</see>
            </summary>
            <param name="in">
                     Input <see cref="T:Lucene.Net.Analysis.TokenStream"/> </param>
            <param name="flags"> OR'ed set from <see cref="F:Lucene.Net.Analysis.Cjk.CJKScript.HAN"/>, <see cref="F:Lucene.Net.Analysis.Cjk.CJKScript.HIRAGANA"/>, 
                   <see cref="F:Lucene.Net.Analysis.Cjk.CJKScript.KATAKANA"/>, <see cref="F:Lucene.Net.Analysis.Cjk.CJKScript.HANGUL"/> </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Cjk.CJKBigramFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Cjk.CJKScript,System.Boolean)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Cjk.CJKBigramFilter"/>, specifying which writing systems should be bigrammed,
            and whether or not unigrams should also be output. </summary>
            <param name="in">
                     Input <see cref="T:Lucene.Net.Analysis.TokenStream"/> </param>
            <param name="flags"> OR'ed set from <see cref="F:Lucene.Net.Analysis.Cjk.CJKScript.HAN"/>, <see cref="F:Lucene.Net.Analysis.Cjk.CJKScript.HIRAGANA"/>, 
                   <see cref="F:Lucene.Net.Analysis.Cjk.CJKScript.KATAKANA"/>, <see cref="F:Lucene.Net.Analysis.Cjk.CJKScript.HANGUL"/> </param>
            <param name="outputUnigrams"> true if unigrams for the selected writing systems should also be output.
                   when this is false, this is only done when there are no adjacent characters to form
                   a bigram. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Cjk.CJKBigramFilter.DoNext">
            <summary>
            looks at next input token, returning false is none is available 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Cjk.CJKBigramFilter.Refill">
            <summary>
            refills buffers with new data from the current token.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Cjk.CJKBigramFilter.FlushBigram">
            <summary>
            Flushes a bigram token to output from our buffer 
            This is the normal case, e.g. ABC -> AB BC
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Cjk.CJKBigramFilter.FlushUnigram">
            <summary>
            Flushes a unigram token to output from our buffer.
            This happens when we encounter isolated CJK characters, either the whole
            CJK string is a single character, or we encounter a CJK character surrounded 
            by space, punctuation, english, etc, but not beside any other CJK.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Cjk.CJKBigramFilter.HasBufferedBigram">
            <summary>
            True if we have multiple codepoints sitting in our buffer
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Cjk.CJKBigramFilter.HasBufferedUnigram">
            <summary>
            True if we have a single codepoint sitting in our buffer, where its future
            (whether it is emitted as unigram or forms a bigram) depends upon not-yet-seen
            inputs.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Cjk.CJKBigramFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Cjk.CJKBigramFilter"/>.
            <code>
            &lt;fieldType name="text_cjk" class="solr.TextField"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.CJKWidthFilterFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.CJKBigramFilterFactory" 
                  han="true" hiragana="true" 
                  katakana="true" hangul="true" outputUnigrams="false" /&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Cjk.CJKBigramFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Cjk.CJKBigramFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Cjk.CJKTokenizer">
            <summary>
            CJKTokenizer is designed for Chinese, Japanese, and Korean languages.
            <para>  
            The tokens returned are every two adjacent characters with overlap match.
            </para>
            <para>
            Example: "java C1C2C3C4" will be segmented to: "java" "C1C2" "C2C3" "C3C4".
            </para>
            Additionally, the following is applied to Latin text (such as English):
            <list type="bullet">
                <item><description>Text is converted to lowercase.</description></item>
                <item><description>Numeric digits, '+', '#', and '_' are tokenized as letters.</description></item>
                <item><description>Full-width forms are converted to half-width forms.</description></item>
            </list>
            For more info on Asian language (Chinese, Japanese, and Korean) text segmentation:
            please search  <a
            href="http://www.google.com/search?q=word+chinese+segment">google</a>
            </summary>
            @deprecated Use StandardTokenizer, CJKWidthFilter, CJKBigramFilter, and LowerCaseFilter instead. 
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKTokenizer.WORD_TYPE">
            <summary>
            Word token type </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKTokenizer.SINGLE_TOKEN_TYPE">
            <summary>
            Single byte token type </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKTokenizer.DOUBLE_TOKEN_TYPE">
            <summary>
            Double byte token type </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKTokenizer.TOKEN_TYPE_NAMES">
            <summary>
            Names for token types </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKTokenizer.MAX_WORD_LEN">
            <summary>
            Max word length </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKTokenizer.IO_BUFFER_SIZE">
            <summary>
            buffer size: </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKTokenizer.offset">
            <summary>
            word offset, used to imply which character(in ) is parsed </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKTokenizer.bufferIndex">
            <summary>
            the index used only for ioBuffer </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKTokenizer.dataLen">
            <summary>
            data length </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKTokenizer.buffer">
            <summary>
            character buffer, store the characters which are used to compose 
            the returned Token
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKTokenizer.ioBuffer">
            <summary>
            I/O buffer, used to store the content of the input(one of the
            members of Tokenizer)
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKTokenizer.tokenType">
            <summary>
            word type: single=>ASCII  double=>non-ASCII word=>default </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKTokenizer.preIsTokened">
            <summary>
            tag: previous character is a cached double-byte character  "C1C2C3C4"
            ----(set the C1 isTokened) C1C2 "C2C3C4" ----(set the C2 isTokened)
            C1C2 C2C3 "C3C4" ----(set the C3 isTokened) "C1C2 C2C3 C3C4"
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Cjk.CJKTokenizer.#ctor(System.IO.TextReader)">
            <summary>
            Construct a token stream processing the given input.
            </summary>
            <param name="in"> I/O reader </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Cjk.CJKTokenizer.IncrementToken">
            <summary>
            Returns true for the next token in the stream, or false at EOS.
            See http://java.sun.com/j2se/1.3/docs/api/java/lang/Character.UnicodeBlock.html
            for detail.
            </summary>
            <returns> false for end of stream, true otherwise
            </returns>
            <exception cref="T:System.IO.IOException"> when read error
                    happened in the InputStream
             </exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Cjk.CJKTokenizerFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Cjk.CJKTokenizer"/>. 
            <code>
            &lt;fieldType name="text_cjk" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.CJKTokenizerFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
            @deprecated Use <see cref="T:Lucene.Net.Analysis.Cjk.CJKBigramFilterFactory"/> instead. 
        </member>
        <member name="M:Lucene.Net.Analysis.Cjk.CJKTokenizerFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Cjk.CJKTokenizerFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Cjk.CJKWidthFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that normalizes CJK width differences:
            <list type="bullet">
              <item><description>Folds fullwidth ASCII variants into the equivalent basic latin</description></item>
              <item><description>Folds halfwidth Katakana variants into the equivalent kana</description></item>
            </list>
            <para>
            NOTE: this filter can be viewed as a (practical) subset of NFKC/NFKD
            Unicode normalization. See the normalization support in the ICU package
            for full normalization.
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKWidthFilter.KANA_NORM">
            <summary>
            halfwidth kana mappings: 0xFF65-0xFF9D 
            <para/>
            note: 0xFF9C and 0xFF9D are only mapped to 0x3099 and 0x309A
            as a fallback when they cannot properly combine with a preceding 
            character into a composed form.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cjk.CJKWidthFilter.KANA_COMBINE_VOICED">
            <summary>kana combining diffs: 0x30A6-0x30FD </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Cjk.CJKWidthFilter.Combine(System.Char[],System.Int32,System.Char)">
            <summary>
            returns true if we successfully combined the voice mark </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Cjk.CJKWidthFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Cjk.CJKWidthFilter"/>.
            <code>
            &lt;fieldType name="text_cjk" class="solr.TextField"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.CJKWidthFilterFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.CJKBigramFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Cjk.CJKWidthFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Cjk.CJKWidthFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Ckb.SoraniAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Sorani Kurdish.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Ckb.SoraniAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Kurdish stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Ckb.SoraniAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Ckb.SoraniAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Ckb.SoraniAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ckb.SoraniAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.Ckb.SoraniAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ckb.SoraniAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Ckb.SoraniAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Ckb.SoraniAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Ckb.SoraniNormalizationFilter"/>, 
                    <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.Ckb.SoraniStemFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Ckb.SoraniNormalizationFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Ckb.SoraniNormalizer"/> to normalize the
            orthography.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Ckb.SoraniNormalizationFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Ckb.SoraniNormalizationFilter"/>.
            <code>
            &lt;fieldType name="text_ckbnormal" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.SoraniNormalizationFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ckb.SoraniNormalizationFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Ckb.SoraniNormalizationFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Ckb.SoraniNormalizer">
            <summary>
            Normalizes the Unicode representation of Sorani text.
            <para/>
            Normalization consists of:
            <list type="bullet">
              <item><description>Alternate forms of 'y' (0064, 0649) are converted to 06CC (FARSI YEH)</description></item>
              <item><description>Alternate form of 'k' (0643) is converted to 06A9 (KEHEH)</description></item>
              <item><description>Alternate forms of vowel 'e' (0647+200C, word-final 0647, 0629) are converted to 06D5 (AE)</description></item>
              <item><description>Alternate (joining) form of 'h' (06BE) is converted to 0647</description></item>
              <item><description>Alternate forms of 'rr' (0692, word-initial 0631) are converted to 0695 (REH WITH SMALL V BELOW)</description></item>
              <item><description>Harakat, tatweel, and formatting characters such as directional controls are removed.</description></item>
            </list>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ckb.SoraniNormalizer.Normalize(System.Char[],System.Int32)">
            <summary>
            Normalize an input buffer of Sorani text
            </summary>
            <param name="s"> input buffer </param>
            <param name="len"> length of input buffer </param>
            <returns> length of input buffer after normalization </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Ckb.SoraniStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Ckb.SoraniStemmer"/> to stem Sorani words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para> 
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/>
        </member>
        <member name="T:Lucene.Net.Analysis.Ckb.SoraniStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Ckb.SoraniStemFilter"/>.
            <code>
            &lt;fieldType name="text_ckbstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.SoraniNormalizationFilterFactory"/&gt;
                &lt;filter class="solr.SoraniStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ckb.SoraniStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Ckb.SoraniStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Ckb.SoraniStemmer">
            <summary>
            Light stemmer for Sorani
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ckb.SoraniStemmer.Stem(System.Char[],System.Int32)">
            <summary>
            Stem an input buffer of Sorani text.
            </summary>
            <param name="s"> input buffer </param>
            <param name="len"> length of input buffer </param>
            <returns> length of input buffer after normalization </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Cn.ChineseAnalyzer">
            <summary>
            An <see cref="T:Lucene.Net.Analysis.Analyzer"/> that tokenizes text with <see cref="T:Lucene.Net.Analysis.Cn.ChineseTokenizer"/> and
            filters with <see cref="T:Lucene.Net.Analysis.Cn.ChineseFilter"/>
            </summary>
            @deprecated (3.1) Use <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/> instead, which has the same functionality.
            This analyzer will be removed in Lucene 5.0 
        </member>
        <member name="M:Lucene.Net.Analysis.Cn.ChineseAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            used to tokenize all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from a <see cref="T:Lucene.Net.Analysis.Cn.ChineseTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Cn.ChineseFilter"/> </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Cn.ChineseFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> with a stop word table.  
            <list type="bullet">
                <item><description>Numeric tokens are removed.</description></item>
                <item><description>English tokens must be larger than 1 character.</description></item>
                <item><description>One Chinese character as one Chinese word.</description></item>
            </list>
            TO DO:
            <list type="number">
                <item><description>Add Chinese stop words, such as \ue400</description></item>
                <item><description>Dictionary based Chinese word extraction</description></item>
                <item><description>Intelligent Chinese word extraction</description></item>
            </list>
            </summary>
            @deprecated (3.1) Use <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/> instead, which has the same functionality.
            This filter will be removed in Lucene 5.0 
        </member>
        <member name="T:Lucene.Net.Analysis.Cn.ChineseFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Cn.ChineseFilter"/> </summary>
            @deprecated Use <see cref="T:Lucene.Net.Analysis.Core.StopFilterFactory"/> instead. 
        </member>
        <member name="M:Lucene.Net.Analysis.Cn.ChineseFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Cn.ChineseFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Cn.ChineseTokenizer">
            <summary>
            Tokenize Chinese text as individual chinese characters.
            
            <para>
            The difference between <see cref="T:Lucene.Net.Analysis.Cn.ChineseTokenizer"/> and
            <see cref="T:Lucene.Net.Analysis.Cjk.CJKTokenizer"/> is that they have different
            token parsing logic.
            </para>
            <para>
            For example, if the Chinese text
            "C1C2C3C4" is to be indexed:
            <list type="bullet">
                <item><description>The tokens returned from ChineseTokenizer are C1, C2, C3, C4.</description></item>
                <item><description>The tokens returned from the CJKTokenizer are C1C2, C2C3, C3C4.</description></item>
            </list>
            </para>
            <para>
            Therefore the index created by <see cref="T:Lucene.Net.Analysis.Cjk.CJKTokenizer"/> is much larger.
            </para>
            <para>
            The problem is that when searching for C1, C1C2, C1C3,
            C4C2, C1C2C3 ... the <see cref="T:Lucene.Net.Analysis.Cn.ChineseTokenizer"/> works, but the
            <see cref="T:Lucene.Net.Analysis.Cjk.CJKTokenizer"/> will not work.
            </para> 
            </summary>
            @deprecated (3.1) Use <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> instead, which has the same functionality.
            This filter will be removed in Lucene 5.0 
        </member>
        <member name="T:Lucene.Net.Analysis.Cn.ChineseTokenizerFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Cn.ChineseTokenizer"/>
            </summary>
            @deprecated Use <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizerFactory"/> instead. 
        </member>
        <member name="M:Lucene.Net.Analysis.Cn.ChineseTokenizerFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Cn.ChineseTokenizerFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.CommonGrams.CommonGramsFilter">
            <summary>
            Construct bigrams for frequently occurring terms while indexing. Single terms
            are still indexed too, with bigrams overlaid. This is achieved through the
            use of <see cref="P:Lucene.Net.Analysis.TokenAttributes.PositionIncrementAttribute.PositionIncrement"/>. Bigrams have a type
            of <see cref="F:Lucene.Net.Analysis.CommonGrams.CommonGramsFilter.GRAM_TYPE"/> Example:
            <list type="bullet">
                <item><description>input:"the quick brown fox"</description></item>
                <item><description>output:|"the","the-quick"|"brown"|"fox"|</description></item>
                <item><description>"the-quick" has a position increment of 0 so it is in the same position
            as "the" "the-quick" has a term.type() of "gram"</description></item>
            </list>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CommonGrams.CommonGramsFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Construct a token stream filtering the given input using a Set of common
            words to create bigrams. Outputs both unigrams with position increment and
            bigrams with position increment 0 type=gram where one or both of the words
            in a potential bigram are in the set of common words .
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="input"> <see cref="T:Lucene.Net.Analysis.TokenStream"/> input in filter chain </param>
            <param name="commonWords"> The set of common words. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.CommonGrams.CommonGramsFilter.IncrementToken">
            <summary>
            Inserts bigrams for common words into a token stream. For each input token,
            output the token. If the token and/or the following token are in the list
            of common words also output a bigram with position increment 0 and
            type="gram"
            <para/>
            TODO:Consider adding an option to not emit unigram stopwords
            as in CDL XTF BigramStopFilter, <see cref="T:Lucene.Net.Analysis.CommonGrams.CommonGramsQueryFilter"/> would need to be
            changed to work with this.
            <para/>
            TODO: Consider optimizing for the case of three
            commongrams i.e "man of the year" normally produces 3 bigrams: "man-of",
            "of-the", "the-year" but with proper management of positions we could
            eliminate the middle bigram "of-the"and save a disk seek and a whole set of
            position lookups.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CommonGrams.CommonGramsFilter.Reset">
            <summary>
            This method is called by a consumer before it begins consumption using
            <see cref="M:Lucene.Net.Analysis.CommonGrams.CommonGramsFilter.IncrementToken"/>.
            <para/>
            Resets this stream to a clean state. Stateful implementations must implement
            this method so that they can be reused, just as if they had been created fresh.
            <para/>
            If you override this method, always call <c>base.Reset()</c>, otherwise
            some internal state will not be correctly reset (e.g., <see cref="T:Lucene.Net.Analysis.Tokenizer"/> will
            throw <see cref="T:System.InvalidOperationException"/> on further usage).
            </summary>
            <remarks>
            <b>NOTE:</b>
            The default implementation chains the call to the input <see cref="T:Lucene.Net.Analysis.TokenStream"/>, so
            be sure to call <c>base.Reset()</c> when overriding this method.
            </remarks>
        </member>
        <member name="P:Lucene.Net.Analysis.CommonGrams.CommonGramsFilter.IsCommon">
            <summary>
            Determines if the current token is a common term
            </summary>
            <returns> <c>true</c> if the current token is a common term, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.CommonGrams.CommonGramsFilter.SaveTermBuffer">
            <summary>
            Saves this information to form the left part of a gram
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CommonGrams.CommonGramsFilter.GramToken">
            <summary>
            Constructs a compound token.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.CommonGrams.CommonGramsFilterFactory">
            <summary>
            Constructs a <see cref="T:Lucene.Net.Analysis.CommonGrams.CommonGramsFilter"/>.
            <code>
            &lt;fieldType name="text_cmmngrms" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.CommonGramsFilterFactory" words="commongramsstopwords.txt" ignoreCase="false"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CommonGrams.CommonGramsFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.CommonGrams.CommonGramsFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.CommonGrams.CommonGramsQueryFilter">
            <summary>
            Wrap a <see cref="T:Lucene.Net.Analysis.CommonGrams.CommonGramsFilter"/> optimizing phrase queries by only returning single
            words when they are not a member of a bigram.
            <para/>
            Example:
            <list type="bullet">
                <item><description>query input to CommonGramsFilter: "the rain in spain falls mainly"</description></item>
                <item><description>output of CommomGramsFilter/input to CommonGramsQueryFilter:
                |"the, "the-rain"|"rain" "rain-in"|"in, "in-spain"|"spain"|"falls"|"mainly"</description></item>
                <item><description>output of CommonGramsQueryFilter:"the-rain", "rain-in" ,"in-spain",
                "falls", "mainly"</description></item>
            </list>
            </summary>
            <remarks>
            See:http://hudson.zones.apache.org/hudson/job/Lucene-trunk/javadoc//all/org/apache/lucene/analysis/TokenStream.html and
            http://svn.apache.org/viewvc/lucene/dev/trunk/lucene/src/java/org/apache/lucene/analysis/package.html?revision=718798
            </remarks>
        </member>
        <member name="M:Lucene.Net.Analysis.CommonGrams.CommonGramsQueryFilter.#ctor(Lucene.Net.Analysis.CommonGrams.CommonGramsFilter)">
            <summary>
            Constructs a new CommonGramsQueryFilter based on the provided CommomGramsFilter 
            </summary>
            <param name="input"> CommonGramsFilter the QueryFilter will use </param>
        </member>
        <member name="M:Lucene.Net.Analysis.CommonGrams.CommonGramsQueryFilter.Reset">
            <summary>
            This method is called by a consumer before it begins consumption using
            <see cref="M:Lucene.Net.Analysis.CommonGrams.CommonGramsQueryFilter.IncrementToken"/>.
            <para/>
            Resets this stream to a clean state. Stateful implementations must implement
            this method so that they can be reused, just as if they had been created fresh.
            <para/>
            If you override this method, always call <c>base.Reset()</c>, otherwise
            some internal state will not be correctly reset (e.g., <see cref="T:Lucene.Net.Analysis.Tokenizer"/> will
            throw <see cref="T:System.InvalidOperationException"/> on further usage).
            </summary>
            <remarks>
            <b>NOTE:</b>
            The default implementation chains the call to the input <see cref="T:Lucene.Net.Analysis.TokenStream"/>, so
            be sure to call <c>base.Reset()</c> when overriding this method.
            </remarks>
        </member>
        <member name="M:Lucene.Net.Analysis.CommonGrams.CommonGramsQueryFilter.IncrementToken">
            <summary>
            Output bigrams whenever possible to optimize queries. Only output unigrams
            when they are not a member of a bigram. Example:
            <list type="bullet">
                <item><description>input: "the rain in spain falls mainly"</description></item>
                <item><description>output:"the-rain", "rain-in" ,"in-spain", "falls", "mainly"</description></item>
            </list>
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.CommonGrams.CommonGramsQueryFilter.IsGramType">
            <summary>
            Convenience method to check if the current type is a gram type
            </summary>
            <returns> <c>true</c> if the current type is a gram type, <c>false</c> otherwise </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.CommonGrams.CommonGramsQueryFilterFactory">
            <summary>
            Construct <see cref="T:Lucene.Net.Analysis.CommonGrams.CommonGramsQueryFilter"/>.
            
            <code>
            &lt;fieldType name="text_cmmngrmsqry" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.CommonGramsQueryFilterFactory" words="commongramsquerystopwords.txt" ignoreCase="false"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CommonGrams.CommonGramsQueryFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.CommonGrams.CommonGramsQueryFilterFactory"/> </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.CommonGrams.CommonGramsQueryFilterFactory.Create(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Create a <see cref="T:Lucene.Net.Analysis.CommonGrams.CommonGramsFilter"/> and wrap it with a <see cref="T:Lucene.Net.Analysis.CommonGrams.CommonGramsQueryFilter"/>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Compound.CompoundWordTokenFilterBase">
            <summary>
            Base class for decomposition token filters.
            <para/>
            You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/> compatibility when creating
            <see cref="T:Lucene.Net.Analysis.Compound.CompoundWordTokenFilterBase"/>:
            <list type="bullet">
                <item><description>As of 3.1, CompoundWordTokenFilterBase correctly handles Unicode 4.0
                supplementary characters in strings and char arrays provided as compound word
                dictionaries.</description></item>
                <item><description>As of 4.4, <see cref="T:Lucene.Net.Analysis.Compound.CompoundWordTokenFilterBase"/> doesn't update offsets.</description></item>
            </list>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE">
            <summary>
            The default for minimal word length that gets decomposed
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE">
            <summary>
            The default for minimal length of subwords that get propagated to the output of this filter
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE">
            <summary>
            The default for maximal length of subwords that get propagated to the output of this filter
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.CompoundWordTokenFilterBase.Decompose">
            <summary>
            Decomposes the current <see cref="F:Lucene.Net.Analysis.Compound.CompoundWordTokenFilterBase.m_termAtt"/> and places <see cref="T:Lucene.Net.Analysis.Compound.CompoundWordTokenFilterBase.CompoundToken"/> instances in the <see cref="F:Lucene.Net.Analysis.Compound.CompoundWordTokenFilterBase.m_tokens"/> list.
            The original token may not be placed in the list, as it is automatically passed through this filter.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Compound.CompoundWordTokenFilterBase.CompoundToken">
            <summary>
            Helper class to hold decompounded token information
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.CompoundWordTokenFilterBase.CompoundToken.#ctor(Lucene.Net.Analysis.Compound.CompoundWordTokenFilterBase,System.Int32,System.Int32)">
            <summary>
            Construct the compound token based on a slice of the current <see cref="F:Lucene.Net.Analysis.Compound.CompoundWordTokenFilterBase.m_termAtt"/>. </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Compound.DictionaryCompoundWordTokenFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that decomposes compound words found in many Germanic languages.
            <para>
            "Donaudampfschiff" becomes Donau, dampf, schiff so that you can find
            "Donaudampfschiff" even when you only enter "schiff". 
             It uses a brute-force algorithm to achieve this.
            </para>
            <para>
            You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/> compatibility when creating
            <see cref="T:Lucene.Net.Analysis.Compound.CompoundWordTokenFilterBase"/>:
            <list type="bullet">
                <item><description>As of 3.1, CompoundWordTokenFilterBase correctly handles Unicode 4.0
                supplementary characters in strings and char arrays provided as compound word
                dictionaries.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.DictionaryCompoundWordTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Compound.DictionaryCompoundWordTokenFilter"/>
            </summary>
            <param name="matchVersion">
                     Lucene version to enable correct Unicode 4.0 behavior in the
                     dictionaries if Version > 3.0. See <a
                     href="CompoundWordTokenFilterBase.html#version"
                     >CompoundWordTokenFilterBase</a> for details. </param>
            <param name="input">
                     the <see cref="T:Lucene.Net.Analysis.TokenStream"/> to process </param>
            <param name="dictionary">
                     the word dictionary to match against. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.DictionaryCompoundWordTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Util.CharArraySet,System.Int32,System.Int32,System.Int32,System.Boolean)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Compound.DictionaryCompoundWordTokenFilter"/>
            </summary>
            <param name="matchVersion">
                     Lucene version to enable correct Unicode 4.0 behavior in the
                     dictionaries if Version > 3.0. See <a
                     href="CompoundWordTokenFilterBase.html#version"
                     >CompoundWordTokenFilterBase</a> for details. </param>
            <param name="input">
                     the <see cref="T:Lucene.Net.Analysis.TokenStream"/> to process </param>
            <param name="dictionary">
                     the word dictionary to match against. </param>
            <param name="minWordSize">
                     only words longer than this get processed </param>
            <param name="minSubwordSize">
                     only subwords longer than this get to the output stream </param>
            <param name="maxSubwordSize">
                     only subwords shorter than this get to the output stream </param>
            <param name="onlyLongestMatch">
                     Add only the longest matching subword to the stream </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Compound.DictionaryCompoundWordTokenFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Compound.DictionaryCompoundWordTokenFilter"/>. 
            <code>
            &lt;fieldType name="text_dictcomp" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.DictionaryCompoundWordTokenFilterFactory" dictionary="dictionary.txt"
                    minWordSize="5" minSubwordSize="2" maxSubwordSize="15" onlyLongestMatch="true"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.DictionaryCompoundWordTokenFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Compound.DictionaryCompoundWordTokenFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that decomposes compound words found in many Germanic languages.
            <para>
            "Donaudampfschiff" becomes Donau, dampf, schiff so that you can find
            "Donaudampfschiff" even when you only enter "schiff". It uses a hyphenation
            grammar and a word dictionary to achieve this.
            </para>
            <para>
            You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/> compatibility when creating
            <see cref="T:Lucene.Net.Analysis.Compound.CompoundWordTokenFilterBase"/>:
            <list type="bullet">
                <item><description>As of 3.1, CompoundWordTokenFilterBase correctly handles Unicode 4.0
                supplementary characters in strings and char arrays provided as compound word
                dictionaries.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter"/> instance. 
            </summary>
            <param name="matchVersion">
                     Lucene version to enable correct Unicode 4.0 behavior in the
                     dictionaries if Version > 3.0. See <a
                     href="CompoundWordTokenFilterBase.html#version"
                     >CompoundWordTokenFilterBase</a> for details. </param>
            <param name="input">
                     the <see cref="T:Lucene.Net.Analysis.TokenStream"/> to process </param>
            <param name="hyphenator">
                     the hyphenation pattern tree to use for hyphenation </param>
            <param name="dictionary">
                     the word dictionary to match against. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree,Lucene.Net.Analysis.Util.CharArraySet,System.Int32,System.Int32,System.Int32,System.Boolean)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter"/> instance.
            </summary>
            <param name="matchVersion">
                     Lucene version to enable correct Unicode 4.0 behavior in the
                     dictionaries if Version > 3.0. See <a
                     href="CompoundWordTokenFilterBase.html#version"
                     >CompoundWordTokenFilterBase</a> for details. </param>
            <param name="input">
                     the <see cref="T:Lucene.Net.Analysis.TokenStream"/> to process </param>
            <param name="hyphenator">
                     the hyphenation pattern tree to use for hyphenation </param>
            <param name="dictionary">
                     the word dictionary to match against. </param>
            <param name="minWordSize">
                     only words longer than this get processed </param>
            <param name="minSubwordSize">
                     only subwords longer than this get to the output stream </param>
            <param name="maxSubwordSize">
                     only subwords shorter than this get to the output stream </param>
            <param name="onlyLongestMatch">
                     Add only the longest matching subword to the stream </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree,System.Int32,System.Int32,System.Int32)">
            <summary>
            Create a <see cref="T:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter"/> with no dictionary.
            <para>
            Calls <see cref="M:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree,Lucene.Net.Analysis.Util.CharArraySet,System.Int32,System.Int32,System.Int32,System.Boolean)"/>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree)">
            <summary>
            Create a <see cref="T:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter"/> with no dictionary.
            <para>
            Calls <see cref="M:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree,System.Int32,System.Int32,System.Int32)"/>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter.GetHyphenationTree(System.String)">
            <summary>
            Create a hyphenator tree
            </summary>
            <param name="hyphenationFilename"> the filename of the XML grammar to load </param>
            <returns> An object representing the hyphenation patterns </returns>
            <exception cref="T:System.IO.IOException"> If there is a low-level I/O error. </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter.GetHyphenationTree(System.String,System.Text.Encoding)">
            <summary>
            Create a hyphenator tree
            </summary>
            <param name="hyphenationFilename"> the filename of the XML grammar to load </param>
            <param name="encoding">The character encoding to use</param>
            <returns> An object representing the hyphenation patterns </returns>
            <exception cref="T:System.IO.IOException"> If there is a low-level I/O error. </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter.GetHyphenationTree(System.IO.FileInfo)">
            <summary>
            Create a hyphenator tree
            </summary>
            <param name="hyphenationFile"> the file of the XML grammar to load </param>
            <returns> An object representing the hyphenation patterns </returns>
            <exception cref="T:System.IO.IOException"> If there is a low-level I/O error. </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter.GetHyphenationTree(System.IO.FileInfo,System.Text.Encoding)">
            <summary>
            Create a hyphenator tree
            </summary>
            <param name="hyphenationFile"> the file of the XML grammar to load </param>
            <param name="encoding">The character encoding to use</param>
            <returns> An object representing the hyphenation patterns </returns>
            <exception cref="T:System.IO.IOException"> If there is a low-level I/O error. </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter.GetHyphenationTree(System.IO.Stream)">
            <summary>
            Create a hyphenator tree
            </summary>
            <param name="hyphenationSource"> the InputSource pointing to the XML grammar </param>
            <returns> An object representing the hyphenation patterns </returns>
            <exception cref="T:System.IO.IOException"> If there is a low-level I/O error. </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter.GetHyphenationTree(System.IO.Stream,System.Text.Encoding)">
            <summary>
            Create a hyphenator tree
            </summary>
            <param name="hyphenationSource"> the InputSource pointing to the XML grammar </param>
            <param name="encoding">The character encoding to use</param>
            <returns> An object representing the hyphenation patterns </returns>
            <exception cref="T:System.IO.IOException"> If there is a low-level I/O error. </exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter"/>.
            <para/>
            This factory accepts the following parameters:
            <list type="bullet">
                <item><description><code>hyphenator</code> (mandatory): path to the FOP xml hyphenation pattern. 
                See <a href="http://offo.sourceforge.net/hyphenation/">http://offo.sourceforge.net/hyphenation/</a>.</description></item>
                <item><description><code>encoding</code> (optional): encoding of the xml hyphenation file. defaults to UTF-8.</description></item>
                <item><description><code>dictionary</code> (optional): dictionary of words. defaults to no dictionary.</description></item>
                <item><description><code>minWordSize</code> (optional): minimal word length that gets decomposed. defaults to 5.</description></item>
                <item><description><code>minSubwordSize</code> (optional): minimum length of subwords. defaults to 2.</description></item>
                <item><description><code>maxSubwordSize</code> (optional): maximum length of subwords. defaults to 15.</description></item>
                <item><description><code>onlyLongestMatch</code> (optional): if true, adds only the longest matching subword 
                to the stream. defaults to false.</description></item>
            </list>
            <para>
            <code>
            &lt;fieldType name="text_hyphncomp" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.HyphenationCompoundWordTokenFilterFactory" hyphenator="hyphenator.xml" encoding="UTF-8"
                    dictionary="dictionary.txt" minWordSize="5" minSubwordSize="2" maxSubwordSize="15" onlyLongestMatch="false"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </para>
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilter"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Compound.HyphenationCompoundWordTokenFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Compound.Hyphenation.ByteVector">
            <summary>
            This class implements a simple byte vector with access to the underlying
            array.
            This class has been taken from the Apache FOP project (http://xmlgraphics.apache.org/fop/). They have been slightly modified. 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.ByteVector.DEFAULT_BLOCK_SIZE">
            <summary>
            Capacity increment size
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.ByteVector.array">
            <summary>
            The encapsulated array
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.ByteVector.n">
            <summary>
            Points to next free item
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Compound.Hyphenation.ByteVector.Item(System.Int32)">
            <summary>
            LUCENENET indexer for .NET
            </summary>
            <param name="index"></param>
            <returns></returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Compound.Hyphenation.ByteVector.Length">
            <summary>
            return number of items in array
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Compound.Hyphenation.ByteVector.Capacity">
            <summary>
            returns current capacity of array
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.ByteVector.Alloc(System.Int32)">
            <summary>
            This is to implement memory allocation in the array. Like malloc().
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Compound.Hyphenation.CharVector">
            <summary>
            This class implements a simple char vector with access to the underlying
            array.
            
            This class has been taken from the Apache FOP project (http://xmlgraphics.apache.org/fop/). They have been slightly modified. 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.CharVector.DEFAULT_BLOCK_SIZE">
            <summary>
            Capacity increment size
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.CharVector.array">
            <summary>
            The encapsulated array
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.CharVector.n">
            <summary>
            Points to next free item
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.CharVector.Clear">
            <summary>
            Reset Vector but don't resize or clear elements
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Compound.Hyphenation.CharVector.Item(System.Int32)">
            <summary>
            LUCENENET indexer for .NET
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Compound.Hyphenation.CharVector.Length">
            <summary>
            return number of items in array
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Compound.Hyphenation.CharVector.Capacity">
            <summary>
            returns current capacity of array
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Compound.Hyphenation.Hyphen">
            <summary>
            This class represents a hyphen. A 'full' hyphen is made of 3 parts: the
            pre-break text, post-break text and no-break. If no line-break is generated
            at this position, the no-break text is used, otherwise, pre-break and
            post-break are used. Typically, pre-break is equal to the hyphen character
            and the others are empty. However, this general scheme allows support for
            cases in some languages where words change spelling if they're split across
            lines, like german's 'backen' which hyphenates 'bak-ken'. BTW, this comes
            from TeX.
            <para/>
            This class has been taken from the Apache FOP project (http://xmlgraphics.apache.org/fop/). They have been slightly modified. 
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Compound.Hyphenation.Hyphenation">
            <summary>
            This class represents a hyphenated word.
            <para/>
            This class has been taken from the Apache FOP project (http://xmlgraphics.apache.org/fop/). They have been slightly modified.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.Hyphenation.#ctor(System.Int32[])">
            <summary>
            rawWord as made of alternating strings and <see cref="T:Lucene.Net.Analysis.Compound.Hyphenation.Hyphen"/> instances
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Compound.Hyphenation.Hyphenation.Length">
            <summary> the number of hyphenation points in the word </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Compound.Hyphenation.Hyphenation.HyphenationPoints">
            <summary> the hyphenation points </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree">
            <summary>
            This tree structure stores the hyphenation patterns in an efficient way for
            fast lookup. It provides the provides the method to hyphenate a word.
            <para/>
            This class has been taken from the Apache FOP project (http://xmlgraphics.apache.org/fop/). They have been slightly modified. 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.m_vspace">
            <summary>
            value space: stores the interletter values
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.m_stoplist">
            <summary>
            This map stores hyphenation exceptions
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.m_classmap">
            <summary>
            This map stores the character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.ivalues">
            <summary>
            Temporary map to store interletter values on pattern loading.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.PackValues(System.String)">
            <summary>
            Packs the values by storing them in 4 bits, two values into a byte Values
            range is from 0 to 9. We use zero as terminator, so we'll add 1 to the
            value.
            </summary>
            <param name="values"> a string of digits from '0' to '9' representing the
                   interletter values. </param>
            <returns> the index into the vspace array where the packed values are stored. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.LoadPatterns(System.String)">
            <summary>
            Read hyphenation patterns from an XML file.
            </summary>
            <param name="filename"> the filename </param>
            <exception cref="T:System.IO.IOException"> In case the parsing fails </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.LoadPatterns(System.String,System.Text.Encoding)">
            <summary>
            Read hyphenation patterns from an XML file.
            </summary>
            <param name="filename"> the filename </param>
            <param name="encoding">The character encoding to use</param>
            <exception cref="T:System.IO.IOException"> In case the parsing fails </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.LoadPatterns(System.IO.FileInfo)">
            <summary>
            Read hyphenation patterns from an XML file.
            </summary>
            <param name="f"> a <see cref="T:System.IO.FileInfo"/> object representing the file </param>
            <exception cref="T:System.IO.IOException"> In case the parsing fails </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.LoadPatterns(System.IO.FileInfo,System.Text.Encoding)">
            <summary>
            Read hyphenation patterns from an XML file.
            </summary>
            <param name="f"> a <see cref="T:System.IO.FileInfo"/> object representing the file </param>
            <param name="encoding">The character encoding to use</param>
            <exception cref="T:System.IO.IOException"> In case the parsing fails </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.LoadPatterns(System.IO.Stream)">
            <summary>
            Read hyphenation patterns from an XML file.
            </summary>
            <param name="source"> <see cref="T:System.IO.Stream"/> input source for the file </param>
            <exception cref="T:System.IO.IOException"> In case the parsing fails </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.LoadPatterns(System.IO.Stream,System.Text.Encoding)">
            <summary>
            Read hyphenation patterns from an XML file.
            </summary>
            <param name="source"> <see cref="T:System.IO.Stream"/> input source for the file </param>
            <param name="encoding">The character encoding to use</param>
            <exception cref="T:System.IO.IOException"> In case the parsing fails </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.LoadPatterns(System.Xml.XmlReader)">
            <summary>
            Read hyphenation patterns from an <see cref="T:System.Xml.XmlReader"/>.
            </summary>
            <param name="source"> <see cref="T:System.Xml.XmlReader"/> input source for the file </param>
            <exception cref="T:System.IO.IOException"> In case the parsing fails </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.HStrCmp(System.Char[],System.Int32,System.Char[],System.Int32)">
            <summary>
            String compare, returns 0 if equal or t is a substring of s
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.SearchPatterns(System.Char[],System.Int32,System.Byte[])">
            <summary>
            <para>
            Search for all possible partial matches of word starting at index an update
            interletter values. In other words, it does something like:
            </para>
            <code>
            for (i=0; i&lt;patterns.Length; i++) 
            {
                if (word.Substring(index).StartsWith(patterns[i], StringComparison.Ordinal))
                    update_interletter_values(patterns[i]);
            }
            </code>
            <para>
            But it is done in an efficient way since the patterns are stored in a
            ternary tree. In fact, this is the whole purpose of having the tree: doing
            this search without having to test every single pattern. The number of
            patterns for languages such as English range from 4000 to 10000. Thus,
            doing thousands of string comparisons for each word to hyphenate would be
            really slow without the tree. The tradeoff is memory, but using a ternary
            tree instead of a trie, almost halves the the memory used by Lout or TeX.
            It's also faster than using a hash table
            </para>
            </summary>
            <param name="word"> null terminated word to match </param>
            <param name="index"> start index from word </param>
            <param name="il"> interletter values array to update </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.Hyphenate(System.String,System.Int32,System.Int32)">
            <summary>
            Hyphenate word and return a <see cref="T:Lucene.Net.Analysis.Compound.Hyphenation.Hyphenation"/> object.
            </summary>
            <param name="word"> the word to be hyphenated </param>
            <param name="remainCharCount"> Minimum number of characters allowed before the
                   hyphenation point. </param>
            <param name="pushCharCount"> Minimum number of characters allowed after the
                   hyphenation point. </param>
            <returns> a <see cref="T:Lucene.Net.Analysis.Compound.Hyphenation.Hyphenation"/> object representing the
                    hyphenated word or null if word is not hyphenated. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.Hyphenate(System.Char[],System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Hyphenate word and return an array of hyphenation points.
            </summary>
            <remarks>
            w = "****nnllllllnnn*****", where n is a non-letter, l is a letter, all n
            may be absent, the first n is at offset, the first l is at offset +
            iIgnoreAtBeginning; word = ".llllll.'\0'***", where all l in w are copied
            into word. In the first part of the routine len = w.length, in the second
            part of the routine len = word.length. Three indices are used: index(w),
            the index in w, index(word), the index in word, letterindex(word), the
            index in the letter part of word. The following relations exist: index(w) =
            offset + i - 1 index(word) = i - iIgnoreAtBeginning letterindex(word) =
            index(word) - 1 (see first loop). It follows that: index(w) - index(word) =
            offset - 1 + iIgnoreAtBeginning index(w) = letterindex(word) + offset +
            iIgnoreAtBeginning
            </remarks>
            <param name="w"> char array that contains the word </param>
            <param name="offset"> Offset to first character in word </param>
            <param name="len"> Length of word </param>
            <param name="remainCharCount"> Minimum number of characters allowed before the
                   hyphenation point. </param>
            <param name="pushCharCount"> Minimum number of characters allowed after the
                   hyphenation point. </param>
            <returns> a <see cref="T:Lucene.Net.Analysis.Compound.Hyphenation.Hyphenation"/> object representing the
                    hyphenated word or null if word is not hyphenated. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.AddClass(System.String)">
            <summary>
            Add a character class to the tree. It is used by
            <see cref="T:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser"/> as callback to add character classes.
            Character classes define the valid word characters for hyphenation. If a
            word contains a character not defined in any of the classes, it is not
            hyphenated. It also defines a way to normalize the characters in order to
            compare them with the stored patterns. Usually pattern files use only lower
            case characters, in this case a class for letter 'a', for example, should
            be defined as "aA", the first character being the normalization char.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.AddException(System.String,System.Collections.Generic.IList{System.Object})">
            <summary>
            Add an exception to the tree. It is used by
            <see cref="T:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser"/> class as callback to store the
            hyphenation exceptions.
            </summary>
            <param name="word"> normalized word </param>
            <param name="hyphenatedword"> a vector of alternating strings and
                   <see cref="T:Lucene.Net.Analysis.Compound.Hyphenation.Hyphen"/> objects. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.HyphenationTree.AddPattern(System.String,System.String)">
            <summary>
            Add a pattern to the tree. Mainly, to be used by
            <see cref="T:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser"/> class as callback to add a pattern to
            the tree.
            </summary>
            <param name="pattern"> the hyphenation pattern </param>
            <param name="ivalue"> interletter weight values indicating the desirability and
                   priority of hyphenating at a given point within the pattern. It
                   should contain only digit characters. (i.e. '0' to '9'). </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Compound.Hyphenation.IPatternConsumer">
            <summary>
            This interface is used to connect the XML pattern file parser to the
            hyphenation tree.
            <para/>
            This interface has been taken from the Apache FOP project (http://xmlgraphics.apache.org/fop/). They have been slightly modified.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.IPatternConsumer.AddClass(System.String)">
            <summary>
            Add a character class. A character class defines characters that are
            considered equivalent for the purpose of hyphenation (e.g. "aA"). It
            usually means to ignore case.
            </summary>
            <param name="chargroup"> character group </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.IPatternConsumer.AddException(System.String,System.Collections.Generic.IList{System.Object})">
            <summary>
            Add a hyphenation exception. An exception replaces the result obtained by
            the algorithm for cases for which this fails or the user wants to provide
            his own hyphenation. A hyphenatedword is a vector of alternating String's
            and <see cref="T:Lucene.Net.Analysis.Compound.Hyphenation.Hyphen"/> instances
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.IPatternConsumer.AddPattern(System.String,System.String)">
            <summary>
            Add hyphenation patterns.
            </summary>
            <param name="pattern"> the pattern </param>
            <param name="values"> interletter values expressed as a string of digit characters. </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser">
            <summary>
            A XMLReader document handler to read and parse hyphenation patterns from a XML
            file.
            <para/>
            LUCENENET: This class has been refactored from its Java counterpart to use XmlReader rather
            than a SAX parser.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser.Parse(System.String)">
            <summary>
            Parses a hyphenation pattern file.
            </summary>
            <param name="path">The complete file path to be read.</param>
            <exception cref="T:System.IO.IOException"> In case of an exception while parsing </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser.Parse(System.String,System.Text.Encoding)">
            <summary>
            Parses a hyphenation pattern file.
            </summary>
            <param name="path">The complete file path to be read.</param>
            <param name="encoding">The character encoding to use</param>
            <exception cref="T:System.IO.IOException"> In case of an exception while parsing </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser.Parse(System.IO.FileInfo)">
            <summary>
            Parses a hyphenation pattern file.
            </summary>
            <param name="file">  a <see cref="T:System.IO.FileInfo"/> object representing the file  </param>
            <exception cref="T:System.IO.IOException"> In case of an exception while parsing </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser.Parse(System.IO.FileInfo,System.Text.Encoding)">
            <summary>
            Parses a hyphenation pattern file.
            </summary>
            <param name="file">  a <see cref="T:System.IO.FileInfo"/> object representing the file </param>
            <param name="encoding">The character encoding to use</param>
            <exception cref="T:System.IO.IOException"> In case of an exception while parsing </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser.Parse(System.IO.Stream)">
            <summary>
            Parses a hyphenation pattern file.
            </summary>
            <param name="xmlStream">
            The stream containing the XML data.
            <para/>
            The <see cref="T:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser"/> scans the first bytes of the stream looking for a byte order mark 
            or other sign of encoding. When encoding is determined, the encoding is used to continue reading 
            the stream, and processing continues parsing the input as a stream of (Unicode) characters.
            </param>
            <exception cref="T:System.IO.IOException"> In case of an exception while parsing </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser.Parse(System.Xml.XmlReader)">
            <summary>
            Parses a hyphenation pattern file.
            </summary>
            <param name="source"> <see cref="T:System.Xml.XmlReader"/> input source for the file </param>
            <exception cref="T:System.IO.IOException"> In case of an exception while parsing </exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser.DtdResolver">
            <summary>
            LUCENENET specific helper class to force the DTD file to be read from the embedded resource
            rather than from the file system.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser.StartElement(System.String,System.String,System.String,System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Receive notification of the beginning of an element.
            <para/>
            The Parser will invoke this method at the beginning of every element in the XML document; 
            there will be a corresponding <see cref="M:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser.EndElement(System.String,System.String,System.String)"/> event for every <see cref="M:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser.StartElement(System.String,System.String,System.String,System.Collections.Generic.IDictionary{System.String,System.String})"/> event 
            (even when the element is empty). All of the element's content will be reported, 
            in order, before the corresponding endElement event.
            </summary>
            <param name="uri">the Namespace URI, or the empty string if the element has no Namespace URI or if Namespace processing is not being performed</param>
            <param name="local">the local name (without prefix), or the empty string if Namespace processing is not being performed</param>
            <param name="raw"></param>
            <param name="attrs"> the attributes attached to the element. If there are no attributes, it shall be an empty Attributes object. The value of this object after startElement returns is undefined</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser.EndElement(System.String,System.String,System.String)">
            <summary>
            Receive notification of the end of an element.
            <para/>
            The parser will invoke this method at the end of every element in the XML document; 
            there will be a corresponding <see cref="M:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser.StartElement(System.String,System.String,System.String,System.Collections.Generic.IDictionary{System.String,System.String})"/> event for every 
            <see cref="M:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser.EndElement(System.String,System.String,System.String)"/> event (even when the element is empty).
            </summary>
            <param name="uri">the Namespace URI, or the empty string if the element has no Namespace URI or if Namespace processing is not being performed</param>
            <param name="local">the local name (without prefix), or the empty string if Namespace processing is not being performed</param>
            <param name="raw"></param>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.PatternParser.Characters(System.Char[],System.Int32,System.Int32)">
            <summary>
            Receive notification of character data.
            <para/>
            The Parser will call this method to report each chunk of character data. Parsers may 
            return all contiguous character data in a single chunk, or they may split it into 
            several chunks; however, all of the characters in any single event must come from 
            the same external entity so that the Locator provides useful information.
            <para/>
            The application must not attempt to read from the array outside of the specified range.
            </summary>
            <param name="ch"></param>
            <param name="start"></param>
            <param name="length"></param>
        </member>
        <member name="T:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree">
            <summary>
            <h2>Ternary Search Tree.</h2>
            
            <para>
            A ternary search tree is a hybrid between a binary tree and a digital search
            tree (trie). Keys are limited to strings. A data value of type char is stored
            in each leaf node. It can be used as an index (or pointer) to the data.
            Branches that only contain one key are compressed to one node by storing a
            pointer to the trailer substring of the key. This class is intended to serve
            as base class or helper class to implement Dictionary collections or the
            like. Ternary trees have some nice properties as the following: the tree can
            be traversed in sorted order, partial matches (wildcard) can be implemented,
            retrieval of all keys within a given distance from the target, etc. The
            storage requirements are higher than a binary tree but a lot less than a
            trie. Performance is comparable with a hash table, sometimes it outperforms a
            hash function (most of the time can determine a miss faster than a hash).
            </para>
            
            <para>
            The main purpose of this java port is to serve as a base for implementing
            TeX's hyphenation algorithm (see The TeXBook, appendix H). Each language
            requires from 5000 to 15000 hyphenation patterns which will be keys in this
            tree. The strings patterns are usually small (from 2 to 5 characters), but
            each char in the tree is stored in a node. Thus memory usage is the main
            concern. We will sacrifice 'elegance' to keep memory requirements to the
            minimum. Using java's char type as pointer (yes, I know pointer it is a
            forbidden word in java) we can keep the size of the node to be just 8 bytes
            (3 pointers and the data char). This gives room for about 65000 nodes. In my
            tests the english patterns took 7694 nodes and the german patterns 10055
            nodes, so I think we are safe.
            </para>
            
            <para>
            All said, this is a map with strings as keys and char as value. Pretty
            limited!. It can be extended to a general map by using the string
            representation of an object and using the char value as an index to an array
            that contains the object values.
            </para>
            
            This class has been taken from the Apache FOP project (http://xmlgraphics.apache.org/fop/). They have been slightly modified. 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.m_lo">
            <summary>
            Pointer to low branch and to rest of the key when it is stored directly in
            this node, we don't have unions in java!
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.m_hi">
            <summary>
            Pointer to high branch.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.m_eq">
            <summary>
            Pointer to equal branch and to data when this node is a string terminator.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.m_sc">
            <summary>
            <para>
            The character stored in this node: splitchar. Two special values are
            reserved:
            </para>
            <list type="bullet">
                <item><description>0x0000 as string terminator</description></item>
                <item><description>0xFFFF to indicate that the branch starting at this node is compressed</description></item>
            </list>
            <para>
            This shouldn't be a problem if we give the usual semantics to strings since
            0xFFFF is guaranteed not to be an Unicode character.
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.m_kv">
            <summary>
            This vector holds the trailing of the keys when the branch is compressed.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.Insert(System.String,System.Char)">
            <summary>
            Branches are initially compressed, needing one node per key plus the size
            of the string key. They are decompressed as needed when another key with
            same prefix is inserted. This saves a lot of space, specially for long
            keys.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.Insert(System.Char,System.Char[],System.Int32,System.Char)">
            <summary>
            The actual insertion function, recursive version.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.StrCmp(System.Char[],System.Int32,System.Char[],System.Int32)">
            <summary>
            Compares 2 null terminated char arrays
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.StrCmp(System.String,System.Char[],System.Int32)">
            <summary>
            Compares a string with null terminated char array
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.InsertBalanced(System.String[],System.Char[],System.Int32,System.Int32)">
            <summary>
            Recursively insert the median first and then the median of the lower and
            upper halves, and so on in order to get a balanced tree. The array of keys
            is assumed to be sorted in ascending order.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.Balance">
            <summary>
            Balance the tree for best search performance
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.TrimToSize">
            <summary>
            Each node stores a character (splitchar) which is part of some key(s). In a
            compressed branch (one that only contain a single string key) the trailer
            of the key which is not already in nodes is stored externally in the kv
            array. As items are inserted, key substrings decrease. Some substrings may
            completely disappear when the whole branch is totally decompressed. The
            tree is traversed to find the key substrings actually used. In addition,
            duplicate substrings are removed using a map (implemented with a
            TernaryTree!).
            
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.Iterator">
            <summary>
            Enumerator for TernaryTree
            
            LUCENENET NOTE: This differs a bit from its Java counterpart to adhere to
            .NET IEnumerator semantics. In Java, when the <see cref="T:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.Iterator"/> is
            instantiated, it is already positioned at the first element. However,
            to act like a .NET IEnumerator, the initial state is undefined and considered
            to be before the first element until <see cref="M:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.Iterator.MoveNext"/> is called, and
            if a move took place it will return <c>true</c>;
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.Iterator.cur">
            <summary>
            current node index
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.Iterator.curkey">
            <summary>
            current key
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.Iterator.ns">
            <summary>
            Node stack
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.Iterator.ks">
            <summary>
            key stack implemented with a <see cref="T:System.Text.StringBuilder"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.Iterator.Up">
            <summary>
            traverse upwards
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Compound.Hyphenation.TernaryTree.Iterator.Run">
            <summary>
            traverse the tree to find next key
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.KeywordAnalyzer">
            <summary>
            "Tokenizes" the entire stream as a single token. This is useful
            for data like zip codes, ids, and some product names.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.KeywordTokenizer">
            <summary>
            Emits the entire input as a single token.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Core.KeywordTokenizer.DEFAULT_BUFFER_SIZE">
            <summary>
            Default read buffer size </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.KeywordTokenizerFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Core.KeywordTokenizer"/>. 
            <code>
            &lt;fieldType name="text_keyword" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.KeywordTokenizerFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code> 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.KeywordTokenizerFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Core.KeywordTokenizerFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.LetterTokenizer">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.Core.LetterTokenizer"/> is a tokenizer that divides text at non-letters. That's to
            say, it defines tokens as maximal strings of adjacent letters, as defined by
            <see cref="M:System.Char.IsLetter(System.Char)"/> predicate.
            <para>
            Note: this does a decent job for most European languages, but does a terrible
            job for some Asian languages, where words are not separated by spaces.
            </para>
            <para>
            You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/> compatibility when creating
            <see cref="T:Lucene.Net.Analysis.Core.LetterTokenizer"/>:
            <list type="bullet">
                <item><description>As of 3.1, <see cref="T:Lucene.Net.Analysis.Util.CharTokenizer"/> uses an <see cref="T:System.Int32"/> based API to normalize and
                detect token characters. See <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.IsTokenChar(System.Int32)"/> and
                <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.Normalize(System.Int32)"/> for details.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.LetterTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader)">
            <summary>
            Construct a new <see cref="T:Lucene.Net.Analysis.Core.LetterTokenizer"/>.
            </summary>
            <param name="matchVersion">
                     <see cref="T:Lucene.Net.Util.LuceneVersion"/> to match. </param>
            <param name="in">
                     the input to split up into tokens </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.LetterTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader)">
            <summary>
            Construct a new <see cref="T:Lucene.Net.Analysis.Core.LetterTokenizer"/> using a given
            <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/>.
            </summary>
            <param name="matchVersion">
                     <see cref="T:Lucene.Net.Util.LuceneVersion"/> to match</param>
            <param name="factory">
                     the attribute factory to use for this <see cref="T:Lucene.Net.Analysis.Tokenizer"/> </param>
            <param name="in">
                     the input to split up into tokens </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.LetterTokenizer.IsTokenChar(System.Int32)">
            <summary>
            Collects only characters which satisfy
            <see cref="M:J2N.Character.IsLetter(System.Int32)"/>.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.LetterTokenizerFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Core.LetterTokenizer"/>. 
            <code>
            &lt;fieldType name="text_letter" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.LetterTokenizerFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.LetterTokenizerFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Core.LetterTokenizerFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.LowerCaseFilter">
            <summary>
            Normalizes token text to lower case.
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating LowerCaseFilter:
            <list type="bullet">
                <item><description> As of 3.1, supplementary characters are properly lowercased.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.LowerCaseFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, that normalizes token text to lower case.
            </summary>
            <param name="matchVersion"> See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="in"> <see cref="T:Lucene.Net.Analysis.TokenStream"/> to filter </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.LowerCaseFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>. 
            <code>
            &lt;fieldType name="text_lwrcase" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.LowerCaseFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.LowerCaseTokenizer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Core.LowerCaseTokenizer"/> performs the function of <see cref="T:Lucene.Net.Analysis.Core.LetterTokenizer"/>
            and <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/> together.  It divides text at non-letters and converts
            them to lower case.  While it is functionally equivalent to the combination
            of <see cref="T:Lucene.Net.Analysis.Core.LetterTokenizer"/> and <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, there is a performance advantage
            to doing the two tasks at once, hence this (redundant) implementation.
            <para>
            Note: this does a decent job for most European languages, but does a terrible
            job for some Asian languages, where words are not separated by spaces.
            </para>
            <para>
            You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/> compatibility when creating
            <see cref="T:Lucene.Net.Analysis.Core.LowerCaseTokenizer"/>:
            <list type="bullet">
                <item><description>As of 3.1, <see cref="T:Lucene.Net.Analysis.Util.CharTokenizer"/> uses an int based API to normalize and
                detect token characters. See <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.IsTokenChar(System.Int32)"/> and
                <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.Normalize(System.Int32)"/> for details.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.LowerCaseTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader)">
            <summary>
            Construct a new <see cref="T:Lucene.Net.Analysis.Core.LowerCaseTokenizer"/>.
            </summary>
            <param name="matchVersion">
                     <see cref="T:Lucene.Net.Util.LuceneVersion"/> to match
            </param>
            <param name="in">
                     the input to split up into tokens </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.LowerCaseTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader)">
            <summary>
            Construct a new <see cref="T:Lucene.Net.Analysis.Core.LowerCaseTokenizer"/> using a given
            <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/>.
            </summary>
            <param name="matchVersion">
                     <see cref="T:Lucene.Net.Util.LuceneVersion"/> to match </param>
            <param name="factory">
                     the attribute factory to use for this <see cref="T:Lucene.Net.Analysis.Tokenizer"/> </param>
            <param name="in">
                     the input to split up into tokens </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.LowerCaseTokenizer.Normalize(System.Int32)">
            <summary>
            Converts char to lower case
            <see cref="M:J2N.Character.ToLower(System.Int32)"/>.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.LowerCaseTokenizerFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Core.LowerCaseTokenizer"/>. 
            <code>
            &lt;fieldType name="text_lwrcase" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.LowerCaseTokenizerFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.LowerCaseTokenizerFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Core.LowerCaseTokenizerFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.SimpleAnalyzer">
            <summary>
            An <see cref="T:Lucene.Net.Analysis.Analyzer"/> that filters <see cref="T:Lucene.Net.Analysis.Core.LetterTokenizer"/> 
             with <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/> 
            <para>
            You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/> compatibility
            when creating <see cref="T:Lucene.Net.Analysis.Util.CharTokenizer"/>:
            <list type="bullet">
                <item><description>As of 3.1, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseTokenizer"/> uses an int based API to normalize and
                detect token codepoints. See <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.IsTokenChar(System.Int32)"/> and
                <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.Normalize(System.Int32)"/> for details.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.SimpleAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Core.SimpleAnalyzer"/> </summary>
            <param name="matchVersion"> <see cref="T:Lucene.Net.Util.LuceneVersion"/> to match </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.StopAnalyzer">
            <summary>
            Filters <see cref="T:Lucene.Net.Analysis.Core.LetterTokenizer"/> with <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/> and <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>.
            <para>
            You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.Core.StopAnalyzer"/>:
            <list type="bullet">
                <item><description> As of 3.1, StopFilter correctly handles Unicode 4.0
                    supplementary characters in stopwords</description></item>
                <item><description> As of 2.9, position increments are preserved</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Core.StopAnalyzer.ENGLISH_STOP_WORDS_SET">
            <summary>
            An unmodifiable set containing some common English words that are not usually useful
            for searching.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.StopAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer which removes words in
            <see cref="F:Lucene.Net.Analysis.Core.StopAnalyzer.ENGLISH_STOP_WORDS_SET"/>. </summary>
            <param name="matchVersion"> See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.StopAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the stop words from the given set. </summary>
            <param name="matchVersion"> See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="stopWords"> Set of stop words  </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.StopAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.FileInfo)">
            <summary>
            Builds an analyzer with the stop words from the given file. </summary>
            <seealso cref="M:Lucene.Net.Analysis.Util.WordlistLoader.GetWordSet(System.IO.TextReader,Lucene.Net.Util.LuceneVersion)"/>
            <param name="matchVersion"> See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="stopwordsFile"> File to load stop words from  </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.StopAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader)">
            <summary>
            Builds an analyzer with the stop words from the given reader. </summary>
            <seealso cref="M:Lucene.Net.Analysis.Util.WordlistLoader.GetWordSet(System.IO.TextReader,Lucene.Net.Util.LuceneVersion)"/>
            <param name="matchVersion"> See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="stopwords"> <see cref="T:System.IO.TextReader"/> to load stop words from  </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.StopAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            used to tokenize all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from a <see cref="T:Lucene.Net.Analysis.Core.LowerCaseTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/> </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.StopFilter">
            <summary>
            Removes stop words from a token stream.
            <para>
            You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>:
            <list type="bullet">
                <item><description>As of 3.1, StopFilter correctly handles Unicode 4.0
                    supplementary characters in stopwords and position
                    increments are preserved</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.StopFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Constructs a filter which removes words from the input <see cref="T:Lucene.Net.Analysis.TokenStream"/> that are
            named in the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.
            </summary>
            <param name="matchVersion">
                     Lucene version to enable correct Unicode 4.0 behavior in the stop
                     set if Version > 3.0.  See <see cref="T:Lucene.Net.Util.LuceneVersion"/>> for details. </param>
            <param name="in">
                     Input <see cref="T:Lucene.Net.Analysis.TokenStream"/> </param>
            <param name="stopWords">
                     A <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> representing the stopwords. </param>
            <seealso cref="M:Lucene.Net.Analysis.Core.StopFilter.MakeStopSet(Lucene.Net.Util.LuceneVersion,System.String[])"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.StopFilter.MakeStopSet(Lucene.Net.Util.LuceneVersion,System.String[])">
            <summary>
            Builds a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> from an array of stop words,
            appropriate for passing into the <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/> constructor.
            This permits this <paramref name="stopWords"/> construction to be cached once when
            an <see cref="T:Lucene.Net.Analysis.Analyzer"/> is constructed.
            </summary>
            <param name="matchVersion"> <see cref="T:Lucene.Net.Util.LuceneVersion"/> to enable correct Unicode 4.0 behavior in the returned set if Version > 3.0 </param>
            <param name="stopWords"> An array of stopwords </param>
            <seealso cref="M:Lucene.Net.Analysis.Core.StopFilter.MakeStopSet(Lucene.Net.Util.LuceneVersion,System.String[],System.Boolean)"/> passing false to ignoreCase
        </member>
        <member name="M:Lucene.Net.Analysis.Core.StopFilter.MakeStopSet``1(Lucene.Net.Util.LuceneVersion,System.Collections.Generic.IList{``0})">
            <summary>
            Builds a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> from an array of stop words,
            appropriate for passing into the <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/> constructor.
            This permits this <paramref name="stopWords"/> construction to be cached once when
            an <see cref="T:Lucene.Net.Analysis.Analyzer"/> is constructed.
            </summary>
            <param name="matchVersion"> <see cref="T:Lucene.Net.Util.LuceneVersion"/> to enable correct Unicode 4.0 behavior in the returned set if Version > 3.0 </param>
            <param name="stopWords"> A List of <see cref="T:System.String"/>s or <see cref="T:char[]"/> or any other ToString()-able list representing the stopwords </param>
            <returns> A Set (<see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>) containing the words </returns>
            <seealso cref="M:Lucene.Net.Analysis.Core.StopFilter.MakeStopSet(Lucene.Net.Util.LuceneVersion,System.String[],System.Boolean)"/> passing false to ignoreCase
        </member>
        <member name="M:Lucene.Net.Analysis.Core.StopFilter.MakeStopSet(Lucene.Net.Util.LuceneVersion,System.String[],System.Boolean)">
            <summary>
            Creates a stopword set from the given stopword array.
            </summary>
            <param name="matchVersion"> <see cref="T:Lucene.Net.Util.LuceneVersion"/> to enable correct Unicode 4.0 behavior in the returned set if Version > 3.0 </param>
            <param name="stopWords"> An array of stopwords </param>
            <param name="ignoreCase"> If true, all words are lower cased first. </param>
            <returns> a Set (<see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>) containing the words </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.StopFilter.MakeStopSet``1(Lucene.Net.Util.LuceneVersion,System.Collections.Generic.IList{``0},System.Boolean)">
            <summary>
            Creates a stopword set from the given stopword list. </summary>
            <param name="matchVersion"> <see cref="T:Lucene.Net.Util.LuceneVersion"/> to enable correct Unicode 4.0 behavior in the returned set if Version > 3.0 </param>
            <param name="stopWords"> A List of <see cref="T:System.String"/>s or <see cref="T:char[]"/> or any other ToString()-able list representing the stopwords </param>
            <param name="ignoreCase"> if true, all words are lower cased first </param>
            <returns> A Set (<see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>) containing the words </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.StopFilter.Accept">
            <summary>
            Returns the next input Token whose Term is not a stop word.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.StopFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>.
            
            <code>
            &lt;fieldType name="text_stop" class="solr.TextField" positionIncrementGap="100" autoGeneratePhraseQueries="true"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.StopFilterFactory" ignoreCase="true"
                        words="stopwords.txt" format="wordset" /&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            <para>
            All attributes are optional:
            </para>
            <list type="bullet">
                <item><description><c>ignoreCase</c> defaults to <c>false</c></description></item>
                <item><description><c>words</c> should be the name of a stopwords file to parse, if not 
                 specified the factory will use <see cref="F:Lucene.Net.Analysis.Core.StopAnalyzer.ENGLISH_STOP_WORDS_SET"/>
                </description></item>
                <item><description><c>format</c> defines how the <c>words</c> file will be parsed, 
                 and defaults to <c>wordset</c>.  If <c>words</c> is not specified, 
                 then <c>format</c> must not be specified.
                </description></item>
            </list>
            <para>
            The valid values for the <c>format</c> option are:
            </para>
            <list type="bullet">
             <item><description><c>wordset</c> - This is the default format, which supports one word per 
                 line (including any intra-word whitespace) and allows whole line comments 
                 begining with the "#" character.  Blank lines are ignored.  See 
                 <see cref="M:Lucene.Net.Analysis.Util.WordlistLoader.GetLines(System.IO.Stream,System.Text.Encoding)"/> for details.
             </description></item>
             <item><description><c>snowball</c> - This format allows for multiple words specified on each 
                 line, and trailing comments may be specified using the vertical line ("&#124;"). 
                 Blank lines are ignored.  See 
                 <see cref="M:Lucene.Net.Analysis.Util.WordlistLoader.GetSnowballWordSet(System.IO.TextReader,Lucene.Net.Util.LuceneVersion)"/> 
                 for details.
             </description></item>
            </list>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.StopFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Core.StopFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.TypeTokenFilter">
            <summary>
            Removes tokens whose types appear in a set of blocked types from a token stream.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.TypeTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,System.Boolean,Lucene.Net.Analysis.TokenStream,System.Collections.Generic.ICollection{System.String},System.Boolean)">
            @deprecated enablePositionIncrements=false is not supported anymore as of Lucene 4.4. 
        </member>
        <member name="M:Lucene.Net.Analysis.Core.TypeTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,System.Boolean,Lucene.Net.Analysis.TokenStream,System.Collections.Generic.ICollection{System.String})">
            @deprecated enablePositionIncrements=false is not supported anymore as of Lucene 4.4. 
        </member>
        <member name="M:Lucene.Net.Analysis.Core.TypeTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,System.Collections.Generic.ICollection{System.String},System.Boolean)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Core.TypeTokenFilter"/>. </summary>
            <param name="version">      the <see cref="T:Lucene.Net.Util.LuceneVersion"/> match version </param>
            <param name="input">        the <see cref="T:Lucene.Net.Analysis.TokenStream"/> to consume </param>
            <param name="stopTypes">    the types to filter </param>
            <param name="useWhiteList"> if true, then tokens whose type is in <paramref name="stopTypes"/> will
                                be kept, otherwise they will be filtered out </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.TypeTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,System.Collections.Generic.ICollection{System.String})">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Core.TypeTokenFilter"/> that filters tokens out
            (useWhiteList=false). </summary>
            <seealso cref="M:Lucene.Net.Analysis.Core.TypeTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,System.Collections.Generic.ICollection{System.String},System.Boolean)"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.TypeTokenFilter.Accept">
            <summary>
            By default accept the token if its type is not a stop type.
            When the <see cref="F:Lucene.Net.Analysis.Core.TypeTokenFilter.useWhiteList"/> parameter is set to true then accept the token if its type is contained in the <see cref="F:Lucene.Net.Analysis.Core.TypeTokenFilter.stopTypes"/>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.TypeTokenFilterFactory">
            <summary>
            Factory class for <see cref="T:Lucene.Net.Analysis.Core.TypeTokenFilter"/>.
            <code>
            &lt;fieldType name="chars" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.TypeTokenFilterFactory" types="stoptypes.txt"
                              useWhitelist="false"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.TypeTokenFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Core.TypeTokenFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.UpperCaseFilter">
            <summary>
            Normalizes token text to UPPER CASE.
            <para>
            You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.Core.UpperCaseFilter"/>
            </para>
            <para><b>NOTE:</b> In Unicode, this transformation may lose information when the
            upper case character represents more than one lower case character. Use this filter
            when you Require uppercase tokens.  Use the <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/> for 
            general search matching
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.UpperCaseFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Core.UpperCaseFilter"/>, that normalizes token text to upper case.
            </summary>
            <param name="matchVersion"> See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="in"> <see cref="T:Lucene.Net.Analysis.TokenStream"/> to filter </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.UpperCaseFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Core.UpperCaseFilter"/>. 
            <code>
            &lt;fieldType name="text_uppercase" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.UpperCaseFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            
            <para><b>NOTE:</b> In Unicode, this transformation may lose information when the
            upper case character represents more than one lower case character. Use this filter
            when you require uppercase tokens.  Use the <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilterFactory"/> for 
            general search matching
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.UpperCaseFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Core.UpperCaseFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.WhitespaceAnalyzer">
            <summary>
            An <see cref="T:Lucene.Net.Analysis.Analyzer"/> that uses <see cref="T:Lucene.Net.Analysis.Core.WhitespaceTokenizer"/>.
            <para>
            You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/> compatibility
            when creating <see cref="T:Lucene.Net.Analysis.Util.CharTokenizer"/>:
            <list type="bullet">
                <item><description>As of 3.1, <see cref="T:Lucene.Net.Analysis.Core.WhitespaceTokenizer"/> uses an int based API to normalize and
                detect token codepoints. See <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.IsTokenChar(System.Int32)"/> and
                <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.Normalize(System.Int32)"/> for details.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.WhitespaceAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Core.WhitespaceAnalyzer"/> </summary>
            <param name="matchVersion"> <see cref="T:Lucene.Net.Util.LuceneVersion"/> to match </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.WhitespaceTokenizer">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.Core.WhitespaceTokenizer"/> is a tokenizer that divides text at whitespace.
            Adjacent sequences of non-Whitespace characters form tokens.
            <para>
            You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/> compatibility when creating
            <see cref="T:Lucene.Net.Analysis.Core.WhitespaceTokenizer"/>:
            <list type="bullet">
                <item><description>As of 3.1, <see cref="T:Lucene.Net.Analysis.Util.CharTokenizer"/> uses an int based API to normalize and
                detect token characters. See <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.IsTokenChar(System.Int32)"/> and
                <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.Normalize(System.Int32)"/> for details.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.WhitespaceTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader)">
            Construct a new <see cref="T:Lucene.Net.Analysis.Core.WhitespaceTokenizer"/>. 
            <param name="matchVersion"> <see cref="T:Lucene.Net.Util.LuceneVersion"/> to match</param>
            <param name="in">
                     the input to split up into tokens </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.WhitespaceTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader)">
            <summary>
            Construct a new <see cref="T:Lucene.Net.Analysis.Core.WhitespaceTokenizer"/> using a given
            <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/>.
            </summary>
            <param name="matchVersion"><see cref="T:Lucene.Net.Util.LuceneVersion"/> to match</param>
            <param name="factory">
                     the attribute factory to use for this <see cref="T:Lucene.Net.Analysis.Tokenizer"/> </param>
            <param name="in">
                     the input to split up into tokens </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.WhitespaceTokenizer.IsTokenChar(System.Int32)">
            <summary>
            Collects only characters which do not satisfy
            <see cref="M:System.Char.IsWhiteSpace(System.Char)"/>.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Core.WhitespaceTokenizerFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Core.WhitespaceTokenizer"/>. 
            <code>
            &lt;fieldType name="text_ws" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Core.WhitespaceTokenizerFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Core.WhitespaceTokenizerFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Cz.CzechAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Czech language.
            <para>
            Supports an external list of stopwords (words that will not be indexed at
            all). A default set of stopwords is used unless an alternative list is
            specified.
            </para>
            <para>
            You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/> compatibility when creating
            <see cref="T:Lucene.Net.Analysis.Cz.CzechAnalyzer"/>:
            <list type="bullet">
                <item><description>As of 3.1, words are stemmed with <see cref="T:Lucene.Net.Analysis.Cz.CzechStemFilter"/></description></item>
                <item><description>As of 2.9, StopFilter preserves position increments</description></item>
                <item><description>As of 2.4, Tokens incorrectly identified as acronyms are corrected (see
                <a href="https://issues.apache.org/jira/browse/LUCENE-1068">LUCENE-1068</a>)</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Cz.CzechAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Czech stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Cz.CzechAnalyzer.DefaultStopSet">
            <summary>
            Returns a set of default Czech-stopwords
            </summary>
            <returns> a set of default Czech-stopwords </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Cz.CzechAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words (<see cref="P:Lucene.Net.Analysis.Cz.CzechAnalyzer.DefaultStopSet"/>).
            </summary>
            <param name="matchVersion"> <see cref="T:Lucene.Net.Util.LuceneVersion"/> to match </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Cz.CzechAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> <see cref="T:Lucene.Net.Util.LuceneVersion"/> to match </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Cz.CzechAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words and a set of work to be
            excluded from the <see cref="T:Lucene.Net.Analysis.Cz.CzechStemFilter"/>.
            </summary>
            <param name="matchVersion"> <see cref="T:Lucene.Net.Util.LuceneVersion"/> to match </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionTable"> a stemming exclusion set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Cz.CzechAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            used to tokenize all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from a <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    and <see cref="T:Lucene.Net.Analysis.Cz.CzechStemFilter"/> (only if version is >= LUCENE_31). If
                    a version is >= LUCENE_31 and a stem exclusion set is provided via
                    <see cref="M:Lucene.Net.Analysis.Cz.CzechAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)"/> a
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> is added before
                    <see cref="T:Lucene.Net.Analysis.Cz.CzechStemFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Cz.CzechStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Cz.CzechStemmer"/> to stem Czech words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            <para><b>NOTE</b>: Input is expected to be in lowercase, 
            but with diacritical marks</para> </summary>
            <seealso cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/>
        </member>
        <member name="T:Lucene.Net.Analysis.Cz.CzechStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Cz.CzechStemFilter"/>.
            <code>
            &lt;fieldType name="text_czstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.CzechStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Cz.CzechStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Cz.CzechStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Cz.CzechStemmer">
            <summary>
            Light Stemmer for Czech.
            <para>
            Implements the algorithm described in:  
            <c>
            Indexing and stemming approaches for the Czech language
            </c>
            http://portal.acm.org/citation.cfm?id=1598600
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Cz.CzechStemmer.Stem(System.Char[],System.Int32)">
            <summary>
            Stem an input buffer of Czech text.
            <para><b>NOTE</b>: Input is expected to be in lowercase, 
            but with diacritical marks</para>
            </summary>
            <param name="s"> input buffer </param>
            <param name="len"> length of input buffer </param>
            <returns> length of input buffer after normalization</returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Da.DanishAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Danish.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Da.DanishAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Danish stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Da.DanishAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Da.DanishAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Da.DanishAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Da.DanishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.Da.DanishAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Da.DanishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Da.DanishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Da.DanishAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.De.GermanAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for German language. 
            <para>
            Supports an external list of stopwords (words that
            will not be indexed at all) and an external list of exclusions (word that will
            not be stemmed, but indexed).
            A default set of stopwords is used unless an alternative list is specified, but the
            exclusion list is empty by default.
            </para>
            
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating GermanAnalyzer:
            <list>
              <item><description> As of 3.6, GermanLightStemFilter is used for less aggressive stemming.</description></item>
              <item><description> As of 3.1, Snowball stemming is done with SnowballFilter, and 
                   Snowball stopwords are used by default.</description></item>
              <item><description> As of 2.9, StopFilter preserves position
                   increments</description></item>
            </list>
            
            </para>
            <para><b>NOTE</b>: This class uses the same <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            dependent settings as <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/>.</para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.De.GermanAnalyzer.GERMAN_STOP_WORDS">
            @deprecated in 3.1, remove in Lucene 5.0 (index bw compat) 
        </member>
        <member name="F:Lucene.Net.Analysis.De.GermanAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default German stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.De.GermanAnalyzer.DefaultStopSet">
            <summary>
            Returns a set of default German-stopwords </summary>
            <returns> a set of default German-stopwords  </returns>
        </member>
        <member name="F:Lucene.Net.Analysis.De.GermanAnalyzer.DefaultSetHolder.DEFAULT_SET_30">
            @deprecated in 3.1, remove in Lucene 5.0 (index bw compat) 
        </member>
        <member name="F:Lucene.Net.Analysis.De.GermanAnalyzer.exclusionSet">
            <summary>
            Contains words that should be indexed but not stemmed.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.De.GermanAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words:
            <see cref="P:Lucene.Net.Analysis.De.GermanAnalyzer.DefaultStopSet"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.De.GermanAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words 
            </summary>
            <param name="matchVersion">
                     lucene compatibility version </param>
            <param name="stopwords">
                     a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.De.GermanAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words
            </summary>
            <param name="matchVersion">
                     lucene compatibility version </param>
            <param name="stopwords">
                     a stopword set </param>
            <param name="stemExclusionSet">
                     a stemming exclusion set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.De.GermanAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            used to tokenize all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from a <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided, <see cref="T:Lucene.Net.Analysis.De.GermanNormalizationFilter"/> and <see cref="T:Lucene.Net.Analysis.De.GermanLightStemFilter"/> </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.De.GermanLightStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.De.GermanLightStemmer"/> to stem German
            words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.De.GermanLightStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.De.GermanLightStemFilter"/>.
            <code>
            &lt;fieldType name="text_delgtstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.GermanLightStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.De.GermanLightStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.De.GermanLightStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.De.GermanLightStemmer">
            <summary>
            Light Stemmer for German.
            <para>
            This stemmer implements the "UniNE" algorithm in:
            <c>Light Stemming Approaches for the French, Portuguese, German and Hungarian Languages</c>
            Jacques Savoy
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.De.GermanMinimalStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.De.GermanMinimalStemmer"/> to stem German
            words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.De.GermanMinimalStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.De.GermanMinimalStemFilter"/>.
            <code>
            &lt;fieldType name="text_deminstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.GermanMinimalStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.De.GermanMinimalStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.De.GermanMinimalStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.De.GermanMinimalStemmer">
            <summary>
            Minimal Stemmer for German.
            <para>
            This stemmer implements the following algorithm:
            <c>Morphologie et recherche d'information</c>
            Jacques Savoy.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.De.GermanNormalizationFilter">
            <summary>
            Normalizes German characters according to the heuristics
            of the <c>http://snowball.tartarus.org/algorithms/german2/stemmer.html
            German2 snowball algorithm</c>.
            It allows for the fact that ä, ö and ü are sometimes written as ae, oe and ue.
            <para>
            <list>
                <item><description> 'ß' is replaced by 'ss'</description></item>
                <item><description> 'ä', 'ö', 'ü' are replaced by 'a', 'o', 'u', respectively.</description></item>
                <item><description> 'ae' and 'oe' are replaced by 'a', and 'o', respectively.</description></item>
                <item><description> 'ue' is replaced by 'u', when not following a vowel or q.</description></item>
            </list>
            </para>
            <para>
            This is useful if you want this normalization without using
            the German2 stemmer, or perhaps no stemming at all.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.De.GermanNormalizationFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.De.GermanNormalizationFilter"/>.
            <code>
            &lt;fieldType name="text_denorm" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.GermanNormalizationFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code> 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.De.GermanNormalizationFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.De.GermanNormalizationFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.De.GermanStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that stems German words. 
            <para>
            It supports a table of words that should
            not be stemmed at all. The stemmer used can be changed at runtime after the
            filter object is created (as long as it is a <see cref="T:Lucene.Net.Analysis.De.GermanStemmer"/>).
            </para>
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para> </summary>
            <seealso cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/>
        </member>
        <member name="F:Lucene.Net.Analysis.De.GermanStemFilter.stemmer">
            <summary>
            The actual token in the input stream.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.De.GermanStemFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Creates a <see cref="T:Lucene.Net.Analysis.De.GermanStemFilter"/> instance </summary>
            <param name="in"> the source <see cref="T:Lucene.Net.Analysis.TokenStream"/>  </param>
        </member>
        <member name="M:Lucene.Net.Analysis.De.GermanStemFilter.IncrementToken">
            <returns>  Returns true for next token in the stream, or false at EOS </returns>
        </member>
        <member name="P:Lucene.Net.Analysis.De.GermanStemFilter.Stemmer">
            <summary>
            Set a alternative/custom <see cref="T:Lucene.Net.Analysis.De.GermanStemmer"/> for this filter.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.De.GermanStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.De.GermanStemFilter"/>. 
            <code>
            &lt;fieldType name="text_destem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.GermanStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.De.GermanStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.De.GermanStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.De.GermanStemmer">
            <summary>
            A stemmer for German words. 
            <para>
            The algorithm is based on the report
            "A Fast and Simple Stemming Algorithm for German Words" by Jörg
            Caumanns (joerg.caumanns at isst.fhg.de).
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.De.GermanStemmer.sb">
            <summary>
            Buffer for the terms while stemming them.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.De.GermanStemmer.substCount">
            <summary>
            Amount of characters that are removed with <see cref="M:Lucene.Net.Analysis.De.GermanStemmer.Substitute(System.Text.StringBuilder)"/> while stemming.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.De.GermanStemmer.Stem(System.String)">
            <summary>
            Stemms the given term to an unique <c>discriminator</c>.
            </summary>
            <param name="term">  The term that should be stemmed. </param>
            <returns>      Discriminator for <paramref name="term"/> </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.De.GermanStemmer.IsStemmable(System.String)">
            <summary>
            Checks if a term could be stemmed.
            </summary>
            <returns>  true if, and only if, the given term consists in letters. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.De.GermanStemmer.Strip(System.Text.StringBuilder)">
            <summary>
            suffix stripping (stemming) on the current term. The stripping is reduced
            to the seven "base" suffixes "e", "s", "n", "t", "em", "er" and * "nd",
            from which all regular suffixes are build of. The simplification causes
            some overstemming, and way more irregular stems, but still provides unique.
            discriminators in the most of those cases.
            The algorithm is context free, except of the length restrictions.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.De.GermanStemmer.Optimize(System.Text.StringBuilder)">
            <summary>
            Does some optimizations on the term. This optimisations are
            contextual.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.De.GermanStemmer.RemoveParticleDenotion(System.Text.StringBuilder)">
            <summary>
            Removes a particle denotion ("ge") from a term.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.De.GermanStemmer.Substitute(System.Text.StringBuilder)">
            <summary>
            Do some substitutions for the term to reduce overstemming:
            
            <list type="bullet">
            <item><description>Substitute Umlauts with their corresponding vowel: äöü -> aou,
              "ß" is substituted by "ss"</description></item>
            <item><description>Substitute a second char of a pair of equal characters with
              an asterisk: ?? -> ?*</description></item>
            <item><description>Substitute some common character combinations with a token:
              sch/ch/ei/ie/ig/st -> $/§/%/&amp;/#/!</description></item>
            </list>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.De.GermanStemmer.Resubstitute(System.Text.StringBuilder)">
            <summary>
            Undoes the changes made by <see cref="M:Lucene.Net.Analysis.De.GermanStemmer.Substitute(System.Text.StringBuilder)"/>. That are character pairs and
            character combinations. Umlauts will remain as their corresponding vowel,
            as "ß" remains as "ss".
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.El.GreekAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for the Greek language. 
            <para>
            Supports an external list of stopwords (words
            that will not be indexed at all).
            A default set of stopwords is used unless an alternative list is specified.
            </para>
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.El.GreekAnalyzer"/>:
            <list type="bullet">
              <item><description> As of 3.1, StandardFilter and GreekStemmer are used by default.</description></item>
              <item><description> As of 2.9, StopFilter preserves position
                   increments</description></item>
            </list>
            </para>
            <para><c>NOTE</c>: This class uses the same <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            dependent settings as <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/>.</para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.El.GreekAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Greek stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.El.GreekAnalyzer.DefaultStopSet">
            <summary>
            Returns a set of default Greek-stopwords </summary>
            <returns> a set of default Greek-stopwords  </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.El.GreekAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words. </summary>
            <param name="matchVersion"> Lucene compatibility version,
              See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
        </member>
        <member name="M:Lucene.Net.Analysis.El.GreekAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. 
            <para>
            <b>NOTE:</b> The stopwords set should be pre-processed with the logic of 
            <see cref="T:Lucene.Net.Analysis.El.GreekLowerCaseFilter"/> for best results.
             
            </para>
            </summary>
            <param name="matchVersion"> Lucene compatibility version,
              See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.El.GreekAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            used to tokenize all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from a <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.El.GreekLowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>, and <see cref="T:Lucene.Net.Analysis.El.GreekStemFilter"/> </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.El.GreekLowerCaseFilter">
            <summary>
            Normalizes token text to lower case, removes some Greek diacritics,
            and standardizes final sigma to sigma. 
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.El.GreekLowerCaseFilter"/>:
            <list type="bullet">
                <item><description> As of 3.1, supplementary characters are properly lowercased.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.El.GreekLowerCaseFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream)">
            <summary>
            Create a <see cref="T:Lucene.Net.Analysis.El.GreekLowerCaseFilter"/> that normalizes Greek token text.
            </summary>
            <param name="matchVersion"> Lucene compatibility version, 
              See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="in"> <see cref="T:Lucene.Net.Analysis.TokenStream"/> to filter </param>
        </member>
        <member name="T:Lucene.Net.Analysis.El.GreekLowerCaseFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.El.GreekLowerCaseFilter"/>. 
            <code>
            &lt;fieldType name="text_glc" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.GreekLowerCaseFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.El.GreekLowerCaseFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.El.GreekLowerCaseFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.El.GreekStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.El.GreekStemmer"/> to stem Greek
            words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            <para>
            NOTE: Input is expected to be casefolded for Greek (including folding of final
            sigma to sigma), and with diacritics removed. This can be achieved by using 
            either <see cref="T:Lucene.Net.Analysis.El.GreekLowerCaseFilter"/> or ICUFoldingFilter before <see cref="T:Lucene.Net.Analysis.El.GreekStemFilter"/>.
            @lucene.experimental
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.El.GreekStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.El.GreekStemFilter"/>. 
            <code>
            &lt;fieldType name="text_gstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.GreekLowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.GreekStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.El.GreekStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.El.GreekStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.El.GreekStemmer">
            <summary>
            A stemmer for Greek words, according to: <c>Development of a Stemmer for the
            Greek Language.</c> Georgios Ntais
            <para>
            NOTE: Input is expected to be casefolded for Greek (including folding of final
            sigma to sigma), and with diacritics removed. This can be achieved with 
            either <see cref="T:Lucene.Net.Analysis.El.GreekLowerCaseFilter"/> or ICUFoldingFilter.
            @lucene.experimental
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.El.GreekStemmer.Stem(System.Char[],System.Int32)">
            <summary>
            Stems a word contained in a leading portion of a <see cref="T:char[]"/> array.
            The word is passed through a number of rules that modify it's length.
            </summary>
            <param name="s"> A <see cref="T:char[]"/> array that contains the word to be stemmed. </param>
            <param name="len"> The length of the <see cref="T:char[]"/> array. </param>
            <returns> The new length of the stemmed word. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.El.GreekStemmer.EndsWith(System.Char[],System.Int32,System.String)">
            <summary>
            Checks if the word contained in the leading portion of char[] array , 
            ends with the suffix given as parameter.
            </summary>
            <param name="s"> A char[] array that represents a word. </param>
            <param name="len"> The length of the char[] array. </param>
            <param name="suffix"> A <see cref="T:System.String"/> object to check if the word given ends with these characters. </param>
            <returns> True if the word ends with the suffix given , false otherwise. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.El.GreekStemmer.EndsWithVowel(System.Char[],System.Int32)">
            <summary>
            Checks if the word contained in the leading portion of <see cref="T:char[]"/> array , 
            ends with a Greek vowel.
            </summary>
            <param name="s"> A <see cref="T:char[]"/> array that represents a word. </param>
            <param name="len"> The length of the <see cref="T:char[]"/> array. </param>
            <returns> True if the word contained in the leading portion of <see cref="T:char[]"/> array , 
            ends with a vowel , false otherwise. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.El.GreekStemmer.EndsWithVowelNoY(System.Char[],System.Int32)">
            <summary>
            Checks if the word contained in the leading portion of <see cref="T:char[]"/> array , 
            ends with a Greek vowel.
            </summary>
            <param name="s"> A <see cref="T:char[]"/> array that represents a word. </param>
            <param name="len"> The length of the <see cref="T:char[]"/> array. </param>
            <returns> True if the word contained in the leading portion of <see cref="T:char[]"/> array , 
            ends with a vowel , false otherwise. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.En.EnglishAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for English.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.En.EnglishAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.En.EnglishAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.En.EnglishAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.EnglishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="P:Lucene.Net.Analysis.En.EnglishAnalyzer.DefaultStopSet"/>.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
        </member>
        <member name="M:Lucene.Net.Analysis.En.EnglishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.En.EnglishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.En.EnglishAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.En.EnglishPossessiveFilter"/>, 
                    <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.En.PorterStemFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.En.EnglishMinimalStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.En.EnglishMinimalStemmer"/> to stem 
            English words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.En.EnglishMinimalStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.En.EnglishMinimalStemFilter"/>.
            <code>
            &lt;fieldType name="text_enminstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.EnglishMinimalStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.EnglishMinimalStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.En.EnglishMinimalStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.En.EnglishMinimalStemmer">
            <summary>
            Minimal plural stemmer for English.
            <para>
            This stemmer implements the "S-Stemmer" from
            <c>How Effective Is Suffixing?</c>
            Donna Harman.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.En.EnglishPossessiveFilter">
            <summary>
            TokenFilter that removes possessives (trailing 's) from words.
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.En.EnglishPossessiveFilter"/>:
            <list type="bullet">
               <item><description> As of 3.6, U+2019 RIGHT SINGLE QUOTATION MARK and 
                    U+FF07 FULLWIDTH APOSTROPHE are also treated as
                    quotation marks.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.EnglishPossessiveFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            @deprecated Use <see cref="M:Lucene.Net.Analysis.En.EnglishPossessiveFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream)"/> instead. 
        </member>
        <member name="T:Lucene.Net.Analysis.En.EnglishPossessiveFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.En.EnglishPossessiveFilter"/>. 
            <code>
            &lt;fieldType name="text_enpossessive" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.EnglishPossessiveFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.EnglishPossessiveFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.En.EnglishPossessiveFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.En.KStemData1">
            <summary>
            A list of words used by Kstem
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.En.KStemData2">
            <summary>
            A list of words used by Kstem
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.En.KStemData3">
            <summary>
            A list of words used by Kstem
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.En.KStemData4">
            <summary>
            A list of words used by Kstem
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.En.KStemData5">
            <summary>
            A list of words used by Kstem
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.En.KStemData6">
            <summary>
            A list of words used by Kstem
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.En.KStemData7">
            <summary>
            A list of words used by Kstem
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.En.KStemData8">
            <summary>
            A list of words used by Kstem
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.En.KStemFilter">
            <summary>
            A high-performance kstem filter for english.
            <para/>
            See <a href="http://ciir.cs.umass.edu/pubfiles/ir-35.pdf">
            "Viewing Morphology as an Inference Process"</a>
            (Krovetz, R., Proceedings of the Sixteenth Annual International ACM SIGIR
            Conference on Research and Development in Information Retrieval, 191-203, 1993).
            <para/>
            All terms must already be lowercased for this filter to work correctly.
            
            <para>
            Note: This filter is aware of the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/>. To prevent
            certain terms from being passed to the stemmer
            <see cref="P:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute.IsKeyword"/> should be set to <code>true</code>
            in a previous <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            
            Note: For including the original term as well as the stemmed version, see
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.KeywordRepeatFilterFactory"/>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemFilter.IncrementToken">
            <summary>
            Returns the next, stemmed, input Token. </summary>
             <returns> The stemmed form of a token. </returns>
             <exception cref="T:System.IO.IOException"> If there is a low-level I/O error. </exception>
        </member>
        <member name="T:Lucene.Net.Analysis.En.KStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.En.KStemFilter"/>.
            <code>
            &lt;fieldType name="text_kstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.KStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.En.KStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.En.KStemmer">
            <summary>
            This class implements the Kstem algorithm
            </summary>
            <remarks>
            <para>Title: Kstemmer</para>
            <para>Description: This is a java version of Bob Krovetz' kstem stemmer</para>
            <para>Copyright: Copyright 2008, Luicid Imagination, Inc. </para>
            <para>Copyright: Copyright 2003, CIIR University of Massachusetts Amherst (http://ciir.cs.umass.edu) </para>
            </remarks>
        </member>
        <member name="F:Lucene.Net.Analysis.En.KStemmer.k">
            <summary>
            INDEX of final letter in word. You must add 1 to k to get
            the current length of word. When you want the length of
            word, use the method wordLength, which returns (k+1).
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.En.KStemmer.StemLength">
            <summary>length of stem within word</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.Plural">
            <summary>Convert plurals to singular form, and '-ies' to 'y'</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.SetSuff(System.String,System.Int32)">
            <summary>replace old suffix with s</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.PastTense">
            <summary>convert past tense (-ed) to present, and `-ied' to `y'</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.DoubleC(System.Int32)">
            <summary>return TRUE if word ends with a double consonant</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.Aspect">
            <summary>handle `-ing' endings</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.ItyEndings">
            <summary>
            this routine deals with -ity endings. It accepts -ability, -ibility, and
            -ality, even without checking the dictionary because they are so
            productive. The first two are mapped to -ble, and the -ity is remove for
            the latter
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.NceEndings">
            <summary>handle -ence and -ance</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.NessEndings">
            <summary>handle -ness</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.IsmEndings">
            <summary>handle -ism</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.MentEndings">
            <summary>this routine deals with -ment endings.</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.IzeEndings">
            <summary>this routine deals with -ize endings.</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.NcyEndings">
            <summary>handle -ency and -ancy</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.BleEndings">
            <summary>handle -able and -ible</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.IcEndings">
            <summary>
            handle -ic endings. This is fairly straightforward, but this is also the
            only place we try *expanding* an ending, -ic -> -ical. This is to handle
            cases like `canonic' -> `canonical'
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.IonEndings">
            <summary>
            this routine deals with -ion, -ition, -ation, -ization, and -ication. The
            -ization ending is always converted to -ize
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.ErAndOrEndings">
            <summary>
            this routine deals with -er, -or, -ier, and -eer. The -izer ending is
            always converted to -ize
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.LyEndings">
            <summary>
            this routine deals with -ly endings. The -ally ending is always converted
            to -al Sometimes this will temporarily leave us with a non-word (e.g.,
            heuristically maps to heuristical), but then the -al is removed in the next
            step.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.AlEndings">
            <summary>
            this routine deals with -al endings. Some of the endings from the previous
            routine are finished up here.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.IveEndings">
            <summary>
            this routine deals with -ive endings. It normalizes some of the -ative
            endings directly, and also maps some -ive endings to -ion.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.AsString">
            <summary>
            Returns the result of the stem (assuming the word was changed) as a <see cref="T:System.String"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.KStemmer.Stem(System.Char[],System.Int32)">
            <summary>
            Stems the text in the token. Returns true if changed.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.En.PorterStemFilter">
            <summary>
            Transforms the token stream as per the Porter stemming algorithm.
            <para/>
               Note: the input to the stemming filter must already be in lower case,
               so you will need to use LowerCaseFilter or LowerCaseTokenizer farther
               down the Tokenizer chain in order for this to work properly!
               <para/>
               To use this filter with other analyzers, you'll want to write an
               Analyzer class that sets up the TokenStream chain as you want it.
               To use this with LowerCaseTokenizer, for example, you'd write an
               analyzer like this:
               <para/>
               <code>
               class MyAnalyzer : Analyzer {
                 protected override TokenStreamComponents CreateComponents(string fieldName, TextReader reader) {
                   Tokenizer source = new LowerCaseTokenizer(version, reader);
                   return new TokenStreamComponents(source, new PorterStemFilter(source));
                 }
               }
               </code>
               <para>
               Note: This filter is aware of the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/>. To prevent
               certain terms from being passed to the stemmer
               <see cref="P:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute.IsKeyword"/> should be set to <code>true</code>
               in a previous <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            
               Note: For including the original term as well as the stemmed version, see
              <see cref="T:Lucene.Net.Analysis.Miscellaneous.KeywordRepeatFilterFactory"/>
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.En.PorterStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.En.PorterStemFilter"/>.
            <code>
            &lt;fieldType name="text_porterstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.PorterStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.PorterStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.En.PorterStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.En.PorterStemmer">
            
            <summary>
            Stemmer, implementing the Porter Stemming Algorithm
            
            The Stemmer class transforms a word into its root form.  The input
            word can be provided a character at time (by calling <see cref="M:Lucene.Net.Analysis.En.PorterStemmer.Add(System.Char)"/>), or at once
            by calling one of the various Stem methods, such as <see cref="M:Lucene.Net.Analysis.En.PorterStemmer.Stem(System.String)"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.PorterStemmer.Reset">
            <summary>
            <see cref="M:Lucene.Net.Analysis.En.PorterStemmer.Reset"/> resets the stemmer so it can stem another word.  If you invoke
            the stemmer by calling <see cref="M:Lucene.Net.Analysis.En.PorterStemmer.Add(System.Char)"/> and then <see cref="M:Lucene.Net.Analysis.En.PorterStemmer.Stem"/>, you must call <see cref="M:Lucene.Net.Analysis.En.PorterStemmer.Reset"/>
            before starting another word.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.PorterStemmer.Add(System.Char)">
            <summary>
            Add a character to the word being stemmed.  When you are finished
            adding characters, you can call <see cref="M:Lucene.Net.Analysis.En.PorterStemmer.Stem"/> to process the word.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.PorterStemmer.ToString">
            <summary>
            After a word has been stemmed, it can be retrieved by <see cref="M:Lucene.Net.Analysis.En.PorterStemmer.ToString"/>,
            or a reference to the internal buffer can be retrieved by <see cref="P:Lucene.Net.Analysis.En.PorterStemmer.ResultBuffer"/>
            and <see cref="P:Lucene.Net.Analysis.En.PorterStemmer.ResultLength"/> (which is generally more efficient.)
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.En.PorterStemmer.ResultLength">
            <summary>
            Returns the length of the word resulting from the stemming process.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.En.PorterStemmer.ResultBuffer">
            <summary>
            Returns a reference to a character buffer containing the results of
            the stemming process.  You also need to consult <see cref="P:Lucene.Net.Analysis.En.PorterStemmer.ResultLength"/>
            to determine the length of the result.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.PorterStemmer.Stem(System.String)">
            <summary>
            Stem a word provided as a <see cref="T:System.String"/>.  Returns the result as a <see cref="T:System.String"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.PorterStemmer.Stem(System.Char[])">
            <summary>
            Stem a word contained in a <see cref="T:char[]"/>.  Returns true if the stemming process
            resulted in a word different from the input.  You can retrieve the
            result with <see cref="P:Lucene.Net.Analysis.En.PorterStemmer.ResultLength"/>/<see cref="P:Lucene.Net.Analysis.En.PorterStemmer.ResultBuffer"/> or <see cref="M:Lucene.Net.Analysis.En.PorterStemmer.ToString"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.PorterStemmer.Stem(System.Char[],System.Int32,System.Int32)">
            <summary>
            Stem a word contained in a portion of a <see cref="T:char[]"/> array.  Returns
            true if the stemming process resulted in a word different from
            the input.  You can retrieve the result with
            <see cref="P:Lucene.Net.Analysis.En.PorterStemmer.ResultLength"/>/<see cref="P:Lucene.Net.Analysis.En.PorterStemmer.ResultBuffer"/> or <see cref="M:Lucene.Net.Analysis.En.PorterStemmer.ToString"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.PorterStemmer.Stem(System.Char[],System.Int32)">
            <summary>
            Stem a word contained in a leading portion of a <see cref="T:char[]"/> array.
            Returns true if the stemming process resulted in a word different
            from the input.  You can retrieve the result with
            <see cref="P:Lucene.Net.Analysis.En.PorterStemmer.ResultLength"/>/<see cref="P:Lucene.Net.Analysis.En.PorterStemmer.ResultBuffer"/> or <see cref="M:Lucene.Net.Analysis.En.PorterStemmer.ToString"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.En.PorterStemmer.Stem">
            <summary>
            Stem the word placed into the Stemmer buffer through calls to <see cref="M:Lucene.Net.Analysis.En.PorterStemmer.Add(System.Char)"/>.
            Returns true if the stemming process resulted in a word different
            from the input.  You can retrieve the result with
            <see cref="P:Lucene.Net.Analysis.En.PorterStemmer.ResultLength"/>/<see cref="P:Lucene.Net.Analysis.En.PorterStemmer.ResultBuffer"/> or <see cref="M:Lucene.Net.Analysis.En.PorterStemmer.ToString"/>.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Es.SpanishAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Spanish.
            <para>
            </para>
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.Es.SpanishAnalyzer"/>:
            <list type="bullet">
                <item><description> As of 3.6, <see cref="T:Lucene.Net.Analysis.Es.SpanishLightStemFilter"/> is used for less aggressive stemming.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Es.SpanishAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Spanish stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Es.SpanishAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Es.SpanishAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Es.SpanishAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Es.SpanishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.Es.SpanishAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Es.SpanishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Es.SpanishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Es.SpanishAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.Es.SpanishLightStemFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Es.SpanishLightStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Es.SpanishLightStemmer"/> to stem Spanish
            words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Es.SpanishLightStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Es.SpanishLightStemFilter"/>.
            <code>
            &lt;fieldType name="text_eslgtstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.SpanishLightStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Es.SpanishLightStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Es.SpanishLightStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Es.SpanishLightStemmer">
            <summary>
            Light Stemmer for Spanish
            <para>
            This stemmer implements the algorithm described in:
            <c>Report on CLEF-2001 Experiments</c>
            Jacques Savoy
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Eu.BasqueAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Basque.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Eu.BasqueAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Basque stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Eu.BasqueAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Eu.BasqueAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Eu.BasqueAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Eu.BasqueAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.Eu.BasqueAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Eu.BasqueAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Eu.BasqueAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Eu.BasqueAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Fa.PersianAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Persian.
            <para>
            This Analyzer uses <see cref="T:Lucene.Net.Analysis.Fa.PersianCharFilter"/> which implies tokenizing around
            zero-width non-joiner in addition to whitespace. Some persian-specific variant forms (such as farsi
            yeh and keheh) are standardized. "Stemming" is accomplished via stopwords.
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Fa.PersianAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Persian stopwords.
            
            Default stopword list is from
            http://members.unine.ch/jacques.savoy/clef/index.html. The stopword list is
            BSD-Licensed.
            
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Fa.PersianAnalyzer.STOPWORDS_COMMENT">
            <summary>
            The comment character in the stopwords file. All lines prefixed with this
            will be ignored
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Fa.PersianAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop-words set. </summary>
            <returns> an unmodifiable instance of the default stop-words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Fa.PersianAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Fa.PersianAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fa.PersianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words:
            <see cref="F:Lucene.Net.Analysis.Fa.PersianAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fa.PersianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words 
            </summary>
            <param name="matchVersion">
                     lucene compatibility version </param>
            <param name="stopwords">
                     a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Fa.PersianAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            used to tokenize all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from a <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Ar.ArabicNormalizationFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Fa.PersianNormalizationFilter"/> and Persian Stop words </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Fa.PersianAnalyzer.InitReader(System.String,System.IO.TextReader)">
            <summary>
            Wraps the <see cref="T:System.IO.TextReader"/> with <see cref="T:Lucene.Net.Analysis.Fa.PersianCharFilter"/>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Fa.PersianCharFilter">
            <summary>
            <see cref="T:Lucene.Net.Analysis.CharFilter"/> that replaces instances of Zero-width non-joiner with an
            ordinary space.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Fa.PersianCharFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Fa.PersianCharFilter"/>.
            <code>
            &lt;fieldType name="text_fa" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;charFilter class="solr.PersianCharFilterFactory"/&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fa.PersianCharFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Fa.PersianCharFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Fa.PersianNormalizationFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Fa.PersianNormalizer"/> to normalize the
            orthography.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Fa.PersianNormalizationFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Fa.PersianNormalizationFilter"/>.
            <code>
            &lt;fieldType name="text_fanormal" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;charFilter class="solr.PersianCharFilterFactory"/&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.PersianNormalizationFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fa.PersianNormalizationFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Fa.PersianNormalizationFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Fa.PersianNormalizer">
            <summary>
            Normalizer for Persian.
            <para>
            Normalization is done in-place for efficiency, operating on a termbuffer.
            </para>
            <para>
            Normalization is defined as:
            <list type="bullet">
                <item><description>Normalization of various heh + hamza forms and heh goal to heh.</description></item>
                <item><description>Normalization of farsi yeh and yeh barree to arabic yeh</description></item>
                <item><description>Normalization of persian keheh to arabic kaf</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fa.PersianNormalizer.Normalize(System.Char[],System.Int32)">
            <summary>
            Normalize an input buffer of Persian text
            </summary>
            <param name="s"> input buffer </param>
            <param name="len"> length of input buffer </param>
            <returns> length of input buffer after normalization </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Fi.FinnishAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Finnish.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Fi.FinnishAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Italian stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Fi.FinnishAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Fi.FinnishAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Fi.FinnishAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fi.FinnishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.Fi.FinnishAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fi.FinnishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Fi.FinnishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Fi.FinnishAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Fi.FinnishLightStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Fi.FinnishLightStemmer"/> to stem Finnish
            words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Fi.FinnishLightStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Fi.FinnishLightStemFilter"/>.
            <code>
            &lt;fieldType name="text_filgtstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.FinnishLightStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fi.FinnishLightStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Fi.FinnishLightStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Fi.FinnishLightStemmer">
            <summary>
            Light Stemmer for Finnish.
            <para>
            This stemmer implements the algorithm described in:
            <c>Report on CLEF-2003 Monolingual Tracks</c>
            Jacques Savoy
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Fr.FrenchAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for French language. 
            <para>
            Supports an external list of stopwords (words that
            will not be indexed at all) and an external list of exclusions (word that will
            not be stemmed, but indexed).
            A default set of stopwords is used unless an alternative list is specified, but the
            exclusion list is empty by default.
            </para>
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating FrenchAnalyzer:
            <list type="bullet">
              <item><description> As of 3.6, <see cref="T:Lucene.Net.Analysis.Fr.FrenchLightStemFilter"/> is used for less aggressive stemming.</description></item>
              <item><description> As of 3.1, Snowball stemming is done with <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/>, 
                   <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/> is used prior to <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>, and <see cref="T:Lucene.Net.Analysis.Util.ElisionFilter"/> and 
                   Snowball stopwords are used by default.</description></item>
              <item><description> As of 2.9, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/> preserves position
                   increments</description></item>
            </list>
            
            </para>
            <para><b>NOTE</b>: This class uses the same <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            dependent settings as <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/>.</para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Fr.FrenchAnalyzer.FRENCH_STOP_WORDS">
            <summary>
            Extended list of typical French stopwords. </summary>
            @deprecated (3.1) remove in Lucene 5.0 (index bw compat) 
        </member>
        <member name="F:Lucene.Net.Analysis.Fr.FrenchAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default French stopwords. </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Fr.FrenchAnalyzer.DEFAULT_ARTICLES">
            <summary>
            Default set of articles for <see cref="T:Lucene.Net.Analysis.Util.ElisionFilter"/> </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Fr.FrenchAnalyzer.excltable">
            <summary>
            Contains words that should be indexed but not stemmed.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Fr.FrenchAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop-words set. </summary>
            <returns> an unmodifiable instance of the default stop-words set. </returns>
        </member>
        <member name="F:Lucene.Net.Analysis.Fr.FrenchAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET_30">
            @deprecated (3.1) remove this in Lucene 5.0, index bw compat 
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words (<see cref="P:Lucene.Net.Analysis.Fr.FrenchAnalyzer.DefaultStopSet"/>).
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words
            </summary>
            <param name="matchVersion">
                     lucene compatibility version </param>
            <param name="stopwords">
                     a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words
            </summary>
            <param name="matchVersion">
                     lucene compatibility version </param>
            <param name="stopwords">
                     a stopword set </param>
            <param name="stemExclutionSet">
                     a stemming exclusion set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            used to tokenize all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from a <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Util.ElisionFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided, and <see cref="T:Lucene.Net.Analysis.Fr.FrenchLightStemFilter"/> </returns>
                    
        </member>
        <member name="T:Lucene.Net.Analysis.Fr.FrenchLightStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Fr.FrenchLightStemmer"/> to stem French
            words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Fr.FrenchLightStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Fr.FrenchLightStemFilter"/>.
            <code>
            &lt;fieldType name="text_frlgtstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.ElisionFilterFactory"/&gt;
                &lt;filter class="solr.FrenchLightStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchLightStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Fr.FrenchLightStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Fr.FrenchLightStemmer">
            <summary>
            Light Stemmer for French.
            <para>
            This stemmer implements the "UniNE" algorithm in:
            <c>Light Stemming Approaches for the French, Portuguese, German and Hungarian Languages</c>
            Jacques Savoy
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Fr.FrenchMinimalStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Fr.FrenchMinimalStemmer"/> to stem French
            words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Fr.FrenchMinimalStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Fr.FrenchMinimalStemFilter"/>.
            <code>
            &lt;fieldType name="text_frminstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.ElisionFilterFactory"/&gt;
                &lt;filter class="solr.FrenchMinimalStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchMinimalStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Fr.FrenchMinimalStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Fr.FrenchMinimalStemmer">
            <summary>
            Light Stemmer for French.
            <para>
            This stemmer implements the following algorithm:
            <c>A Stemming procedure and stopword list for general French corpora.</c>
            Jacques Savoy.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Fr.FrenchStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that stems french words. 
            <para>
            The used stemmer can be changed at runtime after the
            filter object is created (as long as it is a <see cref="T:Lucene.Net.Analysis.Fr.FrenchStemmer"/>).
            </para>
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.KeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para> </summary>
            <seealso cref="T:Lucene.Net.Analysis.Miscellaneous.KeywordMarkerFilter"/>
            @deprecated (3.1) Use <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/> with 
            <see cref="T:Lucene.Net.Tartarus.Snowball.Ext.FrenchStemmer"/> instead, which has the
            same functionality. This filter will be removed in Lucene 5.0 
        </member>
        <member name="F:Lucene.Net.Analysis.Fr.FrenchStemFilter.stemmer">
            <summary>
            The actual token in the input stream.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemFilter.IncrementToken">
            <returns>  Returns true for the next token in the stream, or false at EOS </returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Fr.FrenchStemFilter.Stemmer">
            <summary>
            Set a alternative/custom <see cref="T:Lucene.Net.Analysis.Fr.FrenchStemmer"/> for this filter.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Fr.FrenchStemmer">
            <summary>
            A stemmer for French words. 
            <para/>
            The algorithm is based on the work of
            Dr Martin Porter on his snowball project<para/>
            refer to http://snowball.sourceforge.net/french/stemmer.html
            (French stemming algorithm) for details
            </summary>
            @deprecated Use <see cref="T:Lucene.Net.Tartarus.Snowball.Ext.FrenchStemmer"/> instead, 
            which has the same functionality. This filter will be removed in Lucene 4.0
        </member>
        <member name="F:Lucene.Net.Analysis.Fr.FrenchStemmer.sb">
            <summary>
            Buffer for the terms while stemming them.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Fr.FrenchStemmer.tb">
            <summary>
            A temporary buffer, used to reconstruct R2
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Fr.FrenchStemmer.R0">
            <summary>
            Region R0 is equal to the whole buffer
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Fr.FrenchStemmer.RV">
            <summary>
            Region RV
            "If the word begins with two vowels, RV is the region after the third letter,
            otherwise the region after the first vowel not at the beginning of the word,
            or the end of the word if these positions cannot be found."
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Fr.FrenchStemmer.R1">
            <summary>
            Region R1
            "R1 is the region after the first non-vowel following a vowel
            or is the null region at the end of the word if there is no such non-vowel"
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Fr.FrenchStemmer.R2">
            <summary>
            Region R2
            "R2 is the region after the first non-vowel in R1 following a vowel
            or is the null region at the end of the word if there is no such non-vowel"
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Fr.FrenchStemmer.suite">
            <summary>
            Set to true if we need to perform step 2
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Fr.FrenchStemmer.modified">
            <summary>
            Set to true if the buffer was modified
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.Stem(System.String)">
            <summary>
            Stems the given term to a unique <c>discriminator</c>.
            </summary>
            <param name="term"> The term that should be stemmed </param>
            <returns> Discriminator for <paramref name="term"/> </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.SetStrings">
            <summary>
            Sets the search region strings
            it needs to be done each time the buffer was modified
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.Step1">
            <summary>
            First step of the Porter Algorithm<para/>
            refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.Step2a">
            <summary>
            Second step (A) of the Porter Algorithm<para/>
            Will be performed if nothing changed from the first step
            or changed were done in the amment, emment, ments or ment suffixes<para/>
            refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
            </summary>
            <returns> true if something changed in the <see cref="T:System.Text.StringBuilder"/> </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.Step2b">
            <summary>
            Second step (B) of the Porter Algorithm<para/>
            Will be performed if step 2 A was performed unsuccessfully<para/>
            refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.Step3">
            <summary>
            Third step of the Porter Algorithm<para/>
            refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.Step4">
            <summary>
            Fourth step of the Porter Algorithm<para/>
            refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.Step5">
            <summary>
            Fifth step of the Porter Algorithm<para/>
            refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.Step6">
            <summary>
            Sixth (and last!) step of the Porter Algorithm<para/>
            refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.DeleteFromIfPrecededIn(System.String,System.String[],System.String,System.String)">
            <summary>
            Delete a suffix searched in zone "source" if zone "from" contains prefix + search string
            </summary>
            <param name="source"> the primary source zone for search </param>
            <param name="search"> the strings to search for suppression </param>
            <param name="from"> the secondary source zone for search </param>
            <param name="prefix"> the prefix to add to the search string to test </param>
            <returns> true if modified </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.DeleteFromIfTestVowelBeforeIn(System.String,System.String[],System.Boolean,System.String)">
            <summary>
            Delete a suffix searched in zone "source" if the preceding letter is (or isn't) a vowel
            </summary>
            <param name="source"> the primary source zone for search </param>
            <param name="search"> the strings to search for suppression </param>
            <param name="vowel"> true if we need a vowel before the search string </param>
            <param name="from"> the secondary source zone for search (where vowel could be) </param>
            <returns> true if modified </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.DeleteButSuffixFrom(System.String,System.String[],System.String,System.Boolean)">
            <summary>
            Delete a suffix searched in zone "source" if preceded by the prefix
            </summary>
            <param name="source"> the primary source zone for search </param>
            <param name="search"> the strings to search for suppression </param>
            <param name="prefix"> the prefix to add to the search string to test </param>
            <param name="without"> true if it will be deleted even without prefix found </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.DeleteButSuffixFromElseReplace(System.String,System.String[],System.String,System.Boolean,System.String,System.String)">
            <summary>
            Delete a suffix searched in zone "source" if preceded by prefix<para/>
            or replace it with the replace string if preceded by the prefix in the zone "from"<para/>
            or delete the suffix if specified
            </summary>
            <param name="source"> the primary source zone for search </param>
            <param name="search"> the strings to search for suppression </param>
            <param name="prefix"> the prefix to add to the search string to test </param>
            <param name="without"> true if it will be deleted even without prefix found </param>
            <param name="from"> the secondary source zone for search </param>
            <param name="replace"> the replacement string </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.ReplaceFrom(System.String,System.String[],System.String)">
            <summary>
            Replace a search string with another within the source zone
            </summary>
            <param name="source"> the source zone for search </param>
            <param name="search"> the strings to search for replacement </param>
            <param name="replace"> the replacement string </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.DeleteFrom(System.String,System.String[])">
            <summary>
            Delete a search string within the source zone
            </summary>
            <param name="source"> the source zone for search </param>
            <param name="suffix"> the strings to search for suppression </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.IsVowel(System.Char)">
            <summary>
            Test if a char is a french vowel, including accentuated ones
            </summary>
            <param name="ch"> the char to test </param>
            <returns> true if the char is a vowel </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.RetrieveR(System.Text.StringBuilder)">
            <summary>
            Retrieve the "R zone" (1 or 2 depending on the buffer) and return the corresponding string<para/>
            "R is the region after the first non-vowel following a vowel
            or is the null region at the end of the word if there is no such non-vowel" </summary>
            <param name="buffer"> the in buffer </param>
            <returns> the resulting string </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.RetrieveRV(System.Text.StringBuilder)">
            <summary>
            Retrieve the "RV zone" from a buffer an return the corresponding string<para/>
            "If the word begins with two vowels, RV is the region after the third letter,
            otherwise the region after the first vowel not at the beginning of the word,
            or the end of the word if these positions cannot be found." </summary>
            <param name="buffer"> the in buffer </param>
            <returns> the resulting string </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.TreatVowels(System.Text.StringBuilder)">
            <summary>
            Turns u and i preceded AND followed by a vowel to UpperCase<para/>
            Turns y preceded OR followed by a vowel to UpperCase<para/>
            Turns u preceded by q to UpperCase
            </summary>
            <param name="buffer"> the buffer to treat </param>
            <returns> the treated buffer </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Fr.FrenchStemmer.IsStemmable(System.String)">
            <summary>
            Checks a term if it can be processed correctly.
            </summary>
            <returns> true if, and only if, the given term consists in letters. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Ga.IrishAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Irish.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Ga.IrishAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Irish stopwords. </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Ga.IrishAnalyzer.HYPHENATIONS">
            <summary>
            When StandardTokenizer splits t‑athair into {t, athair}, we don't
            want to cause a position increment, otherwise there will be problems
            with phrase queries versus tAthair (which would not have a gap).
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Ga.IrishAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Ga.IrishAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Ga.IrishAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ga.IrishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.Ga.IrishAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ga.IrishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Ga.IrishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Ga.IrishAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Ga.IrishLowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Ga.IrishLowerCaseFilter">
            <summary>
            Normalises token text to lower case, handling t-prothesis
            and n-eclipsis (i.e., that 'nAthair' should become 'n-athair')
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ga.IrishLowerCaseFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Create an <see cref="T:Lucene.Net.Analysis.Ga.IrishLowerCaseFilter"/> that normalises Irish token text.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Ga.IrishLowerCaseFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Ga.IrishLowerCaseFilter"/>. 
            <code>
            &lt;fieldType name="text_ga" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.IrishLowerCaseFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ga.IrishLowerCaseFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Ga.IrishLowerCaseFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Gl.GalicianAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Galician.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Gl.GalicianAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Galician stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Gl.GalicianAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Gl.GalicianAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Gl.GalicianAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Gl.GalicianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.Gl.GalicianAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Gl.GalicianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Gl.GalicianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Gl.GalicianAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.Gl.GalicianStemFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Gl.GalicianMinimalStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Gl.GalicianMinimalStemmer"/> to stem 
            Galician words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Gl.GalicianMinimalStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Gl.GalicianMinimalStemFilter"/>. 
            <code>
            &lt;fieldType name="text_glplural" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.GalicianMinimalStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Gl.GalicianMinimalStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Gl.GalicianMinimalStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Gl.GalicianMinimalStemmer">
            <summary>
            Minimal Stemmer for Galician
            <para>
            This follows the "RSLP-S" algorithm, but modified for Galician.
            Hence this stemmer only applies the plural reduction step of:
            "Regras do lematizador para o galego"
            </para>
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Pt.RSLPStemmerBase"/>
        </member>
        <member name="T:Lucene.Net.Analysis.Gl.GalicianStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Gl.GalicianStemmer"/> to stem 
            Galician words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Gl.GalicianStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Gl.GalicianStemFilter"/>. 
            <code>
            &lt;fieldType name="text_glstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.GalicianStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Gl.GalicianStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Gl.GalicianStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Gl.GalicianStemmer">
            <summary>
            Galician stemmer implementing "Regras do lematizador para o galego".
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Pt.RSLPStemmerBase"/>
            <a href="http://bvg.udc.es/recursos_lingua/stemming.jsp">Description of rules</a>
        </member>
        <member name="M:Lucene.Net.Analysis.Gl.GalicianStemmer.Stem(System.Char[],System.Int32)">
            <param name="s"> buffer, oversized to at least <code>len+1</code> </param>
            <param name="len"> initial valid length of buffer </param>
            <returns> new valid length, stemmed </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Hi.HindiAnalyzer">
            <summary>
            Analyzer for Hindi.
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating HindiAnalyzer:
            <list type="bullet">
                <item><description> As of 3.6, StandardTokenizer is used for tokenization</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Hi.HindiAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Hindi stopwords.
            <para/>
            Default stopword list is from http://members.unine.ch/jacques.savoy/clef/index.html
            The stopword list is BSD-Licensed.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Hi.HindiAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop-words set. </summary>
            <returns> an unmodifiable instance of the default stop-words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Hi.HindiAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Hi.HindiAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Hi.HindiAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words
            </summary>
            <param name="version"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a stemming exclusion set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Hi.HindiAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words 
            </summary>
            <param name="version"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Hi.HindiAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words:
            <see cref="F:Lucene.Net.Analysis.Hi.HindiAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Hi.HindiAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            used to tokenize all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from a <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.In.IndicNormalizationFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Hi.HindiNormalizationFilter"/>, <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/>
                    if a stem exclusion set is provided, <see cref="T:Lucene.Net.Analysis.Hi.HindiStemFilter"/>, and
                    Hindi Stop words </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Hi.HindiNormalizationFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Hi.HindiNormalizer"/> to normalize the
            orthography.
            <para>
            In some cases the normalization may cause unrelated terms to conflate, so
            to prevent terms from being normalized use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para> 
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Hi.HindiNormalizer"/>
        </member>
        <member name="T:Lucene.Net.Analysis.Hi.HindiNormalizationFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Hi.HindiNormalizationFilter"/>. 
            <code>
            &lt;fieldType name="text_hinormal" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.HindiNormalizationFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Hi.HindiNormalizationFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Hi.HindiNormalizationFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Hi.HindiNormalizer">
            <summary>
            Normalizer for Hindi.
            <para>
            Normalizes text to remove some differences in spelling variations.
            </para>
            <para>
            Implements the Hindi-language specific algorithm specified in:
            <c>Word normalization in Indian languages</c>
            Prasad Pingali and Vasudeva Varma.
            http://web2py.iiit.ac.in/publications/default/download/inproceedings.pdf.3fe5b38c-02ee-41ce-9a8f-3e745670be32.pdf
            </para>
            <para>
            with the following additions from <c>Hindi CLIR in Thirty Days</c>
            Leah S. Larkey, Margaret E. Connell, and Nasreen AbdulJaleel.
            http://maroo.cs.umass.edu/pub/web/getpdf.php?id=454:
            <list type="bullet">
                <item><description>Internal Zero-width joiner and Zero-width non-joiners are removed</description></item>
                <item><description>In addition to chandrabindu, NA+halant is normalized to anusvara</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Hi.HindiNormalizer.Normalize(System.Char[],System.Int32)">
            <summary>
            Normalize an input buffer of Hindi text
            </summary>
            <param name="s"> input buffer </param>
            <param name="len"> length of input buffer </param>
            <returns> length of input buffer after normalization </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Hi.HindiStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Hi.HindiStemmer"/> to stem Hindi words.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Hi.HindiStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Hi.HindiStemFilter"/>. 
            <code>
            &lt;fieldType name="text_histem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.HindiStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Hi.HindiStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Hi.HindiStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Hi.HindiStemmer">
            <summary>
            Light Stemmer for Hindi.
            <para>
            Implements the algorithm specified in:
            <c>A Lightweight Stemmer for Hindi</c>
            Ananthakrishnan Ramanathan and Durgesh D Rao.
            http://computing.open.ac.uk/Sites/EACLSouthAsia/Papers/p6-Ramanathan.pdf
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Hunspell.Dictionary">
            <summary>
            In-memory structure for the dictionary (.dic) and affix (.aff)
            data of a hunspell dictionary.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Dictionary.#ctor(System.IO.Stream,System.IO.Stream)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Hunspell.Dictionary"/> containing the information read from the provided <see cref="T:System.IO.Stream"/>s to hunspell affix
            and dictionary files.
            You have to dispose the provided <see cref="T:System.IO.Stream"/>s yourself.
            </summary>
            <param name="affix"> <see cref="T:System.IO.Stream"/> for reading the hunspell affix file (won't be disposed). </param>
            <param name="dictionary"> <see cref="T:System.IO.Stream"/> for reading the hunspell dictionary file (won't be disposed). </param>
            <exception cref="T:System.IO.IOException"> Can be thrown while reading from the <see cref="T:System.IO.Stream"/>s </exception>
            <exception cref="T:System.Exception"> Can be thrown if the content of the files does not meet expected formats </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Dictionary.#ctor(System.IO.Stream,System.Collections.Generic.IList{System.IO.Stream},System.Boolean)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Hunspell.Dictionary"/> containing the information read from the provided <see cref="T:System.IO.Stream"/>s to hunspell affix
            and dictionary files.
            You have to dispose the provided <see cref="T:System.IO.Stream"/>s yourself.
            </summary>
            <param name="affix"> <see cref="T:System.IO.Stream"/> for reading the hunspell affix file (won't be disposed). </param>
            <param name="dictionaries"> <see cref="T:System.IO.Stream"/> for reading the hunspell dictionary files (won't be disposed). </param>
            <param name="ignoreCase"> ignore case? </param>
            <exception cref="T:System.IO.IOException"> Can be thrown while reading from the <see cref="T:System.IO.Stream"/>s </exception>
            <exception cref="T:System.Exception"> Can be thrown if the content of the files does not meet expected formats </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Dictionary.LookupWord(System.Char[],System.Int32,System.Int32)">
            <summary>
            Looks up Hunspell word forms from the dictionary
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Dictionary.LookupPrefix(System.Char[],System.Int32,System.Int32)">
            <summary>
            Looks up HunspellAffix prefixes that have an append that matches the <see cref="T:System.String"/> created from the given <see cref="T:System.Char"/> array, offset and length
            </summary>
            <param name="word"> <see cref="T:System.Char"/> array to generate the <see cref="T:System.String"/> from </param>
            <param name="offset"> Offset in the <see cref="T:System.Char"/> array that the <see cref="T:System.String"/> starts at </param>
            <param name="length"> Length from the offset that the <see cref="T:System.String"/> is </param>
            <returns> List of HunspellAffix prefixes with an append that matches the <see cref="T:System.String"/>, or <c>null</c> if none are found </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Dictionary.LookupSuffix(System.Char[],System.Int32,System.Int32)">
            <summary>
            Looks up HunspellAffix suffixes that have an append that matches the <see cref="T:System.String"/> created from the given <see cref="T:System.Char"/> array, offset and length
            </summary>
            <param name="word"> <see cref="T:System.Char"/> array to generate the <see cref="T:System.String"/> from </param>
            <param name="offset"> Offset in the char array that the <see cref="T:System.String"/> starts at </param>
            <param name="length"> Length from the offset that the <see cref="T:System.String"/> is </param>
            <returns> List of HunspellAffix suffixes with an append that matches the <see cref="T:System.String"/>, or <c>null</c> if none are found </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Dictionary.ReadAffixFile(System.IO.Stream,System.Text.Encoding)">
            <summary>
            Reads the affix file through the provided <see cref="T:System.IO.Stream"/>, building up the prefix and suffix maps
            </summary>
            <param name="affixStream"> <see cref="T:System.IO.Stream"/> to read the content of the affix file from </param>
            <param name="decoder"> <see cref="T:System.Text.Encoding"/> to decode the content of the file </param>
            <exception cref="T:System.IO.IOException"> Can be thrown while reading from the InputStream </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Dictionary.ParseAffix(J2N.Collections.Generic.SortedDictionary{System.String,System.Collections.Generic.IList{System.Nullable{System.Char}}},System.String,System.IO.TextReader,System.String,System.Collections.Generic.IDictionary{System.String,System.Nullable{System.Int32}},System.Collections.Generic.IDictionary{System.String,System.Nullable{System.Int32}})">
            <summary>
            Parses a specific affix rule putting the result into the provided affix map
            </summary>
            <param name="affixes"> <see cref="T:J2N.Collections.Generic.SortedDictionary`2"/> where the result of the parsing will be put </param>
            <param name="header"> Header line of the affix rule </param>
            <param name="reader"> <see cref="T:System.IO.TextReader"/> to read the content of the rule from </param>
            <param name="conditionPattern"> <see cref="M:System.String.Format(System.String,System.Object[])"/> pattern to be used to generate the condition regex
                                    pattern </param>
            <param name="seenPatterns"> map from condition -> index of patterns, for deduplication. </param>
            <param name="seenStrips"></param>
            <exception cref="T:System.IO.IOException"> Can be thrown while reading the rule </exception>
        </member>
        <member name="F:Lucene.Net.Analysis.Hunspell.Dictionary.ENCODING_PATTERN">
            <summary>
            pattern accepts optional BOM + SET + any whitespace </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Dictionary.GetDictionaryEncoding(System.IO.Stream)">
            <summary>
            Parses the encoding specified in the affix file readable through the provided <see cref="T:System.IO.Stream"/>
            </summary>
            <param name="affix"> <see cref="T:System.IO.Stream"/> for reading the affix file </param>
            <returns> Encoding specified in the affix file </returns>
            <exception cref="T:System.IO.IOException"> Can be thrown while reading from the <see cref="T:System.IO.Stream"/> </exception>
            <exception cref="T:System.Exception"> Thrown if the first non-empty non-comment line read from the file does not adhere to the format <c>SET &lt;encoding&gt;</c></exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Dictionary.GetSystemEncoding(System.String)">
            <summary>
            Retrieves the <see cref="T:System.Text.Encoding"/> for the given encoding.  Note, This isn't perfect as I think ISCII-DEVANAGARI and
            MICROSOFT-CP1251 etc are allowed...
            </summary>
            <param name="encoding"> Encoding to retrieve the <see cref="T:System.Text.Encoding"/> instance for </param>
            <returns> <see cref="T:System.Text.Encoding"/> for the given encoding <see cref="T:System.String"/> </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Dictionary.GetFlagParsingStrategy(System.String)">
            <summary>
            Determines the appropriate <see cref="T:Lucene.Net.Analysis.Hunspell.Dictionary.FlagParsingStrategy"/> based on the FLAG definition line taken from the affix file
            </summary>
            <param name="flagLine"> Line containing the flag information </param>
            <returns> <see cref="T:Lucene.Net.Analysis.Hunspell.Dictionary.FlagParsingStrategy"/> that handles parsing flags in the way specified in the FLAG definition </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Dictionary.ReadDictionaryFiles(System.Collections.Generic.IList{System.IO.Stream},System.Text.Encoding,Lucene.Net.Util.Fst.Builder{Lucene.Net.Util.Int32sRef})">
            <summary>
            Reads the dictionary file through the provided <see cref="T:System.IO.Stream"/>s, building up the words map
            </summary>
            <param name="dictionaries"> <see cref="T:System.IO.Stream"/>s to read the dictionary file through </param>
            <param name="decoder"> <see cref="T:System.Text.Encoding"/> used to decode the contents of the file </param>
            <param name="words"></param>
            <exception cref="T:System.IO.IOException"> Can be thrown while reading from the file </exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Hunspell.Dictionary.FlagParsingStrategy">
            <summary>
            Abstraction of the process of parsing flags taken from the affix and dic files
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Dictionary.FlagParsingStrategy.ParseFlag(System.String)">
            <summary>
            Parses the given <see cref="T:System.String"/> into a single flag
            </summary>
            <param name="rawFlag"> <see cref="T:System.String"/> to parse into a flag </param>
            <returns> Parsed flag </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Dictionary.FlagParsingStrategy.ParseFlags(System.String)">
            <summary>
            Parses the given <see cref="T:System.String"/> into multiple flags
            </summary>
            <param name="rawFlags"> <see cref="T:System.String"/> to parse into flags </param>
            <returns> Parsed flags </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Hunspell.Dictionary.SimpleFlagParsingStrategy">
            <summary>
            Simple implementation of <see cref="T:Lucene.Net.Analysis.Hunspell.Dictionary.FlagParsingStrategy"/> that treats the chars in each <see cref="T:System.String"/> as a individual flags.
            Can be used with both the ASCII and UTF-8 flag types.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Hunspell.Dictionary.NumFlagParsingStrategy">
            <summary>
            Implementation of <see cref="T:Lucene.Net.Analysis.Hunspell.Dictionary.FlagParsingStrategy"/> that assumes each flag is encoded in its numerical form.  In the case
            of multiple flags, each number is separated by a comma.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Hunspell.Dictionary.DoubleASCIIFlagParsingStrategy">
            <summary>
            Implementation of <see cref="T:Lucene.Net.Analysis.Hunspell.Dictionary.FlagParsingStrategy"/> that assumes each flag is encoded as two ASCII characters whose codes
            must be combined into a single character.
            
            TODO (rmuir) test
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Hunspell.HunspellStemFilter">
            <summary>
            <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that uses hunspell affix rules and words to stem tokens.  
            Since hunspell supports a word having multiple stems, this filter can emit 
            multiple tokens for each consumed token
            
            <para>
            Note: This filter is aware of the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/>. To prevent
            certain terms from being passed to the stemmer
            <see cref="P:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute.IsKeyword"/> should be set to <c>true</c>
            in a previous <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            
            Note: For including the original term as well as the stemmed version, see
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.KeywordRepeatFilterFactory"/>
            </para>
            
            @lucene.experimental
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.HunspellStemFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Hunspell.Dictionary)">
            <summary>
            Create a <see cref="T:Lucene.Net.Analysis.Hunspell.HunspellStemFilter"/> outputting all possible stems. </summary>
            <seealso cref="M:Lucene.Net.Analysis.Hunspell.HunspellStemFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Hunspell.Dictionary,System.Boolean)"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.HunspellStemFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Hunspell.Dictionary,System.Boolean)">
            <summary>
            Create a <see cref="T:Lucene.Net.Analysis.Hunspell.HunspellStemFilter"/> outputting all possible stems. </summary>
            <seealso cref="M:Lucene.Net.Analysis.Hunspell.HunspellStemFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Hunspell.Dictionary,System.Boolean,System.Boolean)"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.HunspellStemFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Hunspell.Dictionary,System.Boolean,System.Boolean)">
            <summary>
            Creates a new HunspellStemFilter that will stem tokens from the given <see cref="T:Lucene.Net.Analysis.TokenStream"/> using affix rules in the provided
            Dictionary
            </summary>
            <param name="input"> <see cref="T:Lucene.Net.Analysis.TokenStream"/> whose tokens will be stemmed </param>
            <param name="dictionary"> Hunspell <see cref="T:Lucene.Net.Analysis.Hunspell.Dictionary"/> containing the affix rules and words that will be used to stem the tokens </param>
            <param name="dedup"> remove duplicates </param>
            <param name="longestOnly"> true if only the longest term should be output. </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Hunspell.HunspellStemFilterFactory">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Util.TokenFilterFactory"/> that creates instances of <see cref="T:Lucene.Net.Analysis.Hunspell.HunspellStemFilter"/>.
            Example config for British English:
            <code>
            &lt;filter class=&quot;solr.HunspellStemFilterFactory&quot;
                    dictionary=&quot;en_GB.dic,my_custom.dic&quot;
                    affix=&quot;en_GB.aff&quot; 
                    ignoreCase=&quot;false&quot;
                    longestOnly=&quot;false&quot; /&gt;</code>
            Both parameters dictionary and affix are mandatory.
            Dictionaries for many languages are available through the OpenOffice project.
            
            See <a href="http://wiki.apache.org/solr/Hunspell">http://wiki.apache.org/solr/Hunspell</a>
            @lucene.experimental
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.HunspellStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Hunspell.HunspellStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Hunspell.Stemmer">
            <summary>
            Stemmer uses the affix rules declared in the <see cref="T:Lucene.Net.Analysis.Hunspell.Dictionary"/> to generate one or more stems for a word.  It
            conforms to the algorithm in the original hunspell algorithm, including recursive suffix stripping.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Stemmer.#ctor(Lucene.Net.Analysis.Hunspell.Dictionary)">
            <summary>
            Constructs a new Stemmer which will use the provided <see cref="T:Lucene.Net.Analysis.Hunspell.Dictionary"/> to create its stems.
            </summary>
            <param name="dictionary"> <see cref="T:Lucene.Net.Analysis.Hunspell.Dictionary"/> that will be used to create the stems </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Stemmer.Stem(System.String)">
            <summary>
            Find the stem(s) of the provided word.
            </summary>
            <param name="word"> Word to find the stems for </param>
            <returns> <see cref="T:System.Collections.Generic.IList`1"/> of stems for the word </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Stemmer.Stem(System.Char[],System.Int32)">
            <summary>
            Find the stem(s) of the provided word
            </summary>
            <param name="word"> Word to find the stems for </param>
            <param name="length"> length </param>
            <returns> <see cref="T:System.Collections.Generic.IList`1"/> of stems for the word </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Stemmer.UniqueStems(System.Char[],System.Int32)">
            <summary>
            Find the unique stem(s) of the provided word
            </summary>
            <param name="word"> Word to find the stems for </param>
            <param name="length"> length </param>
            <returns> <see cref="T:System.Collections.Generic.IList`1"/> of stems for the word </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Stemmer.Stem(System.Char[],System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Boolean,System.Boolean,System.Boolean,System.Boolean)">
            <summary>
            Generates a list of stems for the provided word
            </summary>
            <param name="word"> Word to generate the stems for </param>
            <param name="length"> length </param>
            <param name="previous"> previous affix that was removed (so we dont remove same one twice) </param>
            <param name="prevFlag"> Flag from a previous stemming step that need to be cross-checked with any affixes in this recursive step </param>
            <param name="prefixFlag"> flag of the most inner removed prefix, so that when removing a suffix, its also checked against the word </param>
            <param name="recursionDepth"> current recursiondepth </param>
            <param name="doPrefix"> true if we should remove prefixes </param>
            <param name="doSuffix"> true if we should remove suffixes </param>
            <param name="previousWasPrefix"> true if the previous removal was a prefix:
                   if we are removing a suffix, and it has no continuation requirements, its ok.
                   but two prefixes (COMPLEXPREFIXES) or two suffixes must have continuation requirements to recurse. </param>
            <param name="circumfix"> true if the previous prefix removal was signed as a circumfix
                   this means inner most suffix must also contain circumfix flag. </param>
            <returns> <see cref="T:System.Collections.Generic.IList`1"/> of stems, or empty list if no stems are found </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Stemmer.CheckCondition(System.Int32,System.Char[],System.Int32,System.Int32,System.Char[],System.Int32,System.Int32)">
            <summary>
            checks condition of the concatenation of two strings </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Stemmer.ApplyAffix(System.Char[],System.Int32,System.Int32,System.Int32,System.Int32,System.Boolean,System.Boolean)">
            <summary>
            Applies the affix rule to the given word, producing a list of stems if any are found
            </summary>
            <param name="strippedWord"> Word the affix has been removed and the strip added </param>
            <param name="length"> valid length of stripped word </param>
            <param name="affix"> HunspellAffix representing the affix rule itself </param>
            <param name="prefixFlag"> when we already stripped a prefix, we cant simply recurse and check the suffix, unless both are compatible
                              so we must check dictionary form against both to add it as a stem! </param>
            <param name="recursionDepth"> current recursion depth </param>
            <param name="prefix"> true if we are removing a prefix (false if its a suffix) </param>
            <param name="circumfix"> true if the previous prefix removal was signed as a circumfix
                   this means inner most suffix must also contain circumfix flag. </param>
            <returns> <see cref="T:System.Collections.Generic.IList`1"/> of stems for the word, or an empty list if none are found </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Hunspell.Stemmer.HasCrossCheckedFlag(System.Char,System.Char[],System.Boolean)">
            <summary>
            Checks if the given flag cross checks with the given array of flags
            </summary>
            <param name="flag"> Flag to cross check with the array of flags </param>
            <param name="flags"> Array of flags to cross check against.  Can be <c>null</c> </param>
            <param name="matchEmpty"> If true, will match a zero length flags array. </param>
            <returns> <c>true</c> if the flag is found in the array or the array is <c>null</c>, <c>false</c> otherwise </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Hu.HungarianAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Hungarian.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Hu.HungarianAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Hungarian stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Hu.HungarianAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Hu.HungarianAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Hu.HungarianAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Hu.HungarianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.Hu.HungarianAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Hu.HungarianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Hu.HungarianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Hu.HungarianAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Hu.HungarianLightStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Hu.HungarianLightStemmer"/> to stem
            Hungarian words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Hu.HungarianLightStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Hu.HungarianLightStemFilter"/>. 
            <code>
            &lt;fieldType name="text_hulgtstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.HungarianLightStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Hu.HungarianLightStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Hu.HungarianLightStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Hu.HungarianLightStemmer">
            <summary>
            Light Stemmer for Hungarian.
            <para>
            This stemmer implements the "UniNE" algorithm in:
            <c>Light Stemming Approaches for the French, Portuguese, German and Hungarian Languages</c>
            Jacques Savoy
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Hy.ArmenianAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Armenian.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Hy.ArmenianAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Armenian stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Hy.ArmenianAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Hy.ArmenianAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Hy.ArmenianAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Hy.ArmenianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.Hy.ArmenianAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Hy.ArmenianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Hy.ArmenianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Hy.ArmenianAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Id.IndonesianAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Indonesian (Bahasa)
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Id.IndonesianAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Indonesian stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Id.IndonesianAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop-words set. </summary>
            <returns> an unmodifiable instance of the default stop-words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Id.IndonesianAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Id.IndonesianAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Id.IndonesianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.Id.IndonesianAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Id.IndonesianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words
            </summary>
            <param name="matchVersion">
                     lucene compatibility version </param>
            <param name="stopwords">
                     a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Id.IndonesianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop word. If a none-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            <see cref="T:Lucene.Net.Analysis.Id.IndonesianStemFilter"/>.
            </summary>
            <param name="matchVersion">
                     lucene compatibility version </param>
            <param name="stopwords">
                     a stopword set </param>
            <param name="stemExclusionSet">
                     a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Id.IndonesianAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            used to tokenize all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>, <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/>
                    if a stem exclusion set is provided and <see cref="T:Lucene.Net.Analysis.Id.IndonesianStemFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Id.IndonesianStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Id.IndonesianStemmer"/> to stem Indonesian words.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Id.IndonesianStemFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Calls <see cref="M:Lucene.Net.Analysis.Id.IndonesianStemFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Boolean)">IndonesianStemFilter(input, true)</see>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Id.IndonesianStemFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Boolean)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Id.IndonesianStemFilter"/>.
            <para>
            If <paramref name="stemDerivational"/> is false, 
            only inflectional suffixes (particles and possessive pronouns) are stemmed.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Id.IndonesianStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Id.IndonesianStemFilter"/>. 
            <code>
            &lt;fieldType name="text_idstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.IndonesianStemFilterFactory" stemDerivational="true"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Id.IndonesianStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Id.IndonesianStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Id.IndonesianStemmer">
            <summary>
            Stemmer for Indonesian.
            <para>
            Stems Indonesian words with the algorithm presented in:
            <c>A Study of Stemming Effects on Information Retrieval in 
            Bahasa Indonesia</c>, Fadillah Z Tala.
            http://www.illc.uva.nl/Publications/ResearchReports/MoL-2003-02.text.pdf
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Id.IndonesianStemmer.Stem(System.Char[],System.Int32,System.Boolean)">
            <summary>
            Stem a term (returning its new length).
            <para>
            Use <paramref name="stemDerivational"/> to control whether full stemming
            or only light inflectional stemming is done.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.In.IndicNormalizationFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.In.IndicNormalizer"/> to normalize text
            in Indian Languages.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.In.IndicNormalizationFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.In.IndicNormalizationFilter"/>. 
            <code>
            &lt;fieldType name="text_innormal" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.IndicNormalizationFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.In.IndicNormalizationFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.In.IndicNormalizationFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.In.IndicNormalizer">
            <summary>
            Normalizes the Unicode representation of text in Indian languages.
            <para>
            Follows guidelines from Unicode 5.2, chapter 6, South Asian Scripts I
            and graphical decompositions from http://ldc.upenn.edu/myl/IndianScriptsUnicode.html
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.In.IndicNormalizer.decompositions">
            <summary>
            Decompositions according to Unicode 5.2, 
            and http://ldc.upenn.edu/myl/IndianScriptsUnicode.html
            <para/>
            Most of these are not handled by unicode normalization anyway.
            <para/>
            The numbers here represent offsets into the respective codepages,
            with -1 representing null and 0xFF representing zero-width joiner.
            <para/>
            the columns are: ch1, ch2, ch3, res, flags
            ch1, ch2, and ch3 are the decomposition
            res is the composition, and flags are the scripts to which it applies.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.In.IndicNormalizer.Normalize(System.Char[],System.Int32)">
            <summary>
            Normalizes input text, and returns the new length.
            The length will always be less than or equal to the existing length.
            </summary>
            <param name="text"> input text </param>
            <param name="len"> valid length </param>
            <returns> normalized length </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.In.IndicNormalizer.Compose(System.Int32,System.Text.RegularExpressions.Regex,Lucene.Net.Analysis.In.IndicNormalizer.ScriptData,System.Char[],System.Int32,System.Int32)">
            <summary>
            Compose into standard form any compositions in the decompositions table.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.In.IndicNormalizer.GetBlockForChar(System.Char)">
            <summary>
            LUCENENET: Returns the unicode block for the specified character
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.In.IndicTokenizer">
            <summary>
            Simple Tokenizer for text in Indian Languages. </summary>
            @deprecated (3.6) Use <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> instead. 
        </member>
        <member name="T:Lucene.Net.Analysis.It.ItalianAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Italian.
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.It.ItalianAnalyzer"/>:
            <list type="bullet">
                <item><description> As of 3.6, <see cref="T:Lucene.Net.Analysis.It.ItalianLightStemFilter"/> is used for less aggressive stemming.</description></item>
                <item><description> As of 3.2, <see cref="T:Lucene.Net.Analysis.Util.ElisionFilter"/> with a set of Italian 
                   contractions is used by default.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.It.ItalianAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Italian stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.It.ItalianAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.It.ItalianAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.It.ItalianAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.It.ItalianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.It.ItalianAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
        </member>
        <member name="M:Lucene.Net.Analysis.It.ItalianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.It.ItalianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.It.ItalianAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Util.ElisionFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.It.ItalianLightStemFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.It.ItalianLightStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.It.ItalianLightStemmer"/> to stem Italian
            words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.It.ItalianLightStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.It.ItalianLightStemFilter"/>. 
            <code>
            &lt;fieldType name="text_itlgtstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.ItalianLightStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code> 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.It.ItalianLightStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.It.ItalianLightStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.It.ItalianLightStemmer">
            <summary>
            Light Stemmer for Italian.
            <para>
            This stemmer implements the algorithm described in:
            <c>Report on CLEF-2001 Experiments</c>
            Jacques Savoy
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Lv.LatvianAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Latvian.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Lv.LatvianAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Latvian stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Lv.LatvianAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Lv.LatvianAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Lv.LatvianAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Lv.LatvianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.Lv.LatvianAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Lv.LatvianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Lv.LatvianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Lv.LatvianAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.Lv.LatvianStemFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Lv.LatvianStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Lv.LatvianStemmer"/> to stem Latvian
            words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Lv.LatvianStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Lv.LatvianStemFilter"/>. 
            <code>
            &lt;fieldType name="text_lvstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.LatvianStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Lv.LatvianStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Lv.LatvianStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Lv.LatvianStemmer">
            <summary>
            Light stemmer for Latvian.
            <para>
            This is a light version of the algorithm in Karlis Kreslin's PhD thesis
            <c>A stemming algorithm for Latvian</c> with the following modifications:
            <list type="bullet">
              <item><description>Only explicitly stems noun and adjective morphology</description></item>
              <item><description>Stricter length/vowel checks for the resulting stems (verb etc suffix stripping is removed)</description></item>
              <item><description>Removes only the primary inflectional suffixes: case and number for nouns ; 
                  case, number, gender, and definitiveness for adjectives.</description></item>
              <item><description>Palatalization is only handled when a declension II,V,VI noun suffix is removed.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Lv.LatvianStemmer.Stem(System.Char[],System.Int32)">
            <summary>
            Stem a latvian word. returns the new adjusted length.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Lv.LatvianStemmer.Unpalatalize(System.Char[],System.Int32)">
            <summary>
            Most cases are handled except for the ambiguous ones:
            <list type="bullet">
                <item><description> s -> š</description></item>
                <item><description> t -> š</description></item>
                <item><description> d -> ž</description></item>
                <item><description> z -> ž</description></item>
            </list>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Lv.LatvianStemmer.NumVowels(System.Char[],System.Int32)">
            <summary>
            Count the vowels in the string, we always require at least
            one in the remaining stem to accept it.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.ASCIIFoldingFilter">
            <summary>
            This class converts alphabetic, numeric, and symbolic Unicode characters
            which are not in the first 127 ASCII characters (the "Basic Latin" Unicode
            block) into their ASCII equivalents, if one exists.
            <para/>
            Characters from the following Unicode blocks are converted; however, only
            those characters with reasonable ASCII alternatives are converted:
            
            <ul>
              <item><description>C1 Controls and Latin-1 Supplement: <a href="http://www.unicode.org/charts/PDF/U0080.pdf">http://www.unicode.org/charts/PDF/U0080.pdf</a></description></item>
              <item><description>Latin Extended-A: <a href="http://www.unicode.org/charts/PDF/U0100.pdf">http://www.unicode.org/charts/PDF/U0100.pdf</a></description></item>
              <item><description>Latin Extended-B: <a href="http://www.unicode.org/charts/PDF/U0180.pdf">http://www.unicode.org/charts/PDF/U0180.pdf</a></description></item>
              <item><description>Latin Extended Additional: <a href="http://www.unicode.org/charts/PDF/U1E00.pdf">http://www.unicode.org/charts/PDF/U1E00.pdf</a></description></item>
              <item><description>Latin Extended-C: <a href="http://www.unicode.org/charts/PDF/U2C60.pdf">http://www.unicode.org/charts/PDF/U2C60.pdf</a></description></item>
              <item><description>Latin Extended-D: <a href="http://www.unicode.org/charts/PDF/UA720.pdf">http://www.unicode.org/charts/PDF/UA720.pdf</a></description></item>
              <item><description>IPA Extensions: <a href="http://www.unicode.org/charts/PDF/U0250.pdf">http://www.unicode.org/charts/PDF/U0250.pdf</a></description></item>
              <item><description>Phonetic Extensions: <a href="http://www.unicode.org/charts/PDF/U1D00.pdf">http://www.unicode.org/charts/PDF/U1D00.pdf</a></description></item>
              <item><description>Phonetic Extensions Supplement: <a href="http://www.unicode.org/charts/PDF/U1D80.pdf">http://www.unicode.org/charts/PDF/U1D80.pdf</a></description></item>
              <item><description>General Punctuation: <a href="http://www.unicode.org/charts/PDF/U2000.pdf">http://www.unicode.org/charts/PDF/U2000.pdf</a></description></item>
              <item><description>Superscripts and Subscripts: <a href="http://www.unicode.org/charts/PDF/U2070.pdf">http://www.unicode.org/charts/PDF/U2070.pdf</a></description></item>
              <item><description>Enclosed Alphanumerics: <a href="http://www.unicode.org/charts/PDF/U2460.pdf">http://www.unicode.org/charts/PDF/U2460.pdf</a></description></item>
              <item><description>Dingbats: <a href="http://www.unicode.org/charts/PDF/U2700.pdf">http://www.unicode.org/charts/PDF/U2700.pdf</a></description></item>
              <item><description>Supplemental Punctuation: <a href="http://www.unicode.org/charts/PDF/U2E00.pdf">http://www.unicode.org/charts/PDF/U2E00.pdf</a></description></item>
              <item><description>Alphabetic Presentation Forms: <a href="http://www.unicode.org/charts/PDF/UFB00.pdf">http://www.unicode.org/charts/PDF/UFB00.pdf</a></description></item>
              <item><description>Halfwidth and Fullwidth Forms: <a href="http://www.unicode.org/charts/PDF/UFF00.pdf">http://www.unicode.org/charts/PDF/UFF00.pdf</a></description></item>
            </ul>
            <para/>
            See: <a href="http://en.wikipedia.org/wiki/Latin_characters_in_Unicode">http://en.wikipedia.org/wiki/Latin_characters_in_Unicode</a>
            <para/>
            For example, '&amp;agrave;' will be replaced by 'a'.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.ASCIIFoldingFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Boolean)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.ASCIIFoldingFilter"/>.
            </summary>
            <param name="input">
                     TokenStream to filter </param>
            <param name="preserveOriginal">
                     should the original tokens be kept on the input stream with a 0 position increment
                     from the folded tokens?
             </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Miscellaneous.ASCIIFoldingFilter.PreserveOriginal">
            <summary>
            Does the filter preserve the original tokens?
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.ASCIIFoldingFilter.FoldToASCII(System.Char[],System.Int32)">
            <summary>
            Converts characters above ASCII to their ASCII equivalents.  For example,
            accents are removed from accented characters. </summary>
            <param name="input"> The string to fold </param>
            <param name="length"> The number of characters in the input string </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.ASCIIFoldingFilter.FoldToASCII(System.Char[],System.Int32,System.Char[],System.Int32,System.Int32)">
            <summary>
            Converts characters above ASCII to their ASCII equivalents.  For example,
            accents are removed from accented characters. 
            <para/>
            @lucene.internal
            </summary>
            <param name="input">     The characters to fold </param>
            <param name="inputPos">  Index of the first character to fold </param>
            <param name="output">    The result of the folding. Should be of size >= <c>length * 4</c>. </param>
            <param name="outputPos"> Index of output where to put the result of the folding </param>
            <param name="length">    The number of characters to fold </param>
            <returns> length of output </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.ASCIIFoldingFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Miscellaneous.ASCIIFoldingFilter"/>.
            <code>
            &lt;fieldType name="text_ascii" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.ASCIIFoldingFilterFactory" preserveOriginal="false"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.ASCIIFoldingFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.ASCIIFoldingFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.CapitalizationFilter">
            <summary>
            A filter to apply normal capitalization rules to Tokens.  It will make the first letter
            capital and the rest lower case.
            <para/>
            This filter is particularly useful to build nice looking facet parameters.  This filter
            is not appropriate if you intend to use a prefix query.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.CapitalizationFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Creates a <see cref="T:Lucene.Net.Analysis.Miscellaneous.CapitalizationFilter"/> with the default parameters using the culture from the current thread.
            <para>
            Calls <see cref="M:Lucene.Net.Analysis.Miscellaneous.CapitalizationFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Boolean,Lucene.Net.Analysis.Util.CharArraySet,System.Boolean,System.Collections.Generic.ICollection{System.Char[]},System.Int32,System.Int32,System.Int32)">
                CapitalizationFilter(in, true, null, true, null, 0, DEFAULT_MAX_WORD_COUNT, DEFAULT_MAX_TOKEN_LENGTH, null)
            </see>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.CapitalizationFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Globalization.CultureInfo)">
            <summary>
            Creates a <see cref="T:Lucene.Net.Analysis.Miscellaneous.CapitalizationFilter"/> with the default parameters and the specified <paramref name="culture"/>.
            <para>
            Calls <see cref="M:Lucene.Net.Analysis.Miscellaneous.CapitalizationFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Boolean,Lucene.Net.Analysis.Util.CharArraySet,System.Boolean,System.Collections.Generic.ICollection{System.Char[]},System.Int32,System.Int32,System.Int32)">
                CapitalizationFilter(in, true, null, true, null, 0, DEFAULT_MAX_WORD_COUNT, DEFAULT_MAX_TOKEN_LENGTH)
            </see>
            </para>
            </summary>
            <param name="in"> input tokenstream </param>
            <param name="culture"> The culture to use for the casing operation. If null, the culture of the current thread will be used. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.CapitalizationFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Boolean,Lucene.Net.Analysis.Util.CharArraySet,System.Boolean,System.Collections.Generic.ICollection{System.Char[]},System.Int32,System.Int32,System.Int32)">
            <summary>
            Creates a <see cref="T:Lucene.Net.Analysis.Miscellaneous.CapitalizationFilter"/> with the specified parameters using the culture from the current thread.</summary>
            <param name="in"> input tokenstream </param>
            <param name="onlyFirstWord"> should each word be capitalized or all of the words? </param>
            <param name="keep"> a keep word list.  Each word that should be kept separated by whitespace. </param>
            <param name="forceFirstLetter"> Force the first letter to be capitalized even if it is in the keep list. </param>
            <param name="okPrefix"> do not change word capitalization if a word begins with something in this list. </param>
            <param name="minWordLength"> how long the word needs to be to get capitalization applied.  If the
                                 minWordLength is 3, "and" &gt; "And" but "or" stays "or". </param>
            <param name="maxWordCount"> if the token contains more then maxWordCount words, the capitalization is
                                assumed to be correct. </param>
            <param name="maxTokenLength"> The maximum length for an individual token. Tokens that exceed this length will not have the capitalization operation performed. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.CapitalizationFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Boolean,Lucene.Net.Analysis.Util.CharArraySet,System.Boolean,System.Collections.Generic.ICollection{System.Char[]},System.Int32,System.Int32,System.Int32,System.Globalization.CultureInfo)">
            <summary>
            Creates a <see cref="T:Lucene.Net.Analysis.Miscellaneous.CapitalizationFilter"/> with the specified parameters and the specified <paramref name="culture"/>. </summary>
            <param name="in"> input tokenstream </param>
            <param name="onlyFirstWord"> should each word be capitalized or all of the words? </param>
            <param name="keep"> a keep word list.  Each word that should be kept separated by whitespace. </param>
            <param name="forceFirstLetter"> Force the first letter to be capitalized even if it is in the keep list. </param>
            <param name="okPrefix"> do not change word capitalization if a word begins with something in this list. </param>
            <param name="minWordLength"> how long the word needs to be to get capitalization applied.  If the
                                 minWordLength is 3, "and" &gt; "And" but "or" stays "or". </param>
            <param name="maxWordCount"> if the token contains more then maxWordCount words, the capitalization is
                                assumed to be correct. </param>
            <param name="maxTokenLength"> The maximum length for an individual token. Tokens that exceed this length will not have the capitalization operation performed. </param>
            <param name="culture"> The culture to use for the casing operation. If null, the culture of the current thread will be used. </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.CapitalizationFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Miscellaneous.CapitalizationFilter"/>.
            <para/>
            The factory takes parameters:<para/>
            "onlyFirstWord" - should each word be capitalized or all of the words?<para/>
            "keep" - a keep word list.  Each word that should be kept separated by whitespace.<para/>
            "keepIgnoreCase - true or false.  If true, the keep list will be considered case-insensitive.<para/>
            "forceFirstLetter" - Force the first letter to be capitalized even if it is in the keep list<para/>
            "okPrefix" - do not change word capitalization if a word begins with something in this list.
            for example if "McK" is on the okPrefix list, the word "McKinley" should not be changed to
            "Mckinley"<para/>
            "minWordLength" - how long the word needs to be to get capitalization applied.  If the
            minWordLength is 3, "and" > "And" but "or" stays "or"<para/>
            "maxWordCount" - if the token contains more then maxWordCount words, the capitalization is
            assumed to be correct.<para/>
            
            <code>
            &lt;fieldType name="text_cptlztn" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.CapitalizationFilterFactory" onlyFirstWord="true"
                      keep="java solr lucene" keepIgnoreCase="false"
                      okPrefix="McK McD McA"/&gt;   
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            
            @since solr 1.3
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.CapitalizationFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.CapitalizationFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.CodepointCountFilter">
            <summary>
            Removes words that are too long or too short from the stream.
            <para>
            Note: Length is calculated as the number of Unicode codepoints.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.CodepointCountFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,System.Int32,System.Int32)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.CodepointCountFilter"/>. This will filter out tokens whose
            <see cref="T:Lucene.Net.Analysis.TokenAttributes.CharTermAttribute"/> is either too short (<see cref="M:J2N.Character.CodePointCount(System.Char[],System.Int32,System.Int32)"/>
            &lt; min) or too long (<see cref="M:J2N.Character.CodePointCount(System.Char[],System.Int32,System.Int32)"/> &gt; max). </summary>
            <param name="version"> the Lucene match version </param>
            <param name="in">      the <see cref="T:Lucene.Net.Analysis.TokenStream"/> to consume </param>
            <param name="min">     the minimum length </param>
            <param name="max">     the maximum length </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.CodepointCountFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Miscellaneous.CodepointCountFilter"/>. 
            <code>
            &lt;fieldType name="text_lngth" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.CodepointCountFilterFactory" min="0" max="1" /&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.CodepointCountFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.CodepointCountFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.EmptyTokenStream">
            <summary>
            An always exhausted token stream.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.HyphenatedWordsFilter">
            <summary>
            When the plain text is extracted from documents, we will often have many words hyphenated and broken into
            two lines. This is often the case with documents where narrow text columns are used, such as newsletters.
            In order to increase search efficiency, this filter puts hyphenated words broken into two lines back together.
            This filter should be used on indexing time only.
            Example field definition in schema.xml:
            <code>
            &lt;fieldtype name="text" class="solr.TextField" positionIncrementGap="100"&gt;
             &lt;analyzer type="index"&gt;
               &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                 &lt;filter class="solr.SynonymFilterFactory" synonyms="index_synonyms.txt" ignoreCase="true" expand="false"/&gt;
                 &lt;filter class="solr.StopFilterFactory" ignoreCase="true"/&gt;
                 &lt;filter class="solr.HyphenatedWordsFilterFactory"/&gt;
                 &lt;filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0"/&gt;
                 &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                 &lt;filter class="solr.RemoveDuplicatesTokenFilterFactory"/&gt;
             &lt;/analyzer&gt;
             &lt;analyzer type="query"&gt;
                 &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                 &lt;filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/&gt;
                 &lt;filter class="solr.StopFilterFactory" ignoreCase="true"/&gt;
                 &lt;filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0"/&gt;
                 &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                 &lt;filter class="solr.RemoveDuplicatesTokenFilterFactory"/&gt;
             &lt;/analyzer&gt;
            &lt;/fieldtype&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.HyphenatedWordsFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.HyphenatedWordsFilter"/>
            </summary>
            <param name="in"> <see cref="T:Lucene.Net.Analysis.TokenStream"/> that will be filtered </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.HyphenatedWordsFilter.IncrementToken">
            <summary>
            Consumers (i.e., <see cref="T:Lucene.Net.Index.IndexWriter"/>) use this method to advance the stream to
            the next token. Implementing classes must implement this method and update
            the appropriate <see cref="T:Lucene.Net.Util.Attribute"/>s with the attributes of the next
            token.
            <para/>
            The producer must make no assumptions about the attributes after the method
            has been returned: the caller may arbitrarily change it. If the producer
            needs to preserve the state for subsequent calls, it can use
            <see cref="M:Lucene.Net.Util.AttributeSource.CaptureState"/> to create a copy of the current attribute state.
            <para/>
            this method is called for every token of a document, so an efficient
            implementation is crucial for good performance. To avoid calls to
            <see cref="M:Lucene.Net.Util.AttributeSource.AddAttribute``1"/> and <see cref="M:Lucene.Net.Util.AttributeSource.GetAttribute``1"/>,
            references to all <see cref="T:Lucene.Net.Util.Attribute"/>s that this stream uses should be
            retrieved during instantiation.
            <para/>
            To ensure that filters and consumers know which attributes are available,
            the attributes must be added during instantiation. Filters and consumers
            are not required to check for availability of attributes in
            <see cref="M:Lucene.Net.Analysis.Miscellaneous.HyphenatedWordsFilter.IncrementToken"/>.
            </summary>
            <returns> false for end of stream; true otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.HyphenatedWordsFilter.Reset">
            <summary>
            This method is called by a consumer before it begins consumption using
            <see cref="M:Lucene.Net.Analysis.Miscellaneous.HyphenatedWordsFilter.IncrementToken"/>.
            <para/>
            Resets this stream to a clean state. Stateful implementations must implement
            this method so that they can be reused, just as if they had been created fresh.
            <para/>
            If you override this method, always call <c>base.Reset()</c>, otherwise
            some internal state will not be correctly reset (e.g., <see cref="T:Lucene.Net.Analysis.Tokenizer"/> will
            throw <see cref="T:System.InvalidOperationException"/> on further usage).
            </summary>
            <remarks>
            <b>NOTE:</b>
            The default implementation chains the call to the input <see cref="T:Lucene.Net.Analysis.TokenStream"/>, so
            be sure to call <c>base.Reset()</c> when overriding this method.
            </remarks>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.HyphenatedWordsFilter.Unhyphenate">
            <summary>
            Writes the joined unhyphenated term
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.HyphenatedWordsFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Miscellaneous.HyphenatedWordsFilter"/>.
            <code>
            &lt;fieldType name="text_hyphn" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.HyphenatedWordsFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.HyphenatedWordsFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.HyphenatedWordsFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.KeepWordFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that only keeps tokens with text contained in the
            required words.  This filter behaves like the inverse of <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>.
            
            @since solr 1.3
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.KeepWordFilter.#ctor(Lucene.Net.Util.LuceneVersion,System.Boolean,Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Util.CharArraySet)">
            @deprecated enablePositionIncrements=false is not supported anymore as of Lucene 4.4. 
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.KeepWordFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.KeepWordFilter"/>.
            <para><c>NOTE</c>: The words set passed to this constructor will be directly
            used by this filter and should not be modified.
            </para>
            </summary>
            <param name="version"> the Lucene match version </param>
            <param name="in">      the <see cref="T:Lucene.Net.Analysis.TokenStream"/> to consume </param>
            <param name="words">   the words to keep </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.KeepWordFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Miscellaneous.KeepWordFilter"/>. 
            <code>
            &lt;fieldType name="text_keepword" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.KeepWordFilterFactory" words="keepwords.txt" ignoreCase="false"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.KeepWordFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.KeepWordFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.KeywordMarkerFilter">
            <summary>
            Marks terms as keywords via the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/>.
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.KeywordMarkerFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.KeywordMarkerFilter"/> </summary>
            <param name="in"> the input stream </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.KeywordMarkerFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Miscellaneous.KeywordMarkerFilter"/>.
            <code>
            &lt;fieldType name="text_keyword" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.KeywordMarkerFilterFactory" protected="protectedkeyword.txt" pattern="^.+er$" ignoreCase="false"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.KeywordMarkerFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.KeywordMarkerFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.KeywordRepeatFilter">
            <summary>
            This TokenFilter emits each incoming token twice once as keyword and once non-keyword, in other words once with
            <see cref="P:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute.IsKeyword"/> set to <c>true</c> and once set to <c>false</c>.
            This is useful if used with a stem filter that respects the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> to index the stemmed and the
            un-stemmed version of a term into the same field.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.KeywordRepeatFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Construct a token stream filtering the given input.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.KeywordRepeatFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Miscellaneous.KeywordRepeatFilter"/>.
            
            Since <see cref="T:Lucene.Net.Analysis.Miscellaneous.KeywordRepeatFilter"/> emits two tokens for every input token, and any tokens that aren't transformed
            later in the analysis chain will be in the document twice. Therefore, consider adding
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.RemoveDuplicatesTokenFilterFactory"/> later in the analysis chain.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.KeywordRepeatFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.KeywordRepeatFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.LengthFilter">
            <summary>
            Removes words that are too long or too short from the stream.
            <para>
            Note: Length is calculated as the number of UTF-16 code units.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.LengthFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,System.Int32,System.Int32)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.LengthFilter"/>. This will filter out tokens whose
            <see cref="T:Lucene.Net.Analysis.TokenAttributes.CharTermAttribute"/> is either too short (<see cref="P:Lucene.Net.Analysis.TokenAttributes.ICharTermAttribute.Length"/>
            &lt; min) or too long (<see cref="P:Lucene.Net.Analysis.TokenAttributes.ICharTermAttribute.Length"/> &gt; max). </summary>
            <param name="version"> the Lucene match version </param>
            <param name="in">      the <see cref="T:Lucene.Net.Analysis.TokenStream"/> to consume </param>
            <param name="min">     the minimum length </param>
            <param name="max">     the maximum length </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.LengthFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Miscellaneous.LengthFilter"/>. 
            <code>
            &lt;fieldType name="text_lngth" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.LengthFilterFactory" min="0" max="1" /&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.LengthFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.LengthFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountAnalyzer">
            <summary>
            This <see cref="T:Lucene.Net.Analysis.Analyzer"/> limits the number of tokens while indexing. It is
            a replacement for the maximum field length setting inside <see cref="T:Lucene.Net.Index.IndexWriter"/>. </summary>
            <seealso cref="T:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountFilter"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountAnalyzer.#ctor(Lucene.Net.Analysis.Analyzer,System.Int32)">
            <summary>
            Build an analyzer that limits the maximum number of tokens per field.
            This analyzer will not consume any tokens beyond the maxTokenCount limit
            </summary>
            <seealso cref="M:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountAnalyzer.#ctor(Lucene.Net.Analysis.Analyzer,System.Int32,System.Boolean)"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountAnalyzer.#ctor(Lucene.Net.Analysis.Analyzer,System.Int32,System.Boolean)">
            <summary>
            Build an analyzer that limits the maximum number of tokens per field. </summary>
            <param name="delegate"> the analyzer to wrap </param>
            <param name="maxTokenCount"> max number of tokens to produce </param>
            <param name="consumeAllTokens"> whether all tokens from the delegate should be consumed even if maxTokenCount is reached. </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountFilter">
            <summary>
            This <see cref="T:Lucene.Net.Analysis.TokenFilter"/> limits the number of tokens while indexing. It is
            a replacement for the maximum field length setting inside <see cref="T:Lucene.Net.Index.IndexWriter"/>.
            <para>
            By default, this filter ignores any tokens in the wrapped <see cref="T:Lucene.Net.Analysis.TokenStream"/>
            once the limit has been reached, which can result in <see cref="M:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountFilter.Reset"/> being 
            called prior to <see cref="M:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountFilter.IncrementToken"/> returning <c>false</c>.  For most 
            <see cref="T:Lucene.Net.Analysis.TokenStream"/> implementations this should be acceptable, and faster 
            then consuming the full stream. If you are wrapping a <see cref="T:Lucene.Net.Analysis.TokenStream"/> 
            which requires that the full stream of tokens be exhausted in order to 
            function properly, use the 
            <see cref="M:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Int32,System.Boolean)"/> consumeAllTokens
            option.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Int32)">
            <summary>
            Build a filter that only accepts tokens up to a maximum number.
            This filter will not consume any tokens beyond the <paramref name="maxTokenCount"/> limit
            </summary>
            <param name="in"> the stream to wrap </param>
            <param name="maxTokenCount"> max number of tokens to produce </param>
            <seealso cref="M:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Int32,System.Boolean)"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Int32,System.Boolean)">
            <summary>
            Build an filter that limits the maximum number of tokens per field. </summary>
            <param name="in"> the stream to wrap </param>
            <param name="maxTokenCount"> max number of tokens to produce </param>
            <param name="consumeAllTokens"> whether all tokens from the input must be consumed even if <paramref name="maxTokenCount"/> is reached. </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountFilter"/>. 
            <code>
            &lt;fieldType name="text_lngthcnt" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.LimitTokenCountFilterFactory" maxTokenCount="10" consumeAllTokens="false" /&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            <para>
            The <see cref="F:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountFilterFactory.consumeAllTokens"/> property is optional and defaults to <c>false</c>.  
            See <see cref="T:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountFilter"/> for an explanation of it's use.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.LimitTokenCountFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.LimitTokenPositionFilter">
            <summary>
            This <see cref="T:Lucene.Net.Analysis.TokenFilter"/> limits its emitted tokens to those with positions that
            are not greater than the configured limit.
            <para>
            By default, this filter ignores any tokens in the wrapped <see cref="T:Lucene.Net.Analysis.TokenStream"/>
            once the limit has been exceeded, which can result in <see cref="M:Lucene.Net.Analysis.Miscellaneous.LimitTokenPositionFilter.Reset"/> being 
            called prior to <see cref="M:Lucene.Net.Analysis.Miscellaneous.LimitTokenPositionFilter.IncrementToken"/> returning <c>false</c>.  For most 
            <see cref="T:Lucene.Net.Analysis.TokenStream"/> implementations this should be acceptable, and faster 
            then consuming the full stream. If you are wrapping a <see cref="T:Lucene.Net.Analysis.TokenStream"/>
            which requires that the full stream of tokens be exhausted in order to 
            function properly, use the 
            <see cref="M:Lucene.Net.Analysis.Miscellaneous.LimitTokenPositionFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Int32,System.Boolean)"/> consumeAllTokens
            option.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.LimitTokenPositionFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Int32)">
            <summary>
            Build a filter that only accepts tokens up to and including the given maximum position.
            This filter will not consume any tokens with position greater than the <paramref name="maxTokenPosition"/> limit.
            </summary>
            <param name="in"> the stream to wrap </param>
            <param name="maxTokenPosition"> max position of tokens to produce (1st token always has position 1)
            </param>
            <seealso cref="M:Lucene.Net.Analysis.Miscellaneous.LimitTokenPositionFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Int32,System.Boolean)"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.LimitTokenPositionFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Int32,System.Boolean)">
            <summary>
            Build a filter that limits the maximum position of tokens to emit.
            </summary>
            <param name="in"> the stream to wrap </param>
            <param name="maxTokenPosition"> max position of tokens to produce (1st token always has position 1) </param>
            <param name="consumeAllTokens"> whether all tokens from the wrapped input stream must be consumed
                                    even if maxTokenPosition is exceeded. </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.LimitTokenPositionFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Miscellaneous.LimitTokenPositionFilter"/>. 
            <code>
            &lt;fieldType name="text_limit_pos" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.LimitTokenPositionFilterFactory" maxTokenPosition="3" consumeAllTokens="false" /&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            <para>
            The <see cref="F:Lucene.Net.Analysis.Miscellaneous.LimitTokenPositionFilterFactory.consumeAllTokens"/> property is optional and defaults to <c>false</c>.  
            See <see cref="T:Lucene.Net.Analysis.Miscellaneous.LimitTokenPositionFilter"/> for an explanation of its use.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.LimitTokenPositionFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.LimitTokenPositionFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter">
            <summary>
            Old Broken version of <see cref="T:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter"/>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.protWords">
            <summary>
            If not null is the set of tokens to protect from being delimited
            
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Byte[],Lucene.Net.Analysis.Miscellaneous.WordDelimiterFlags,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter"/>
            </summary>
            <param name="in"> <see cref="T:Lucene.Net.Analysis.TokenStream"/> to be filtered </param>
            <param name="charTypeTable"> table containing character types </param>
            <param name="configurationFlags"> Flags configuring the filter </param>
            <param name="protWords"> If not null is the set of tokens to protect from being delimited </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Miscellaneous.WordDelimiterFlags,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter"/> using <see cref="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE"/>
            as its charTypeTable
            </summary>
            <param name="in"> <see cref="T:Lucene.Net.Analysis.TokenStream"/> to be filtered </param>
            <param name="configurationFlags"> Flags configuring the filter </param>
            <param name="protWords"> If not null is the set of tokens to protect from being delimited </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.Reset">
            <summary>
            This method is called by a consumer before it begins consumption using
            <see cref="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.IncrementToken"/>.
            <para/>
            Resets this stream to a clean state. Stateful implementations must implement
            this method so that they can be reused, just as if they had been created fresh.
            <para/>
            If you override this method, always call <c>base.Reset()</c>, otherwise
            some internal state will not be correctly reset (e.g., <see cref="T:Lucene.Net.Analysis.Tokenizer"/> will
            throw <see cref="T:System.InvalidOperationException"/> on further usage).
            </summary>
            <remarks>
            <b>NOTE:</b>
            The default implementation chains the call to the input <see cref="T:Lucene.Net.Analysis.TokenStream"/>, so
            be sure to call <c>base.Reset()</c> when overriding this method.
            </remarks>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.SaveState">
            <summary>
            Saves the existing attribute states
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.FlushConcatenation(Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.WordDelimiterConcatenation)">
            <summary>
            Flushes the given <see cref="T:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.WordDelimiterConcatenation"/> by either writing its concat and then clearing, or just clearing.
            </summary>
            <param name="concatenation"> <see cref="T:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.WordDelimiterConcatenation"/> that will be flushed </param>
            <returns> <c>true</c> if the concatenation was written before it was cleared, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.ShouldConcatenate(System.Int32)">
            <summary>
            Determines whether to concatenate a word or number if the current word is the given type
            </summary>
            <param name="wordType"> Type of the current word used to determine if it should be concatenated </param>
            <returns> <c>true</c> if concatenation should occur, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.ShouldGenerateParts(System.Int32)">
            <summary>
            Determines whether a word/number part should be generated for a word of the given type
            </summary>
            <param name="wordType"> Type of the word used to determine if a word/number part should be generated </param>
            <returns> <c>true</c> if a word/number part should be generated, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.Concatenate(Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.WordDelimiterConcatenation)">
            <summary>
            Concatenates the saved buffer to the given WordDelimiterConcatenation
            </summary>
            <param name="concatenation"> WordDelimiterConcatenation to concatenate the buffer to </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.GeneratePart(System.Boolean)">
            <summary>
            Generates a word/number part, updating the appropriate attributes
            </summary>
            <param name="isSingleWord"> <c>true</c> if the generation is occurring from a single word, <c>false</c> otherwise </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.Position(System.Boolean)">
            <summary>
            Get the position increment gap for a subword or concatenation
            </summary>
            <param name="inject"> true if this token wants to be injected </param>
            <returns> position increment gap </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.IsAlpha(System.Int32)">
            <summary>
            Checks if the given word type includes <see cref="F:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.ALPHA"/>
            </summary>
            <param name="type"> Word type to check </param>
            <returns> <c>true</c> if the type contains <see cref="F:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.ALPHA"/>, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.IsDigit(System.Int32)">
            <summary>
            Checks if the given word type includes <see cref="F:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.DIGIT"/>
            </summary>
            <param name="type"> Word type to check </param>
            <returns> <c>true</c> if the type contains <see cref="F:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.DIGIT"/>, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.IsSubwordDelim(System.Int32)">
            <summary>
            Checks if the given word type includes <see cref="F:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.SUBWORD_DELIM"/>
            </summary>
            <param name="type"> Word type to check </param>
            <returns> <c>true</c> if the type contains <see cref="F:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.SUBWORD_DELIM"/>, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.IsUpper(System.Int32)">
            <summary>
            Checks if the given word type includes <see cref="F:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.UPPER"/>
            </summary>
            <param name="type"> Word type to check </param>
            <returns> <c>true</c> if the type contains <see cref="F:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.UPPER"/>, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.Has(Lucene.Net.Analysis.Miscellaneous.WordDelimiterFlags)">
            <summary>
            Determines whether the given flag is set
            </summary>
            <param name="flag"> Flag to see if set </param>
            <returns> <c>true</c> if flag is set </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.WordDelimiterConcatenation">
            <summary>
            A WDF concatenated 'run'
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.WordDelimiterConcatenation.Append(System.Char[],System.Int32,System.Int32)">
            <summary>
            Appends the given text of the given length, to the concetenation at the given offset
            </summary>
            <param name="text"> Text to append </param>
            <param name="offset"> Offset in the concetenation to add the text </param>
            <param name="length"> Length of the text to append </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.WordDelimiterConcatenation.Write">
            <summary>
            Writes the concatenation to the attributes
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.WordDelimiterConcatenation.IsEmpty">
            <summary>
            Determines if the concatenation is empty
            </summary>
            <returns> <c>true</c> if the concatenation is empty, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.WordDelimiterConcatenation.Clear">
            <summary>
            Clears the concatenation and resets its state
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.Lucene47WordDelimiterFilter.WordDelimiterConcatenation.WriteAndClear">
            <summary>
            Convenience method for the common scenario of having to write the concetenation and then clearing its state
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.PatternAnalyzer">
            <summary>
            Efficient Lucene analyzer/tokenizer that preferably operates on a <see cref="T:System.String"/> rather than a
            <see cref="T:System.IO.TextReader"/>, that can flexibly separate text into terms via a regular expression <see cref="T:System.Text.RegularExpressions.Regex"/>
            (with behaviour similar to <see cref="M:System.Text.RegularExpressions.Regex.Split(System.String)"/>),
            and that combines the functionality of
            <see cref="T:Lucene.Net.Analysis.Core.LetterTokenizer"/>,
            <see cref="T:Lucene.Net.Analysis.Core.LowerCaseTokenizer"/>,
            <see cref="T:Lucene.Net.Analysis.Core.WhitespaceTokenizer"/>,
            <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/> into a single efficient
            multi-purpose class.
            <para>
            If you are unsure how exactly a regular expression should look like, consider 
            prototyping by simply trying various expressions on some test texts via
            <see cref="M:System.Text.RegularExpressions.Regex.Split(System.String)"/>. Once you are satisfied, give that regex to 
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.PatternAnalyzer"/>. Also see <a target="_blank" 
            href="http://www.regular-expressions.info/">Regular Expression Tutorial</a>.
            </para>
            <para>
            This class can be considerably faster than the "normal" Lucene tokenizers. 
            It can also serve as a building block in a compound Lucene
            <see cref="T:Lucene.Net.Analysis.TokenFilter"/> chain. For example as in this 
            stemming example:
            <code>
            PatternAnalyzer pat = ...
            TokenStream tokenStream = new SnowballFilter(
                pat.GetTokenStream("content", "James is running round in the woods"), 
                "English"));
            </code>
            </para>
            </summary>
            @deprecated (4.0) use the pattern-based analysis in the analysis/pattern package instead. 
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.PatternAnalyzer.NON_WORD_PATTERN">
            <summary>
            <c>"\\W+"</c>; Divides text at non-letters (NOT Character.isLetter(c)) </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.PatternAnalyzer.WHITESPACE_PATTERN">
            <summary>
            <c>"\\s+"</c>; Divides text at whitespaces (Character.isWhitespace(c)) </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.PatternAnalyzer.DEFAULT_ANALYZER">
            <summary>
            A lower-casing word analyzer with English stop words (can be shared
            freely across threads without harm); global per class loader.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.PatternAnalyzer.EXTENDED_ANALYZER">
            <summary>
            A lower-casing word analyzer with <b>extended</b> English stop words
            (can be shared freely across threads without harm); global per class
            loader. The stop words are borrowed from
            http://thomas.loc.gov/home/stopwords.html, see
            http://thomas.loc.gov/home/all.about.inquery.html
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.PatternAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,System.Text.RegularExpressions.Regex,System.Boolean,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Constructs a new instance with the given parameters.
            </summary>
            <param name="matchVersion"> currently does nothing </param>
            <param name="pattern">
                       a regular expression delimiting tokens </param>
            <param name="toLowerCase">
                       if <code>true</code> returns tokens after applying
                       String.toLowerCase() </param>
            <param name="stopWords">
                       if non-null, ignores all tokens that are contained in the
                       given stop set (after previously having applied toLowerCase()
                       if applicable). For example, created via
                       <see cref="M:Lucene.Net.Analysis.Core.StopFilter.MakeStopSet(Lucene.Net.Util.LuceneVersion,System.String[])"/>and/or
                       <see cref="T:Lucene.Net.Analysis.Util.WordlistLoader"/>as in
                       <code>WordlistLoader.getWordSet(new File("samples/fulltext/stopwords.txt")</code>
                       or <a href="http://www.unine.ch/info/clef/">other stop words
                       lists </a>. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.PatternAnalyzer.CreateComponents(System.String,System.IO.TextReader,System.String)">
            <summary>
            Creates a token stream that tokenizes the given string into token terms
            (aka words).
            </summary>
            <param name="fieldName">
                       the name of the field to tokenize (currently ignored). </param>
            <param name="reader">
                       reader (e.g. charfilter) of the original text. can be null. </param>
            <param name="text">
                       the string to tokenize </param>
            <returns> a new token stream </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.PatternAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a token stream that tokenizes all the text in the given SetReader;
            This implementation forwards to <see cref="M:Lucene.Net.Analysis.Analyzer.GetTokenStream(System.String,System.IO.TextReader)"/> and is
            less efficient than <see cref="M:Lucene.Net.Analysis.Analyzer.GetTokenStream(System.String,System.IO.TextReader)"/>.
            </summary>
            <param name="fieldName">
                       the name of the field to tokenize (currently ignored). </param>
            <param name="reader">
                       the reader delivering the text </param>
            <returns> a new token stream </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.PatternAnalyzer.Equals(System.Object)">
            <summary>
            Indicates whether some other object is "equal to" this one.
            </summary>
            <param name="other">
                       the reference object with which to compare. </param>
            <returns> true if equal, false otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.PatternAnalyzer.GetHashCode">
            <summary>
            Returns a hash code value for the object.
            </summary>
            <returns> the hash code. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.PatternAnalyzer.Eq(System.Object,System.Object)">
            <summary>
            equality where o1 and/or o2 can be null </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.PatternAnalyzer.EqPattern(System.Text.RegularExpressions.Regex,System.Text.RegularExpressions.Regex)">
            <summary>
            assumes p1 and p2 are not null </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.PatternAnalyzer.ToString(System.IO.TextReader)">
            <summary>
            Reads until end-of-stream and returns all read chars, finally closes the stream.
            </summary>
            <param name="input"> the input stream </param>
            <exception cref="T:System.IO.IOException"> if an I/O error occurs while reading the stream </exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.PatternAnalyzer.PatternTokenizer">
            <summary>
            The work horse; performance isn't fantastic, but it's not nearly as bad
            as one might think - kudos to the Sun regex developers.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.PatternAnalyzer.FastStringTokenizer">
            <summary>
            Special-case class for best performance in common cases; this class is
            otherwise unnecessary.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.PatternAnalyzer.FastStringReader">
            <summary>
            A <see cref="T:System.IO.StringReader"/> that exposes it's contained string for fast direct access.
            Might make sense to generalize this to ICharSequence and make it public?
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.PatternKeywordMarkerFilter">
            <summary>
            Marks terms as keywords via the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/>. Each token
            that matches the provided pattern is marked as a keyword by setting
            <see cref="P:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute.IsKeyword"/> to <c>true</c>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.PatternKeywordMarkerFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Text.RegularExpressions.Regex)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.PatternKeywordMarkerFilter"/>, that marks the current
            token as a keyword if the tokens term buffer matches the provided
            <see cref="T:System.Text.RegularExpressions.Regex"/> via the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/>.
            </summary>
            <param name="in">
                     <see cref="T:Lucene.Net.Analysis.TokenStream"/> to filter </param>
            <param name="pattern">
                     the pattern to apply to the incoming term buffer
             </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.PerFieldAnalyzerWrapper">
            <summary>
            This analyzer is used to facilitate scenarios where different
            fields Require different analysis techniques.  Use the Map
            argument in <see cref="M:Lucene.Net.Analysis.Miscellaneous.PerFieldAnalyzerWrapper.#ctor(Lucene.Net.Analysis.Analyzer,System.Collections.Generic.IDictionary{System.String,Lucene.Net.Analysis.Analyzer})"/>
            to add non-default analyzers for fields.
            
            <para>Example usage:
            
            <code>
            IDictionary&lt;string, Analyzer&gt; analyzerPerField = new Dictionary&lt;string, Analyzer&gt;();
            analyzerPerField["firstname"] = new KeywordAnalyzer();
            analyzerPerField["lastname"] = new KeywordAnalyzer();
            
            PerFieldAnalyzerWrapper aWrapper =
              new PerFieldAnalyzerWrapper(new StandardAnalyzer(version), analyzerPerField);
            </code>
            </para>
            <para>
            In this example, <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/> will be used for all fields except "firstname"
            and "lastname", for which <see cref="T:Lucene.Net.Analysis.Core.KeywordAnalyzer"/> will be used.
            </para>
            <para>A <see cref="T:Lucene.Net.Analysis.Miscellaneous.PerFieldAnalyzerWrapper"/> can be used like any other analyzer, for both indexing
            and query parsing.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.PerFieldAnalyzerWrapper.#ctor(Lucene.Net.Analysis.Analyzer)">
            <summary>
            Constructs with default analyzer.
            </summary>
            <param name="defaultAnalyzer"> Any fields not specifically
            defined to use a different analyzer will use the one provided here. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.PerFieldAnalyzerWrapper.#ctor(Lucene.Net.Analysis.Analyzer,System.Collections.Generic.IDictionary{System.String,Lucene.Net.Analysis.Analyzer})">
            <summary>
            Constructs with default analyzer and a map of analyzers to use for 
            specific fields.
            <para/>
            The type of <see cref="T:System.Collections.Generic.IDictionary`2"/> supplied will determine the type of behavior.
            <list type="table">
                <item>
                    <term><see cref="T:System.Collections.Generic.Dictionary`2"/></term>
                    <description>General use. <c>null</c> keys are not supported.</description>
                </item>
                <item>
                    <term><see cref="T:System.Collections.Generic.SortedDictionary`2"/></term>
                    <description>Use when sorted keys are required. <c>null</c> keys are not supported.</description>
                </item>
                <item>
                    <term><see cref="T:J2N.Collections.Generic.Dictionary`2"/></term>
                    <description>Similar behavior as <see cref="T:System.Collections.Generic.Dictionary`2"/>. <c>null</c> keys are supported.</description>
                </item>
                <item>
                    <term><see cref="T:J2N.Collections.Generic.SortedDictionary`2"/></term>
                    <description>Use when sorted keys are required. <c>null</c> keys are supported.</description>
                </item>
                <item>
                    <term><see cref="T:J2N.Collections.Generic.LinkedDictionary`2"/></term>
                    <description>Use when insertion order must be preserved (<see cref="T:System.Collections.Generic.Dictionary`2"/> preserves insertion
                        order only until items are removed). <c>null</c> keys are supported.</description>
                </item>
            </list>
            Or, use a 3rd party or custom <see cref="T:System.Collections.Generic.IDictionary`2"/> if other behavior is desired.
            </summary>
            <param name="defaultAnalyzer"> Any fields not specifically
            defined to use a different analyzer will use the one provided here. </param>
            <param name="fieldAnalyzers"> A <see cref="T:System.Collections.Generic.IDictionary`2"/> (String field name to the Analyzer) to be 
            used for those fields. </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.PrefixAndSuffixAwareTokenFilter">
            <summary>
            Links two <see cref="T:Lucene.Net.Analysis.Miscellaneous.PrefixAwareTokenFilter"/>.
            <para/>
            <b>NOTE:</b> This filter might not behave correctly if used with custom 
            <see cref="T:Lucene.Net.Util.IAttribute"/>s, i.e. <see cref="T:Lucene.Net.Util.IAttribute"/>s other than
            the ones located in Lucene.Net.Analysis.TokenAttributes. 
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.PrefixAwareTokenFilter">
            <summary>
            Joins two token streams and leaves the last token of the first stream available
            to be used when updating the token values in the second stream based on that token.
            
            The default implementation adds last prefix token end offset to the suffix token start and end offsets.
            <para/>
            <b>NOTE:</b> This filter might not behave correctly if used with custom 
            <see cref="T:Lucene.Net.Util.IAttribute"/>s, i.e. <see cref="T:Lucene.Net.Util.IAttribute"/>s other than
            the ones located in Lucene.Net.Analysis.TokenAttributes.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.PrefixAwareTokenFilter.UpdateSuffixToken(Lucene.Net.Analysis.Token,Lucene.Net.Analysis.Token)">
            <summary>
            The default implementation adds last prefix token end offset to the suffix token start and end offsets.
            </summary>
            <param name="suffixToken"> a token from the suffix stream </param>
            <param name="lastPrefixToken"> the last token from the prefix stream </param>
            <returns> consumer token </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.RemoveDuplicatesTokenFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> which filters out <see cref="T:Lucene.Net.Analysis.Token"/>s at the same position and Term text as the previous token in the stream.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.RemoveDuplicatesTokenFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Creates a new RemoveDuplicatesTokenFilter
            </summary>
            <param name="in"> TokenStream that will be filtered </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.RemoveDuplicatesTokenFilter.IncrementToken">
            <summary>
            Consumers (i.e., <see cref="T:Lucene.Net.Index.IndexWriter"/>) use this method to advance the stream to
            the next token. Implementing classes must implement this method and update
            the appropriate <see cref="T:Lucene.Net.Util.IAttribute"/>s with the attributes of the next
            token.
            <para/>
            The producer must make no assumptions about the attributes after the method
            has been returned: the caller may arbitrarily change it. If the producer
            needs to preserve the state for subsequent calls, it can use
            <see cref="M:Lucene.Net.Util.AttributeSource.CaptureState"/> to create a copy of the current attribute state.
            <para/>
            this method is called for every token of a document, so an efficient
            implementation is crucial for good performance. To avoid calls to
            <see cref="M:Lucene.Net.Util.AttributeSource.AddAttribute``1"/> and <see cref="M:Lucene.Net.Util.AttributeSource.GetAttribute``1"/>,
            references to all <see cref="T:Lucene.Net.Util.IAttribute"/>s that this stream uses should be
            retrieved during instantiation.
            <para/>
            To ensure that filters and consumers know which attributes are available,
            the attributes must be added during instantiation. Filters and consumers
            are not required to check for availability of attributes in
            <see cref="M:Lucene.Net.Analysis.Miscellaneous.RemoveDuplicatesTokenFilter.IncrementToken"/>.
            </summary>
            <returns> false for end of stream; true otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.RemoveDuplicatesTokenFilter.Reset">
            <summary>
            This method is called by a consumer before it begins consumption using
            <see cref="M:Lucene.Net.Analysis.Miscellaneous.RemoveDuplicatesTokenFilter.IncrementToken"/>.
            <para/>
            Resets this stream to a clean state. Stateful implementations must implement
            this method so that they can be reused, just as if they had been created fresh.
            <para/>
            If you override this method, always call <c>base.Reset()</c>, otherwise
            some internal state will not be correctly reset (e.g., <see cref="T:Lucene.Net.Analysis.Tokenizer"/> will
            throw <see cref="T:System.InvalidOperationException"/> on further usage).
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.RemoveDuplicatesTokenFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Miscellaneous.RemoveDuplicatesTokenFilter"/>.
            <code>
            &lt;fieldType name="text_rmdup" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.RemoveDuplicatesTokenFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.RemoveDuplicatesTokenFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.RemoveDuplicatesTokenFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.ScandinavianFoldingFilter">
            <summary>
            This filter folds Scandinavian characters åÅäæÄÆ->a and öÖøØ->o.
            It also discriminate against use of double vowels aa, ae, ao, oe and oo, leaving just the first one.
            <para/>
            It's is a semantically more destructive solution than <see cref="T:Lucene.Net.Analysis.Miscellaneous.ScandinavianNormalizationFilter"/> but
            can in addition help with matching raksmorgas as räksmörgås.
            <para/>
            blåbærsyltetøj == blåbärsyltetöj == blaabaarsyltetoej == blabarsyltetoj
            räksmörgås == ræksmørgås == ræksmörgaos == raeksmoergaas == raksmorgas
            <para/>
            Background:
            Swedish åäö are in fact the same letters as Norwegian and Danish åæø and thus interchangeable
            when used between these languages. They are however folded differently when people type
            them on a keyboard lacking these characters.
            <para/>
            In that situation almost all Swedish people use a, a, o instead of å, ä, ö.
            <para/>
            Norwegians and Danes on the other hand usually type aa, ae and oe instead of å, æ and ø.
            Some do however use a, a, o, oo, ao and sometimes permutations of everything above.
            <para/>
            This filter solves that mismatch problem, but might also cause new.
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Miscellaneous.ScandinavianNormalizationFilter"/>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.ScandinavianFoldingFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Miscellaneous.ScandinavianFoldingFilter"/>.
            <code>
            &lt;fieldType name="text_scandfold" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.ScandinavianFoldingFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.ScandinavianFoldingFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.ScandinavianFoldingFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.ScandinavianNormalizationFilter">
            <summary>
            This filter normalize use of the interchangeable Scandinavian characters æÆäÄöÖøØ
            and folded variants (aa, ao, ae, oe and oo) by transforming them to åÅæÆøØ.
            <para/>
            It's a semantically less destructive solution than <see cref="T:Lucene.Net.Analysis.Miscellaneous.ScandinavianFoldingFilter"/>,
            most useful when a person with a Norwegian or Danish keyboard queries a Swedish index
            and vice versa. This filter does <b>not</b>  the common Swedish folds of å and ä to a nor ö to o.
            <para/>
            blåbærsyltetøj == blåbärsyltetöj == blaabaarsyltetoej but not blabarsyltetoj
            räksmörgås == ræksmørgås == ræksmörgaos == raeksmoergaas but not raksmorgas
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Miscellaneous.ScandinavianFoldingFilter"/>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.ScandinavianNormalizationFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Miscellaneous.ScandinavianNormalizationFilter"/>.
            <code>
            &lt;fieldType name="text_scandnorm" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.ScandinavianNormalizationFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.ScandinavianNormalizationFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.ScandinavianNormalizationFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter">
            <summary>
            Marks terms as keywords via the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/>. Each token
            contained in the provided set is marked as a keyword by setting
            <see cref="P:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute.IsKeyword"/> to <c>true</c>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/>, that marks the current token as a
            keyword if the tokens term buffer is contained in the given set via the
            <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/>.
            </summary>
            <param name="in">
                     <see cref="T:Lucene.Net.Analysis.TokenStream"/> to filter </param>
            <param name="keywordSet">
                     the keywords set to lookup the current termbuffer </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.SingleTokenTokenStream">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenStream"/> containing a single token.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter">
            <summary>
            Provides the ability to override any <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> aware stemmer
            with custom dictionary-based stemming.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.StemmerOverrideMap)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter"/>, performing dictionary-based stemming
            with the provided dictionary (<paramref name="stemmerOverrideMap"/>).
            <para>
            Any dictionary-stemmed terms will be marked with <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/>
            so that they will not be stemmed with stemmers down the chain.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.StemmerOverrideMap">
            <summary>
            A read-only 4-byte FST backed map that allows fast case-insensitive key
            value lookups for <see cref="T:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.StemmerOverrideMap.#ctor(Lucene.Net.Util.Fst.FST{Lucene.Net.Util.BytesRef},System.Boolean)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.StemmerOverrideMap"/> </summary>
            <param name="fst"> the fst to lookup the overrides </param>
            <param name="ignoreCase"> if the keys case should be ingored </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.StemmerOverrideMap.GetBytesReader">
            <summary>
            Returns a <see cref="T:Lucene.Net.Util.Fst.FST.BytesReader"/> to pass to the <see cref="M:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.StemmerOverrideMap.Get(System.Char[],System.Int32,Lucene.Net.Util.Fst.FST.Arc{Lucene.Net.Util.BytesRef},Lucene.Net.Util.Fst.FST.BytesReader)"/> method.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.StemmerOverrideMap.Get(System.Char[],System.Int32,Lucene.Net.Util.Fst.FST.Arc{Lucene.Net.Util.BytesRef},Lucene.Net.Util.Fst.FST.BytesReader)">
            <summary>
            Returns the value mapped to the given key or <code>null</code> if the key is not in the FST dictionary.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.Builder">
            <summary>
            This builder builds an <see cref="T:Lucene.Net.Util.Fst.FST"/> for the <see cref="T:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.Builder.#ctor">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.Builder"/> with <see cref="F:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.Builder.ignoreCase"/> set to <c>false</c> 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.Builder.#ctor(System.Boolean)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.Builder"/> </summary>
            <param name="ignoreCase"> if the input case should be ignored. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.Builder.Add(System.String,System.String)">
            <summary>
            Adds an input string and it's stemmer override output to this builder.
            </summary>
            <param name="input"> the input char sequence </param>
            <param name="output"> the stemmer override output char sequence </param>
            <returns> <c>false</c> if the input has already been added to this builder otherwise <c>true</c>. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.Builder.Build">
            <summary>
            Returns a <see cref="T:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.StemmerOverrideMap"/> to be used with the <see cref="T:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter"/> </summary>
            <returns> a <see cref="T:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter.StemmerOverrideMap"/> to be used with the <see cref="T:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter"/> </returns>
            <exception cref="T:System.IO.IOException"> if an <see cref="T:System.IO.IOException"/> occurs; </exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter"/>.
            <code>
            &lt;fieldType name="text_dicstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.StemmerOverrideFilterFactory" dictionary="dictionary.txt" ignoreCase="false"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.TrimFilter">
            <summary>
            Trims leading and trailing whitespace from Tokens in the stream.
            <para>As of Lucene 4.4, this filter does not support updateOffsets=true anymore
            as it can lead to broken token streams.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.TrimFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,System.Boolean)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.TrimFilter"/>. </summary>
            <param name="version">       the Lucene match version </param>
            <param name="in">            the stream to consume </param>
            <param name="updateOffsets"> whether to update offsets </param>
            @deprecated Offset updates are not supported anymore as of Lucene 4.4. 
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.TrimFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.TrimFilter"/> on top of <paramref name="in"/>. </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.TrimFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Miscellaneous.TrimFilter"/>.
            <code>
            &lt;fieldType name="text_trm" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.NGramTokenizerFactory"/&gt;
                &lt;filter class="solr.TrimFilterFactory" /&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Miscellaneous.TrimFilter"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.TrimFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.TrimFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.TruncateTokenFilter">
            <summary>
            A token filter for truncating the terms into a specific length.
            Fixed prefix truncation, as a stemming method, produces good results on Turkish language.
            It is reported that F5, using first 5 characters, produced best results in
            <a href="http://www.users.muohio.edu/canf/papers/JASIST2008offPrint.pdf">
            Information Retrieval on Turkish Texts</a>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.TruncateTokenFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Miscellaneous.TruncateTokenFilter"/>. The following type is recommended for "<i>diacritics-insensitive search</i>" for Turkish.
            <code>
            &lt;fieldType name="text_tr_ascii_f5" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.ApostropheFilterFactory"/&gt;
                &lt;filter class="solr.TurkishLowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.ASCIIFoldingFilterFactory" preserveOriginal="true"/&gt;
                &lt;filter class="solr.KeywordRepeatFilterFactory"/&gt;
                &lt;filter class="solr.TruncateTokenFilterFactory" prefixLength="5"/&gt;
                &lt;filter class="solr.RemoveDuplicatesTokenFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.TruncateTokenFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.TruncateTokenFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFlags">
            <summary>
            Configuration options for the <see cref="T:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter"/>.
            <para/>
            LUCENENET specific - these options were passed as int constant flags in Lucene.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFlags.GENERATE_WORD_PARTS">
            <summary>
            Causes parts of words to be generated:
            <para/>
            "PowerShot" => "Power" "Shot"
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFlags.GENERATE_NUMBER_PARTS">
            <summary>
            Causes number subwords to be generated:
            <para/>
            "500-42" => "500" "42"
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFlags.CATENATE_WORDS">
            <summary>
            Causes maximum runs of word parts to be catenated:
            <para/>
            "wi-fi" => "wifi"
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFlags.CATENATE_NUMBERS">
            <summary>
            Causes maximum runs of word parts to be catenated:
            <para/>
            "wi-fi" => "wifi"
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFlags.CATENATE_ALL">
            <summary>
            Causes all subword parts to be catenated:
            <para/>
            "wi-fi-4000" => "wifi4000"
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFlags.PRESERVE_ORIGINAL">
            <summary>
            Causes original words are preserved and added to the subword list (Defaults to false)
            <para/>
            "500-42" => "500" "42" "500-42"
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFlags.SPLIT_ON_CASE_CHANGE">
            <summary>
            If not set, causes case changes to be ignored (subwords will only be generated
            given SUBWORD_DELIM tokens)
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFlags.SPLIT_ON_NUMERICS">
            <summary>
            If not set, causes numeric changes to be ignored (subwords will only be generated
            given SUBWORD_DELIM tokens).
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFlags.STEM_ENGLISH_POSSESSIVE">
            <summary>
            Causes trailing "'s" to be removed for each subword
            <para/>
            "O'Neil's" => "O", "Neil"
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter">
            <summary>
            Splits words into subwords and performs optional transformations on subword
            groups. Words are split into subwords with the following rules:
            <list type="bullet">
                <item><description>split on intra-word delimiters (by default, all non alpha-numeric
                    characters): <c>"Wi-Fi"</c> &#8594; <c>"Wi", "Fi"</c></description></item>
                <item><description>split on case transitions: <c>"PowerShot"</c> &#8594;
                    <c>"Power", "Shot"</c></description></item>
                <item><description>split on letter-number transitions: <c>"SD500"</c> &#8594;
                    <c>"SD", "500"</c></description></item>
                <item><description>leading and trailing intra-word delimiters on each subword are ignored:
                    <c>"//hello---there, 'dude'"</c> &#8594;
                    <c>"hello", "there", "dude"</c></description></item>
                <item><description>trailing "'s" are removed for each subword: <c>"O'Neil's"</c>
                    &#8594; <c>"O", "Neil"</c>
                    <ul>
                        <item><description>Note: this step isn't performed in a separate filter because of possible
                            subword combinations.</description></item>
                    </ul>
                </description></item>
            </list>
            <para/>
            The <b>combinations</b> parameter affects how subwords are combined:
            <list type="bullet">
                <item><description>combinations="0" causes no subword combinations: <code>"PowerShot"</code>
                    &#8594; <c>0:"Power", 1:"Shot"</c> (0 and 1 are the token positions)</description></item>
                <item><description>combinations="1" means that in addition to the subwords, maximum runs of
                    non-numeric subwords are catenated and produced at the same position of the
                    last subword in the run:
                    <ul>
                        <item><description><c>"PowerShot"</c> &#8594;
                            <c>0:"Power", 1:"Shot" 1:"PowerShot"</c></description></item>
                        <item><description><c>"A's+B's&amp;C's"</c> -gt; <c>0:"A", 1:"B", 2:"C", 2:"ABC"</c>
                        </description></item>
                        <item><description><c>"Super-Duper-XL500-42-AutoCoder!"</c> &#8594;
                            <c>0:"Super", 1:"Duper", 2:"XL", 2:"SuperDuperXL", 3:"500" 4:"42", 5:"Auto", 6:"Coder", 6:"AutoCoder"</c>
                        </description></item>
                    </ul>
                </description></item>
            </list>
            <para/>
            One use for <see cref="T:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter"/> is to help match words with different
            subword delimiters. For example, if the source text contained "wi-fi" one may
            want "wifi" "WiFi" "wi-fi" "wi+fi" queries to all match. One way of doing so
            is to specify combinations="1" in the analyzer used for indexing, and
            combinations="0" (the default) in the analyzer used for querying. Given that
            the current <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> immediately removes many intra-word
            delimiters, it is recommended that this filter be used after a tokenizer that
            does not do this (such as <see cref="T:Lucene.Net.Analysis.Core.WhitespaceTokenizer"/>).
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.protWords">
            <summary>
            If not null is the set of tokens to protect from being delimited
            
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,System.Byte[],Lucene.Net.Analysis.Miscellaneous.WordDelimiterFlags,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Creates a new WordDelimiterFilter
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="in"> TokenStream to be filtered </param>
            <param name="charTypeTable"> table containing character types </param>
            <param name="configurationFlags"> Flags configuring the filter </param>
            <param name="protWords"> If not null is the set of tokens to protect from being delimited </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Miscellaneous.WordDelimiterFlags,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Creates a new WordDelimiterFilter using <see cref="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE"/>
            as its charTypeTable
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="in"> <see cref="T:Lucene.Net.Analysis.TokenStream"/> to be filtered </param>
            <param name="configurationFlags"> Flags configuring the filter </param>
            <param name="protWords"> If not null is the set of tokens to protect from being delimited </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.SaveState">
            <summary>
            Saves the existing attribute states
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.FlushConcatenation(Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.WordDelimiterConcatenation)">
            <summary>
            Flushes the given <see cref="T:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.WordDelimiterConcatenation"/> by either writing its concat and then clearing, or just clearing.
            </summary>
            <param name="concatenation"> <see cref="T:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.WordDelimiterConcatenation"/> that will be flushed </param>
            <returns> <c>true</c> if the concatenation was written before it was cleared, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.ShouldConcatenate(System.Int32)">
            <summary>
            Determines whether to concatenate a word or number if the current word is the given type
            </summary>
            <param name="wordType"> Type of the current word used to determine if it should be concatenated </param>
            <returns> <c>true</c> if concatenation should occur, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.ShouldGenerateParts(System.Int32)">
            <summary>
            Determines whether a word/number part should be generated for a word of the given type
            </summary>
            <param name="wordType"> Type of the word used to determine if a word/number part should be generated </param>
            <returns> <c>true</c> if a word/number part should be generated, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.Concatenate(Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.WordDelimiterConcatenation)">
            <summary>
            Concatenates the saved buffer to the given <see cref="T:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.WordDelimiterConcatenation"/>
            </summary>
            <param name="concatenation"> <see cref="T:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.WordDelimiterConcatenation"/> to concatenate the buffer to </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.GeneratePart(System.Boolean)">
            <summary>
            Generates a word/number part, updating the appropriate attributes
            </summary>
            <param name="isSingleWord"> <c>true</c> if the generation is occurring from a single word, <c>false</c> otherwise </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.Position(System.Boolean)">
            <summary>
            Get the position increment gap for a subword or concatenation
            </summary>
            <param name="inject"> true if this token wants to be injected </param>
            <returns> position increment gap </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.IsAlpha(System.Int32)">
            <summary>
            Checks if the given word type includes <see cref="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.ALPHA"/>
            </summary>
            <param name="type"> Word type to check </param>
            <returns> <c>true</c> if the type contains <see cref="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.ALPHA"/>, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.IsDigit(System.Int32)">
            <summary>
            Checks if the given word type includes <see cref="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.DIGIT"/>
            </summary>
            <param name="type"> Word type to check </param>
            <returns> <c>true</c> if the type contains <see cref="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.DIGIT"/>, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.IsSubwordDelim(System.Int32)">
            <summary>
            Checks if the given word type includes <see cref="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.SUBWORD_DELIM"/>
            </summary>
            <param name="type"> Word type to check </param>
            <returns> <c>true</c> if the type contains <see cref="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.SUBWORD_DELIM"/>, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.IsUpper(System.Int32)">
            <summary>
            Checks if the given word type includes <see cref="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.UPPER"/>
            </summary>
            <param name="type"> Word type to check </param>
            <returns> <c>true</c> if the type contains <see cref="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.UPPER"/>, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.Has(Lucene.Net.Analysis.Miscellaneous.WordDelimiterFlags)">
            <summary>
            Determines whether the given flag is set
            </summary>
            <param name="flag"> Flag to see if set </param>
            <returns> <c>true</c> if flag is set </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.WordDelimiterConcatenation">
            <summary>
            A WDF concatenated 'run'
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.WordDelimiterConcatenation.Append(System.Char[],System.Int32,System.Int32)">
            <summary>
            Appends the given text of the given length, to the concetenation at the given offset
            </summary>
            <param name="text"> Text to append </param>
            <param name="offset"> Offset in the concetenation to add the text </param>
            <param name="length"> Length of the text to append </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.WordDelimiterConcatenation.Write">
            <summary>
            Writes the concatenation to the attributes
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.WordDelimiterConcatenation.IsEmpty">
            <summary>
            Determines if the concatenation is empty
            </summary>
            <returns> <c>true</c> if the concatenation is empty, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.WordDelimiterConcatenation.Clear">
            <summary>
            Clears the concatenation and resets its state
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter.WordDelimiterConcatenation.WriteAndClear">
            <summary>
            Convenience method for the common scenario of having to write the concetenation and then clearing its state
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter"/>.
            <code>
            &lt;fieldType name="text_wd" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.WordDelimiterFilterFactory" protected="protectedword.txt"
                        preserveOriginal="0" splitOnNumerics="1" splitOnCaseChange="1"
                        catenateWords="0" catenateNumbers="0" catenateAll="0"
                        generateWordParts="1" generateNumberParts="1" stemEnglishPossessive="1"
                        types="wdfftypes.txt" /&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator">
            <summary>
            A BreakIterator-like API for iterating over subwords in text, according to <see cref="T:Lucene.Net.Analysis.Miscellaneous.WordDelimiterFilter"/> rules.
            @lucene.internal
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.DONE">
            <summary>
            Indicates the end of iteration </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.startBounds">
            <summary>
            start position of text, excluding leading delimiters </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.endBounds">
            <summary>
            end position of text, excluding trailing delimiters </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.current">
            <summary>
            Beginning of subword </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.end">
            <summary>
            End of subword </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.hasFinalPossessive">
            <summary>does this string end with a possessive such as 's</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.splitOnCaseChange">
            <summary>
            If false, causes case changes to be ignored (subwords will only be generated
            given SUBWORD_DELIM tokens). (Defaults to true)
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.splitOnNumerics">
            <summary>
            If false, causes numeric changes to be ignored (subwords will only be generated
            given SUBWORD_DELIM tokens). (Defaults to true)
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.stemEnglishPossessive">
            <summary>
            If true, causes trailing "'s" to be removed for each subword. (Defaults to true)
            <p/>
            "O'Neil's" => "O", "Neil"
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.skipPossessive">
            <summary>
            if true, need to skip over a possessive found in the last call to next() </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.#ctor(System.Byte[],System.Boolean,System.Boolean,System.Boolean)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator"/> operating with the supplied rules.
            </summary>
            <param name="charTypeTable"> table containing character types </param>
            <param name="splitOnCaseChange"> if true, causes "PowerShot" to be two tokens; ("Power-Shot" remains two parts regards) </param>
            <param name="splitOnNumerics"> if true, causes "j2se" to be three tokens; "j" "2" "se" </param>
            <param name="stemEnglishPossessive"> if true, causes trailing "'s" to be removed for each subword: "O'Neil's" => "O", "Neil" </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.Next">
            <summary>
            Advance to the next subword in the string.
            </summary>
            <returns> index of the next subword, or <see cref="F:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.DONE"/> if all subwords have been returned </returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.Type">
            <summary>
            Return the type of the current subword.
            This currently uses the type of the first character in the subword.
            </summary>
            <returns> type of the current word </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.SetText(System.Char[],System.Int32)">
            <summary>
            Reset the text to a new value, and reset all state
            </summary>
            <param name="text"> New text </param>
            <param name="length"> length of the text </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.IsBreak(System.Int32,System.Int32)">
            <summary>
            Determines whether the transition from lastType to type indicates a break
            </summary>
            <param name="lastType"> Last subword type </param>
            <param name="type"> Current subword type </param>
            <returns> <c>true</c> if the transition indicates a break, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.IsSingleWord">
            <summary>
            Determines if the current word contains only one subword.  Note, it could be potentially surrounded by delimiters
            </summary>
            <returns> <c>true</c> if the current word contains only one subword, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.SetBounds">
            <summary>
            Set the internal word bounds (remove leading and trailing delimiters). Note, if a possessive is found, don't remove
            it yet, simply note it.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.EndsWithPossessive(System.Int32)">
            <summary>
            Determines if the text at the given position indicates an English possessive which should be removed
            </summary>
            <param name="pos"> Position in the text to check if it indicates an English possessive </param>
            <returns> <c>true</c> if the text at the position indicates an English posessive, <c>false</c> otherwise </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.CharType(System.Int32)">
            <summary>
            Determines the type of the given character
            </summary>
            <param name="ch"> Character whose type is to be determined </param>
            <returns> Type of the character </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Miscellaneous.WordDelimiterIterator.GetType(System.Int32)">
            <summary>
            Computes the type of the given character
            </summary>
            <param name="ch"> Character whose type is to be determined </param>
            <returns> Type of the character </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.NGram.EdgeNGramFilterFactory">
            <summary>
            Creates new instances of <see cref="T:Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter"/>.
            <code>
            &lt;fieldType name="text_edgngrm" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.EdgeNGramFilterFactory" minGramSize="1" maxGramSize="1"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.EdgeNGramFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.NGram.EdgeNGramFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter">
            <summary>
            Tokenizes the given token into n-grams of given size(s).
            <para>
            This <see cref="T:Lucene.Net.Analysis.TokenFilter"/> create n-grams from the beginning edge or ending edge of a input token.
            </para>
            <para>As of Lucene 4.4, this filter does not support
            <see cref="F:Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter.Side.BACK"/> (you can use <see cref="T:Lucene.Net.Analysis.Reverse.ReverseStringFilter"/> up-front and
            afterward to get the same behavior), handles supplementary characters
            correctly and does not update offsets anymore.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter.Side">
            <summary>
            Specifies which side of the input the n-gram should be generated from </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter.Side.FRONT">
            <summary>
            Get the n-gram from the front of the input </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter.Side.BACK">
            <summary>
            Get the n-gram from the end of the input </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter.GetSide(System.String)">
            <summary>
            Get the appropriate <see cref="T:Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter.Side"/> from a string
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter.Side,System.Int32,System.Int32)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter"/> that can generate n-grams in the sizes of the given range
            </summary>
            <param name="version"> the Lucene match version - See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="input"> <see cref="T:Lucene.Net.Analysis.TokenStream"/> holding the input to be tokenized </param>
            <param name="side"> the <see cref="T:Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter.Side"/> from which to chop off an n-gram </param>
            <param name="minGram"> the smallest n-gram to generate </param>
            <param name="maxGram"> the largest n-gram to generate </param>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,System.String,System.Int32,System.Int32)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter"/> that can generate n-grams in the sizes of the given range
            </summary>
            <param name="version"> the Lucene match version - See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="input"> <see cref="T:Lucene.Net.Analysis.TokenStream"/> holding the input to be tokenized </param>
            <param name="sideLabel"> the name of the <see cref="T:Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter.Side"/> from which to chop off an n-gram </param>
            <param name="minGram"> the smallest n-gram to generate </param>
            <param name="maxGram"> the largest n-gram to generate </param>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,System.Int32,System.Int32)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter"/> that can generate n-grams in the sizes of the given range
            </summary>
            <param name="version"> the Lucene match version - See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="input"> <see cref="T:Lucene.Net.Analysis.TokenStream"/> holding the input to be tokenized </param>
            <param name="minGram"> the smallest n-gram to generate </param>
            <param name="maxGram"> the largest n-gram to generate </param>
        </member>
        <member name="T:Lucene.Net.Analysis.NGram.EdgeNGramTokenizer">
            <summary>
            Tokenizes the input from an edge into n-grams of given size(s).
            <para>
            This <see cref="T:Lucene.Net.Analysis.Tokenizer"/> create n-grams from the beginning edge or ending edge of a input token.
            </para>
            <para>As of Lucene 4.4, this tokenizer
            <list type="bullet">
                <item><description>can handle <code>maxGram</code> larger than 1024 chars, but beware that this will result in increased memory usage</description></item>
                <item><description>doesn't trim the input,</description></item>
                <item><description>sets position increments equal to 1 instead of 1 for the first token and 0 for all other ones</description></item>
                <item><description>doesn't support backward n-grams anymore.</description></item>
                <item><description>supports <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.IsTokenChar(System.Int32)"/> pre-tokenization,</description></item>
                <item><description>correctly handles supplementary characters.</description></item>
            </list>
            </para>
            <para>Although <b style="color:red">highly</b> discouraged, it is still possible
            to use the old behavior through <see cref="T:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer"/>.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.EdgeNGramTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader,System.Int32,System.Int32)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.EdgeNGramTokenizer"/> that can generate n-grams in the sizes of the given range
            </summary>
            <param name="version"> the Lucene match version - See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="input"> <see cref="T:System.IO.TextReader"/> holding the input to be tokenized </param>
            <param name="minGram"> the smallest n-gram to generate </param>
            <param name="maxGram"> the largest n-gram to generate </param>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.EdgeNGramTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader,System.Int32,System.Int32)">
            <summary>
            Creates EdgeNGramTokenizer that can generate n-grams in the sizes of the given range
            </summary>
            <param name="version"> the Lucene match version - See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="factory"> <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/> to use </param>
            <param name="input"> <see cref="T:System.IO.TextReader"/> holding the input to be tokenized </param>
            <param name="minGram"> the smallest n-gram to generate </param>
            <param name="maxGram"> the largest n-gram to generate </param>
        </member>
        <member name="T:Lucene.Net.Analysis.NGram.EdgeNGramTokenizerFactory">
            <summary>
            Creates new instances of <see cref="T:Lucene.Net.Analysis.NGram.EdgeNGramTokenizer"/>.
            <code>
            &lt;fieldType name="text_edgngrm" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.EdgeNGramTokenizerFactory" minGramSize="1" maxGramSize="1"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.EdgeNGramTokenizerFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.NGram.EdgeNGramTokenizerFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer">
            <summary>
            Old version of <see cref="T:Lucene.Net.Analysis.NGram.EdgeNGramTokenizer"/> which doesn't handle correctly
            supplementary characters.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer.Side">
            <summary>
            Specifies which side of the input the n-gram should be generated from </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer.Side.FRONT">
            <summary>
            Get the n-gram from the front of the input </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer.Side.BACK">
            <summary>
            Get the n-gram from the end of the input </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader,Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer.Side,System.Int32,System.Int32)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer"/> that can generate n-grams in the sizes of the given range
            </summary>
            <param name="version"> the Lucene match version - See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="input"> <see cref="T:System.IO.TextReader"/> holding the input to be tokenized </param>
            <param name="side"> the <see cref="T:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer.Side"/> from which to chop off an n-gram </param>
            <param name="minGram"> the smallest n-gram to generate </param>
            <param name="maxGram"> the largest n-gram to generate </param>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader,Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer.Side,System.Int32,System.Int32)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer"/> that can generate n-grams in the sizes of the given range
            </summary>
            <param name="version"> the Lucene match version - See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="factory"> <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/> to use </param>
            <param name="input"> <see cref="T:System.IO.TextReader"/> holding the input to be tokenized </param>
            <param name="side"> the <see cref="T:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer.Side"/> from which to chop off an n-gram </param>
            <param name="minGram"> the smallest n-gram to generate </param>
            <param name="maxGram"> the largest n-gram to generate </param>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader,System.String,System.Int32,System.Int32)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer"/> that can generate n-grams in the sizes of the given range
            </summary>
            <param name="version"> the Lucene match version - See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="input"> <see cref="T:System.IO.TextReader"/> holding the input to be tokenized </param>
            <param name="sideLabel"> the name of the <see cref="T:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer.Side"/> from which to chop off an n-gram </param>
            <param name="minGram"> the smallest n-gram to generate </param>
            <param name="maxGram"> the largest n-gram to generate </param>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader,System.String,System.Int32,System.Int32)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer"/> that can generate n-grams in the sizes of the given range
            </summary>
            <param name="version"> the Lucene match version - See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="factory"> <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/> to use </param>
            <param name="input"> <see cref="T:System.IO.TextReader"/> holding the input to be tokenized </param>
            <param name="sideLabel"> the name of the <see cref="T:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer.Side"/> from which to chop off an n-gram </param>
            <param name="minGram"> the smallest n-gram to generate </param>
            <param name="maxGram"> the largest n-gram to generate </param>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader,System.Int32,System.Int32)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer"/> that can generate n-grams in the sizes of the given range
            </summary>
            <param name="version"> the Lucene match version - See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="input"> <see cref="T:System.IO.TextReader"/> holding the input to be tokenized </param>
            <param name="minGram"> the smallest n-gram to generate </param>
            <param name="maxGram"> the largest n-gram to generate </param>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader,System.Int32,System.Int32)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer"/> that can generate n-grams in the sizes of the given range
            </summary>
            <param name="version"> the Lucene match version - See <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <param name="factory"> <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/> to use </param>
            <param name="input"> <see cref="T:System.IO.TextReader"/> holding the input to be tokenized </param>
            <param name="minGram"> the smallest n-gram to generate </param>
            <param name="maxGram"> the largest n-gram to generate </param>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.Lucene43EdgeNGramTokenizer.IncrementToken">
            <summary>
            Returns the next token in the stream, or null at EOS. </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.NGram.Lucene43NGramTokenizer">
            <summary>
            Old broken version of <see cref="T:Lucene.Net.Analysis.NGram.NGramTokenizer"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.Lucene43NGramTokenizer.#ctor(System.IO.TextReader,System.Int32,System.Int32)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.Lucene43NGramTokenizer"/> with given min and max n-grams. </summary>
            <param name="input"> <see cref="T:System.IO.TextReader"/> holding the input to be tokenized </param>
            <param name="minGram"> the smallest n-gram to generate </param>
            <param name="maxGram"> the largest n-gram to generate </param>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.Lucene43NGramTokenizer.#ctor(Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader,System.Int32,System.Int32)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.Lucene43NGramTokenizer"/> with given min and max n-grams. </summary>
            <param name="factory"> <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/> to use </param>
            <param name="input"> <see cref="T:System.IO.TextReader"/> holding the input to be tokenized </param>
            <param name="minGram"> the smallest n-gram to generate </param>
            <param name="maxGram"> the largest n-gram to generate </param>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.Lucene43NGramTokenizer.#ctor(System.IO.TextReader)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.Lucene43NGramTokenizer"/> with default min and max n-grams. </summary>
            <param name="input"> <see cref="T:System.IO.TextReader"/> holding the input to be tokenized </param>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.Lucene43NGramTokenizer.IncrementToken">
            <summary>
            Returns the next token in the stream, or null at EOS. </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.NGram.NGramFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.NGram.NGramTokenFilter"/>.
            <code>
            &lt;fieldType name="text_ngrm" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.NGramFilterFactory" minGramSize="1" maxGramSize="2"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.NGramFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.NGram.NGramFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.NGram.NGramTokenFilter">
            <summary>
            Tokenizes the input into n-grams of the given size(s).
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/> compatibility when
            creating a <see cref="T:Lucene.Net.Analysis.NGram.NGramTokenFilter"/>. As of Lucene 4.4, this token filters:
            <list type="bullet">
                <item><description>handles supplementary characters correctly,</description></item>
                <item><description>emits all n-grams for the same token at the same position,</description></item>
                <item><description>does not modify offsets,</description></item>
                <item><description>sorts n-grams by their offset in the original token first, then
                    increasing length (meaning that "abc" will give "a", "ab", "abc", "b", "bc",
                    "c").</description></item>
            </list>
            </para>
            <para>You can make this filter use the old behavior by providing a version &lt;
            <see cref="F:Lucene.Net.Util.LuceneVersion.LUCENE_44"/> in the constructor but this is not recommended as
            it will lead to broken <see cref="T:Lucene.Net.Analysis.TokenStream"/>s that will cause highlighting
            bugs.
            </para>
            <para>If you were using this <see cref="T:Lucene.Net.Analysis.TokenFilter"/> to perform partial highlighting,
            this won't work anymore since this filter doesn't update offsets. You should
            modify your analysis chain to use <see cref="T:Lucene.Net.Analysis.NGram.NGramTokenizer"/>, and potentially
            override <see cref="M:Lucene.Net.Analysis.NGram.NGramTokenizer.IsTokenChar(System.Int32)"/> to perform pre-tokenization.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.NGramTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,System.Int32,System.Int32)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.NGramTokenFilter"/> with given min and max n-grams. </summary>
            <param name="version"> Lucene version to enable correct position increments.
                           See <see cref="T:Lucene.Net.Analysis.NGram.NGramTokenFilter"/> for details. </param>
            <param name="input"> <see cref="T:Lucene.Net.Analysis.TokenStream"/> holding the input to be tokenized </param>
            <param name="minGram"> the smallest n-gram to generate </param>
            <param name="maxGram"> the largest n-gram to generate </param>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.NGramTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.NGramTokenFilter"/> with default min and max n-grams. </summary>
            <param name="version"> Lucene version to enable correct position increments.
                           See <see cref="T:Lucene.Net.Analysis.NGram.NGramTokenFilter"/> for details. </param>
            <param name="input"> <see cref="T:Lucene.Net.Analysis.TokenStream"/> holding the input to be tokenized </param>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.NGramTokenFilter.IncrementToken">
            <summary>
            Returns the next token in the stream, or null at EOS.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.NGram.NGramTokenizer">
            <summary>
            Tokenizes the input into n-grams of the given size(s).
            <para>On the contrary to <see cref="T:Lucene.Net.Analysis.NGram.NGramTokenFilter"/>, this class sets offsets so
            that characters between startOffset and endOffset in the original stream are
            the same as the term chars.
            </para>
            <para>For example, "abcde" would be tokenized as (minGram=2, maxGram=3):
            <list type="table">
                <listheader>
                    <term>Term</term>
                    <term>Position increment</term>
                    <term>Position length</term>
                    <term>Offsets</term>
                </listheader>
                <item>
                    <term>ab</term>
                    <term>1</term>
                    <term>1</term>
                    <term>[0,2[</term>
                </item>
                <item>
                    <term>abc</term>
                    <term>1</term>
                    <term>1</term>
                    <term>[0,3[</term>
                </item>
                <item>
                    <term>bc</term>
                    <term>1</term>
                    <term>1</term>
                    <term>[1,3[</term>
                </item>
                <item>
                    <term>bcd</term>
                    <term>1</term>
                    <term>1</term>
                    <term>[1,4[</term>
                </item>
                <item>
                    <term>cd</term>
                    <term>1</term>
                    <term>1</term>
                    <term>[2,4[</term>
                </item>
                <item>
                    <term>cde</term>
                    <term>1</term>
                    <term>1</term>
                    <term>[2,5[</term>
                </item>
                <item>
                    <term>de</term>
                    <term>1</term>
                    <term>1</term>
                    <term>[3,5[</term>
                </item>
            </list>
            </para>
            <para>This tokenizer changed a lot in Lucene 4.4 in order to:
            <list type="bullet">
                <item><description>tokenize in a streaming fashion to support streams which are larger
                    than 1024 chars (limit of the previous version),</description></item>
                <item><description>count grams based on unicode code points instead of java chars (and
                    never split in the middle of surrogate pairs),</description></item>
                <item><description>give the ability to pre-tokenize the stream (<see cref="M:Lucene.Net.Analysis.NGram.NGramTokenizer.IsTokenChar(System.Int32)"/>)
                    before computing n-grams.</description></item>
            </list>
            </para>
            <para>Additionally, this class doesn't trim trailing whitespaces and emits
            tokens in a different order, tokens are now emitted by increasing start
            offsets while they used to be emitted by increasing lengths (which prevented
            from supporting large input streams).
            </para>
            <para>Although <b style="color:red">highly</b> discouraged, it is still possible
            to use the old behavior through <see cref="T:Lucene.Net.Analysis.NGram.Lucene43NGramTokenizer"/>.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.NGramTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader,System.Int32,System.Int32)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.NGramTokenizer"/> with given min and max n-grams. </summary>
            <param name="version"> the lucene compatibility version </param>
            <param name="input"> <see cref="T:System.IO.TextReader"/> holding the input to be tokenized </param>
            <param name="minGram"> the smallest n-gram to generate </param>
            <param name="maxGram"> the largest n-gram to generate </param>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.NGramTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader,System.Int32,System.Int32)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.NGramTokenizer"/> with given min and max n-grams. </summary>
            <param name="version"> the lucene compatibility version </param>
            <param name="factory"> <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/> to use </param>
            <param name="input"> <see cref="T:System.IO.TextReader"/> holding the input to be tokenized </param>
            <param name="minGram"> the smallest n-gram to generate </param>
            <param name="maxGram"> the largest n-gram to generate </param>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.NGramTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader)">
            <summary>
            Creates <see cref="T:Lucene.Net.Analysis.NGram.NGramTokenizer"/> with default min and max n-grams. </summary>
            <param name="version"> the lucene compatibility version </param>
            <param name="input"> <see cref="T:System.IO.TextReader"/> holding the input to be tokenized </param>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.NGramTokenizer.Consume">
            <summary>
            Consume one code point. </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.NGramTokenizer.IsTokenChar(System.Int32)">
            <summary>
            Only collect characters which satisfy this condition. </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.NGram.NGramTokenizerFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.NGram.NGramTokenizer"/>.
            <code>
            &lt;fieldType name="text_ngrm" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.NGramTokenizerFactory" minGramSize="1" maxGramSize="2"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.NGramTokenizerFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.NGram.NGramTokenizerFactory"/> </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.NGram.NGramTokenizerFactory.Create(Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader)">
            <summary>
            Creates the <see cref="T:Lucene.Net.Analysis.TokenStream"/> of n-grams from the given <see cref="T:System.IO.TextReader"/> and <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/>. </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Nl.DutchAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Dutch language. 
            <para>
            Supports an external list of stopwords (words that
            will not be indexed at all), an external list of exclusions (word that will
            not be stemmed, but indexed) and an external list of word-stem pairs that overrule
            the algorithm (dictionary stemming).
            A default set of stopwords is used unless an alternative list is specified, but the
            exclusion list is empty by default.
            </para>
            
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.Nl.DutchAnalyzer"/>:
            <list type="bullet">
              <item><description> As of 3.6, <see cref="M:Lucene.Net.Analysis.Nl.DutchAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)"/> and
                   <see cref="M:Lucene.Net.Analysis.Nl.DutchAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)"/> also populate
                   the default entries for the stem override dictionary</description></item>
              <item><description> As of 3.1, Snowball stemming is done with SnowballFilter, 
                   LowerCaseFilter is used prior to StopFilter, and Snowball 
                   stopwords are used by default.</description></item>
              <item><description> As of 2.9, StopFilter preserves position
                   increments</description></item>
            </list>
            
            </para>
            <para><b>NOTE</b>: This class uses the same <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            dependent settings as <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/>.</para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Nl.DutchAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Dutch stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Nl.DutchAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop-words set. </summary>
            <returns> an unmodifiable instance of the default stop-words set. </returns>
        </member>
        <member name="F:Lucene.Net.Analysis.Nl.DutchAnalyzer.stoptable">
            <summary>
            Contains the stopwords used with the <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Nl.DutchAnalyzer.excltable">
            <summary>
            Contains words that should be indexed but not stemmed.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Nl.DutchAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words (<see cref="P:Lucene.Net.Analysis.Nl.DutchAnalyzer.DefaultStopSet"/>) 
            and a few default entries for the stem exclusion table.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Nl.DutchAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Returns a (possibly reused) <see cref="T:Lucene.Net.Analysis.TokenStream"/> which tokenizes all the 
            text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A <see cref="T:Lucene.Net.Analysis.TokenStream"/> built from a <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/>
              filtered with <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, 
              <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>, <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is provided,
              <see cref="T:Lucene.Net.Analysis.Miscellaneous.StemmerOverrideFilter"/>, and <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/> </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Nl.DutchStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that stems Dutch words. 
            <para>
            It supports a table of words that should
            not be stemmed at all. The stemmer used can be changed at runtime after the
            filter object is created (as long as it is a <see cref="T:Lucene.Net.Analysis.Nl.DutchStemmer"/>).
            </para>
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.KeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para> 
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Miscellaneous.KeywordMarkerFilter"/>
            @deprecated (3.1) Use <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/> with 
            <see cref="T:Lucene.Net.Tartarus.Snowball.Ext.DutchStemmer"/> instead, which has the
            same functionality. This filter will be removed in Lucene 5.0 
        </member>
        <member name="F:Lucene.Net.Analysis.Nl.DutchStemFilter.stemmer">
            <summary>
            The actual token in the input stream.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Nl.DutchStemFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <param name="in"> Input <see cref="T:Lucene.Net.Analysis.TokenStream"/> </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Nl.DutchStemFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Collections.Generic.IDictionary{System.String,System.String})">
            <param name="in"> Input <see cref="T:Lucene.Net.Analysis.TokenStream"/> </param>
            <param name="stemdictionary"> Dictionary of word stem pairs, that overrule the algorithm </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Nl.DutchStemFilter.IncrementToken">
            <summary>
            Returns the next token in the stream, or null at EOS
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Nl.DutchStemFilter.Stemmer">
            <summary>
            Set a alternative/custom <see cref="T:Lucene.Net.Analysis.Nl.DutchStemmer"/> for this filter.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Nl.DutchStemFilter.StemDictionary">
            <summary>
            Set dictionary for stemming, this dictionary overrules the algorithm,
            so you can correct for a particular unwanted word-stem pair.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Nl.DutchStemmer">
            <summary>
            A stemmer for Dutch words. 
            <para>
            The algorithm is an implementation of
            the <a href="http://snowball.tartarus.org/algorithms/dutch/stemmer.html">dutch stemming</a>
            algorithm in Martin Porter's snowball project.
            </para> </summary>
            @deprecated (3.1) Use <see cref="T:Lucene.Net.Tartarus.Snowball.Ext.DutchStemmer"/> instead, 
            which has the same functionality. This filter will be removed in Lucene 5.0 
        </member>
        <member name="F:Lucene.Net.Analysis.Nl.DutchStemmer.sb">
            <summary>
            Buffer for the terms while stemming them.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Nl.DutchStemmer.Stem(System.String)">
            <summary>
            Stems the given term to an unique <c>discriminator</c>.
            </summary>
            <param name="term">The term that should be stemmed.</param>
            <returns>Discriminator for <paramref name="term"/></returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Nl.DutchStemmer.Step2(System.Text.StringBuilder)">
            <summary>
            Delete suffix e if in R1 and
            preceded by a non-vowel, and then undouble the ending
            </summary>
            <param name="sb"> String being stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Nl.DutchStemmer.Step3a(System.Text.StringBuilder)">
            <summary>
            Delete "heid"
            </summary>
            <param name="sb"> String being stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Nl.DutchStemmer.Step3b(System.Text.StringBuilder)">
            <summary>
            <para>A d-suffix, or derivational suffix, enables a new word,
            often with a different grammatical category, or with a different
            sense, to be built from another word. Whether a d-suffix can be
            attached is discovered not from the rules of grammar, but by
            referring to a dictionary. So in English, ness can be added to
            certain adjectives to form corresponding nouns (littleness,
            kindness, foolishness ...) but not to all adjectives
            (not for example, to big, cruel, wise ...) d-suffixes can be
            used to change meaning, often in rather exotic ways.</para>
            Remove "ing", "end", "ig", "lijk", "baar" and "bar"
            </summary>
            <param name="sb"> String being stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Nl.DutchStemmer.Step4(System.Text.StringBuilder)">
            <summary>
            undouble vowel
            If the words ends CVD, where C is a non-vowel, D is a non-vowel other than I, and V is double a, e, o or u, remove one of the vowels from V (for example, maan -> man, brood -> brod).
            </summary>
            <param name="sb"> String being stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Nl.DutchStemmer.IsStemmable(System.String)">
            <summary>
            Checks if a term could be stemmed.
            </summary>
            <returns> true if, and only if, the given term consists in letters. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Nl.DutchStemmer.Substitute(System.Text.StringBuilder)">
            <summary>
            Substitute ä, ë, ï, ö, ü, á , é, í, ó, ú
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.No.NorwegianAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Norwegian.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.No.NorwegianAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Norwegian stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.No.NorwegianAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.No.NorwegianAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.No.NorwegianAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.No.NorwegianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.No.NorwegianAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.No.NorwegianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.No.NorwegianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.No.NorwegianAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.No.NorwegianLightStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.No.NorwegianLightStemmer"/> to stem Norwegian
            words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.No.NorwegianLightStemFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Calls <see cref="M:Lucene.Net.Analysis.No.NorwegianLightStemFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.No.NorwegianStandard)"/>
            - NorwegianLightStemFilter(input, BOKMAAL)
            </summary>
            <param name="input"> the source <see cref="T:Lucene.Net.Analysis.TokenStream"/> to filter </param>
        </member>
        <member name="M:Lucene.Net.Analysis.No.NorwegianLightStemFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.No.NorwegianStandard)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.No.NorwegianLightStemFilter"/> </summary>
            <param name="input"> the source <see cref="T:Lucene.Net.Analysis.TokenStream"/> to filter </param>
            <param name="flags"> set to <see cref="F:Lucene.Net.Analysis.No.NorwegianStandard.BOKMAAL"/>, 
                                <see cref="F:Lucene.Net.Analysis.No.NorwegianStandard.NYNORSK"/>, or both. </param>
        </member>
        <member name="T:Lucene.Net.Analysis.No.NorwegianLightStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.No.NorwegianLightStemFilter"/>.
            <code>
            &lt;fieldType name="text_svlgtstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.NorwegianLightStemFilterFactory" variant="nb"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.No.NorwegianLightStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.No.NorwegianLightStemFilterFactory"/> </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.No.NorwegianStandard.BOKMAAL">
            <summary>
            Constant to remove Bokmål-specific endings </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.No.NorwegianStandard.NYNORSK">
            <summary>
            Constant to remove Nynorsk-specific endings </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.No.NorwegianLightStemmer">
            <summary>
            Light Stemmer for Norwegian.
            <para>
            Parts of this stemmer is adapted from <see cref="T:Lucene.Net.Analysis.Sv.SwedishLightStemFilter"/>, except
            that while the Swedish one has a pre-defined rule set and a corresponding
            corpus to validate against whereas the Norwegian one is hand crafted.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.No.NorwegianLightStemmer.#ctor(Lucene.Net.Analysis.No.NorwegianStandard)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.No.NorwegianLightStemmer"/> </summary>
            <param name="flags"> set to <see cref="F:Lucene.Net.Analysis.No.NorwegianStandard.BOKMAAL"/>, <see cref="F:Lucene.Net.Analysis.No.NorwegianStandard.NYNORSK"/>, or both. </param>
        </member>
        <member name="T:Lucene.Net.Analysis.No.NorwegianMinimalStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.No.NorwegianMinimalStemmer"/> to stem Norwegian
            words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.No.NorwegianMinimalStemFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Calls <see cref="M:Lucene.Net.Analysis.No.NorwegianLightStemFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.No.NorwegianStandard)"/> -
            NorwegianMinimalStemFilter(input, BOKMAAL)
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.No.NorwegianMinimalStemFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.No.NorwegianStandard)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.No.NorwegianLightStemFilter"/> </summary>
            <param name="input"> the source <see cref="T:Lucene.Net.Analysis.TokenStream"/> to filter </param>
            <param name="flags"> set to <see cref="F:Lucene.Net.Analysis.No.NorwegianStandard.BOKMAAL"/>, 
                                <see cref="F:Lucene.Net.Analysis.No.NorwegianStandard.NYNORSK"/>, or both. </param>
        </member>
        <member name="T:Lucene.Net.Analysis.No.NorwegianMinimalStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.No.NorwegianMinimalStemFilter"/>.
            <code>
            &lt;fieldType name="text_svlgtstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.NorwegianMinimalStemFilterFactory" variant="nb"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.No.NorwegianMinimalStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.No.NorwegianMinimalStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.No.NorwegianMinimalStemmer">
            <summary>
            Minimal Stemmer for Norwegian Bokmål (no-nb) and Nynorsk (no-nn)
            <para>
            Stems known plural forms for Norwegian nouns only, together with genitiv -s
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.No.NorwegianMinimalStemmer.#ctor(Lucene.Net.Analysis.No.NorwegianStandard)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.No.NorwegianMinimalStemmer"/> </summary>
            <param name="flags"> set to <see cref="F:Lucene.Net.Analysis.No.NorwegianStandard.BOKMAAL"/>, 
                                <see cref="F:Lucene.Net.Analysis.No.NorwegianStandard.NYNORSK"/>, or both. </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Path.PathHierarchyTokenizer">
            <summary>
            Tokenizer for path-like hierarchies.
            <para>
            Take something like:
            
            <code>
             /something/something/else
            </code>
            
            and make:
            
            <code>
             /something
             /something/something
             /something/something/else
            </code>
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Path.PathHierarchyTokenizerFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Path.PathHierarchyTokenizer"/>. 
            <para>
            This factory is typically configured for use only in the <code>index</code> 
            Analyzer (or only in the <c>query</c> Analyzer, but never both).
            </para>
            <para>
            For example, in the configuration below a query for 
            <c>Books/NonFic</c> will match documents indexed with values like 
            <c>Books/NonFic</c>, <c>Books/NonFic/Law</c>, 
            <c>Books/NonFic/Science/Physics</c>, etc. But it will not match 
            documents indexed with values like <c>Books</c>, or 
            <c>Books/Fic</c>...
            </para>
            
            <code>
            &lt;fieldType name="descendent_path" class="solr.TextField"&gt;
              &lt;analyzer type="index"&gt;
                &lt;tokenizer class="solr.PathHierarchyTokenizerFactory" delimiter="/" /&gt;
              &lt;/analyzer&gt;
              &lt;analyzer type="query"&gt;
                &lt;tokenizer class="solr.KeywordTokenizerFactory" /&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            <para>
            In this example however we see the oposite configuration, so that a query 
            for <c>Books/NonFic/Science/Physics</c> would match documents 
            containing <c>Books/NonFic</c>, <c>Books/NonFic/Science</c>, 
            or <c>Books/NonFic/Science/Physics</c>, but not 
            <c>Books/NonFic/Science/Physics/Theory</c> or 
            <c>Books/NonFic/Law</c>.
            </para>
            <code>
            &lt;fieldType name="descendent_path" class="solr.TextField"&gt;
              &lt;analyzer type="index"&gt;
                &lt;tokenizer class="solr.KeywordTokenizerFactory" /&gt;
              &lt;/analyzer&gt;
              &lt;analyzer type="query"&gt;
                &lt;tokenizer class="solr.PathHierarchyTokenizerFactory" delimiter="/" /&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;
            </code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Path.PathHierarchyTokenizerFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Path.PathHierarchyTokenizerFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Path.ReversePathHierarchyTokenizer">
            <summary>
            Tokenizer for domain-like hierarchies.
            <para>
            Take something like:
            
            <code>
            www.site.co.uk
            </code>
            
            and make:
            
            <code>
            www.site.co.uk
            site.co.uk
            co.uk
            uk
            </code>
            
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Pattern.PatternCaptureGroupFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Pattern.PatternCaptureGroupTokenFilter"/>. 
            <code>
            &lt;fieldType name="text_ptncapturegroup" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.KeywordTokenizerFactory"/&gt;
                &lt;filter class="solr.PatternCaptureGroupFilterFactory" pattern="([^a-z])" preserve_original="true"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Pattern.PatternCaptureGroupTokenFilter"/>
        </member>
        <member name="T:Lucene.Net.Analysis.Pattern.PatternCaptureGroupTokenFilter">
            <summary>
            CaptureGroup uses .NET regexes to emit multiple tokens - one for each capture
            group in one or more patterns.
            
            <para>
            For example, a pattern like:
            </para>
            
            <para>
            <c>"(https?://([a-zA-Z\-_0-9.]+))"</c>
            </para>
            
            <para>
            when matched against the string "http://www.foo.com/index" would return the
            tokens "https://www.foo.com" and "www.foo.com".
            </para>
            
            <para>
            If none of the patterns match, or if preserveOriginal is true, the original
            token will be preserved.
            </para>
            <para>
            Each pattern is matched as often as it can be, so the pattern
            <c> "(...)"</c>, when matched against <c>"abcdefghi"</c> would
            produce <c>["abc","def","ghi"]</c>
            </para>
            <para>
            A camelCaseFilter could be written as:
            </para>
            <para>
            <code>
              "([A-Z]{2,})",                                 
              "(?&lt;![A-Z])([A-Z][a-z]+)",                     
              "(?:^|\\b|(?&lt;=[0-9_])|(?&lt;=[A-Z]{2}))([a-z]+)", 
              "([0-9]+)"
            </code>
            </para>
            <para>
            plus if <see cref="F:Lucene.Net.Analysis.Pattern.PatternCaptureGroupTokenFilter.preserveOriginal"/> is true, it would also return
            <c>camelCaseFilter</c>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Pattern.PatternCaptureGroupTokenFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Boolean,System.Text.RegularExpressions.Regex[])">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Pattern.PatternCaptureGroupTokenFilter"/>
            </summary>
            <param name="input">
                     the input <see cref="T:Lucene.Net.Analysis.TokenStream"/> </param>
            <param name="preserveOriginal">
                     set to true to return the original token even if one of the
                     patterns matches </param>
            <param name="patterns">
                     an array of <see cref="N:Lucene.Net.Analysis.Pattern"/> objects to match against each token </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Pattern.PatternReplaceCharFilter">
            <summary>
            <see cref="T:Lucene.Net.Analysis.CharFilter"/> that uses a regular expression for the target of replace string.
            The pattern match will be done in each "block" in char stream.
            
            <para>
            ex1) source="aa  bb aa bb", pattern="(aa)\\s+(bb)" replacement="$1#$2"
            output="aa#bb aa#bb"
            </para>
            
            NOTE: If you produce a phrase that has different length to source string
            and the field is used for highlighting for a term of the phrase, you will
            face a trouble.
            
            <para>
            ex2) source="aa123bb", pattern="(aa)\\d+(bb)" replacement="$1 $2"
            output="aa bb"
            and you want to search bb and highlight it, you will get
            highlight snippet="aa1&lt;em&gt;23bb&lt;/em&gt;"
            </para>
            
            @since Solr 1.5
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Pattern.PatternReplaceCharFilter.ProcessPattern(System.Text.StringBuilder)">
            <summary>
            Replace pattern in input and mark correction offsets. 
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Pattern.PatternReplaceCharFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Pattern.PatternReplaceCharFilter"/>. 
            <code>
            &lt;fieldType name="text_ptnreplace" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;charFilter class="solr.PatternReplaceCharFilterFactory" 
                               pattern="([^a-z])" replacement=""/&gt;
                &lt;tokenizer class="solr.KeywordTokenizerFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            
            @since Solr 3.1
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Pattern.PatternReplaceCharFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Pattern.PatternReplaceCharFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Pattern.PatternReplaceFilter">
            <summary>
            A TokenFilter which applies a <see cref="T:System.Text.RegularExpressions.Regex"/> to each token in the stream,
            replacing match occurances with the specified replacement string.
            
            <para>
            <b>Note:</b> Depending on the input and the pattern used and the input
            <see cref="T:Lucene.Net.Analysis.TokenStream"/>, this <see cref="T:Lucene.Net.Analysis.TokenFilter"/> may produce <see cref="T:Lucene.Net.Analysis.Token"/>s whose text is the empty
            string.
            </para>
            </summary>
            <seealso cref="T:System.Text.RegularExpressions.Regex"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Pattern.PatternReplaceFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Text.RegularExpressions.Regex,System.String,System.Boolean)">
            <summary>
            Constructs an instance to replace either the first, or all occurances
            </summary>
            <param name="in"> the <see cref="T:Lucene.Net.Analysis.TokenStream"/> to process </param>
            <param name="pattern"> the pattern (a <see cref="T:System.Text.RegularExpressions.Regex"/> object) to apply to each <see cref="T:Lucene.Net.Analysis.Token"/> </param>
            <param name="replacement"> the "replacement string" to substitute, if <c>null</c> a
                   blank string will be used. Note that this is not the literal
                   string that will be used, '$' and '\' have special meaning. </param>
            <param name="all"> if true, all matches will be replaced otherwise just the first match. </param>
            <seealso cref="M:System.Text.RegularExpressions.Regex.Replace(System.String,System.String)"/>
        </member>
        <member name="T:Lucene.Net.Analysis.Pattern.PatternReplaceFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Pattern.PatternReplaceFilter"/>. 
            <code>
            &lt;fieldType name="text_ptnreplace" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.KeywordTokenizerFactory"/&gt;
                &lt;filter class="solr.PatternReplaceFilterFactory" pattern="([^a-z])" replacement=""
                        replace="all"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Pattern.PatternReplaceFilter"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Pattern.PatternReplaceFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Pattern.PatternReplaceFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Pattern.PatternTokenizer">
            <summary>
            This tokenizer uses regex pattern matching to construct distinct tokens
            for the input stream.  It takes two arguments:  "pattern" and "group".
            <para/>
            <list type="bullet">
                <item><description>"pattern" is the regular expression.</description></item>
                <item><description>"group" says which group to extract into tokens.</description></item>
            </list>
            <para>
            group=-1 (the default) is equivalent to "split".  In this case, the tokens will
            be equivalent to the output from (without empty tokens):
            <see cref="M:System.Text.RegularExpressions.Regex.Replace(System.String,System.String)"/>
            </para>
            <para>
            Using group >= 0 selects the matching group as the token.  For example, if you have:<br/>
            <code>
             pattern = \'([^\']+)\'
             group = 0
             input = aaa 'bbb' 'ccc'
            </code>
            the output will be two tokens: 'bbb' and 'ccc' (including the ' marks).  With the same input
            but using group=1, the output would be: bbb and ccc (no ' marks)
            </para>
            <para>NOTE: This <see cref="T:Lucene.Net.Analysis.Tokenizer"/> does not output tokens that are of zero length.</para>
            </summary>
            <seealso cref="T:System.Text.RegularExpressions.Regex"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Pattern.PatternTokenizer.#ctor(System.IO.TextReader,System.Text.RegularExpressions.Regex,System.Int32)">
            <summary>
            creates a new <see cref="T:Lucene.Net.Analysis.Pattern.PatternTokenizer"/> returning tokens from group (-1 for split functionality) </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Pattern.PatternTokenizer.#ctor(Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader,System.Text.RegularExpressions.Regex,System.Int32)">
            <summary>
            creates a new <see cref="T:Lucene.Net.Analysis.Pattern.PatternTokenizer"/> returning tokens from group (-1 for split functionality) </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Pattern.PatternTokenizerFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Pattern.PatternTokenizer"/>.
            This tokenizer uses regex pattern matching to construct distinct tokens
            for the input stream.  It takes two arguments:  "pattern" and "group".
            <para/>
            <list type="bullet">
                <item><description>"pattern" is the regular expression.</description></item>
                <item><description>"group" says which group to extract into tokens.</description></item>
            </list>
            <para>
            group=-1 (the default) is equivalent to "split".  In this case, the tokens will
            be equivalent to the output from (without empty tokens):
            <see cref="M:System.Text.RegularExpressions.Regex.Replace(System.String,System.String)"/>
            </para>
            <para>
            Using group &gt;= 0 selects the matching group as the token.  For example, if you have:<br/>
            <code>
                pattern = \'([^\']+)\'
                group = 0
                input = aaa 'bbb' 'ccc'
            </code>
            the output will be two tokens: 'bbb' and 'ccc' (including the ' marks).  With the same input
            but using group=1, the output would be: bbb and ccc (no ' marks)
            </para>
            <para>NOTE: This Tokenizer does not output tokens that are of zero length.</para>
            
            <code>
            &lt;fieldType name="text_ptn" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.PatternTokenizerFactory" pattern="\'([^\']+)\'" group="1"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code> 
            
            @since solr1.2
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Pattern.PatternTokenizer"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Pattern.PatternTokenizerFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Pattern.PatternTokenizerFactory"/> </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Pattern.PatternTokenizerFactory.Create(Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader)">
            <summary>
            Split the input using configured pattern
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Payloads.AbstractEncoder">
            <summary>
            Base class for payload encoders.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Payloads.DelimitedPayloadTokenFilter">
            <summary>
            Characters before the delimiter are the "token", those after are the payload.
            <para/>
            For example, if the delimiter is '|', then for the string "foo|bar", foo is the token
            and "bar" is a payload.
            <para/>
            Note, you can also include a <see cref="T:Lucene.Net.Analysis.Payloads.IPayloadEncoder"/> to convert the payload in an appropriate way (from characters to bytes).
            <para/>
            Note make sure your <see cref="T:Lucene.Net.Analysis.Tokenizer"/> doesn't split on the delimiter, or this won't work
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Payloads.IPayloadEncoder"/>
        </member>
        <member name="T:Lucene.Net.Analysis.Payloads.DelimitedPayloadTokenFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Payloads.DelimitedPayloadTokenFilter"/>.
            <code>
            &lt;fieldType name="text_dlmtd" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.DelimitedPayloadTokenFilterFactory" encoder="float" delimiter="|"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Payloads.DelimitedPayloadTokenFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Payloads.DelimitedPayloadTokenFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Payloads.SingleEncoder">
            <summary>
            Encode a character array <see cref="T:System.Single"/> as a <see cref="T:Lucene.Net.Util.BytesRef"/>.
            <para/>
            NOTE: This was FloatEncoder in Lucene
            </summary>
            <seealso cref="M:Lucene.Net.Analysis.Payloads.PayloadHelper.EncodeSingle(System.Single,System.Byte[],System.Int32)"/>
        </member>
        <member name="T:Lucene.Net.Analysis.Payloads.IdentityEncoder">
            <summary>
             Does nothing other than convert the char array to a byte array using the specified encoding.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Payloads.IntegerEncoder">
            <summary>
            Encode a character array <see cref="T:System.Int32"/> as a <see cref="T:Lucene.Net.Util.BytesRef"/>.
            <para/>
            See <see cref="M:Lucene.Net.Analysis.Payloads.PayloadHelper.EncodeInt32(System.Int32,System.Byte[],System.Int32)"/>.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Payloads.NumericPayloadTokenFilter">
            <summary>
            Assigns a payload to a token based on the <see cref="P:Lucene.Net.Analysis.Token.Type"/>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Payloads.NumericPayloadTokenFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Payloads.NumericPayloadTokenFilter"/>.
            <code>
            &lt;fieldType name="text_numpayload" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.NumericPayloadTokenFilterFactory" payload="24" typeMatch="word"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Payloads.NumericPayloadTokenFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Payloads.NumericPayloadTokenFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Payloads.IPayloadEncoder">
            <summary>
            Mainly for use with the <see cref="T:Lucene.Net.Analysis.Payloads.DelimitedPayloadTokenFilter"/>, converts char buffers to
            <see cref="T:Lucene.Net.Util.BytesRef"/>.
            <para/>
            NOTE: This interface is subject to change 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Payloads.IPayloadEncoder.Encode(System.Char[],System.Int32,System.Int32)">
            <summary>
            Convert a char array to a <see cref="T:Lucene.Net.Util.BytesRef"/> </summary>
            <returns> encoded <see cref="T:Lucene.Net.Util.BytesRef"/> </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Payloads.PayloadHelper">
            <summary>
            Utility methods for encoding payloads.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Payloads.PayloadHelper.EncodeSingle(System.Single)">
            <summary>
            NOTE: This was encodeFloat() in Lucene
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Payloads.PayloadHelper.EncodeSingle(System.Single,System.Byte[],System.Int32)">
            <summary>
            NOTE: This was encodeFloat() in Lucene
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Payloads.PayloadHelper.EncodeInt32(System.Int32)">
            <summary>
            NOTE: This was encodeInt() in Lucene
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Payloads.PayloadHelper.EncodeInt32(System.Int32,System.Byte[],System.Int32)">
            <summary>
            NOTE: This was encodeInt() in Lucene
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Payloads.PayloadHelper.DecodeSingle(System.Byte[])">
            <summary>
            NOTE: This was decodeFloat() in Lucene
            </summary>
            <seealso cref="M:Lucene.Net.Analysis.Payloads.PayloadHelper.DecodeSingle(System.Byte[],System.Int32)"/>
            <seealso cref="M:Lucene.Net.Analysis.Payloads.PayloadHelper.EncodeSingle(System.Single)"/>
            <returns> the decoded float </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Payloads.PayloadHelper.DecodeSingle(System.Byte[],System.Int32)">
            <summary>
            Decode the payload that was encoded using <see cref="M:Lucene.Net.Analysis.Payloads.PayloadHelper.EncodeSingle(System.Single)"/>.
            NOTE: the length of the array must be at least offset + 4 long. 
            <para/>
            NOTE: This was decodeFloat() in Lucene
            </summary>
            <param name="bytes"> The bytes to decode </param>
            <param name="offset"> The offset into the array. </param>
            <returns> The float that was encoded
            </returns>
            <seealso cref="M:Lucene.Net.Analysis.Payloads.PayloadHelper.EncodeSingle(System.Single)"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Payloads.PayloadHelper.DecodeInt32(System.Byte[],System.Int32)">
            <summary>
            NOTE: This was decodeInt() in Lucene
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Payloads.TokenOffsetPayloadTokenFilter">
            <summary>
            Adds the <see cref="P:Lucene.Net.Analysis.TokenAttributes.OffsetAttribute.StartOffset"/>
            and <see cref="P:Lucene.Net.Analysis.TokenAttributes.OffsetAttribute.EndOffset"/>
            First 4 bytes are the start
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Payloads.TokenOffsetPayloadTokenFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Payloads.TokenOffsetPayloadTokenFilter"/>.
            <code>
            &lt;fieldType name="text_tokenoffset" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.TokenOffsetPayloadTokenFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Payloads.TokenOffsetPayloadTokenFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Payloads.TokenOffsetPayloadTokenFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Payloads.TypeAsPayloadTokenFilter">
            <summary>
            Makes the <see cref="P:Lucene.Net.Analysis.Token.Type"/> a payload.
            <para/>
            Encodes the type using System.Text.Encoding.UTF8.GetBytes(string)
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Payloads.TypeAsPayloadTokenFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Payloads.TypeAsPayloadTokenFilter"/>.
            <code>
            &lt;fieldType name="text_typeaspayload" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.TypeAsPayloadTokenFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Payloads.TypeAsPayloadTokenFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Payloads.TypeAsPayloadTokenFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Position.PositionFilter">
            <summary>
            Set the positionIncrement of all tokens to the "positionIncrement",
            except the first return token which retains its original positionIncrement value.
            The default positionIncrement value is zero. </summary>
            @deprecated (4.4) <see cref="T:Lucene.Net.Analysis.Position.PositionFilter"/> makes <see cref="T:Lucene.Net.Analysis.TokenStream"/> graphs inconsistent
                        which can cause highlighting bugs. Its main use-case being to make
                        QueryParser
                        generate boolean queries instead of phrase queries, it is now advised to use
                        <c>QueryParser.AutoGeneratePhraseQueries = true</c>
                        (for simple cases) or to override <c>QueryParser.NewFieldQuery</c>. 
        </member>
        <member name="F:Lucene.Net.Analysis.Position.PositionFilter.positionIncrement">
            <summary>
            Position increment to assign to all but the first token - default = 0 </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Position.PositionFilter.firstTokenPositioned">
            <summary>
            The first token must have non-zero positionIncrement * </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Position.PositionFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Constructs a <see cref="T:Lucene.Net.Analysis.Position.PositionFilter"/> that assigns a position increment of zero to
            all but the first token from the given input stream.
            </summary>
            <param name="input"> the input stream </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Position.PositionFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Int32)">
            <summary>
            Constructs a <see cref="T:Lucene.Net.Analysis.Position.PositionFilter"/> that assigns the given position increment to
            all but the first token from the given input stream.
            </summary>
            <param name="input"> the input stream </param>
            <param name="positionIncrement"> position increment to assign to all but the first
             token from the input stream </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Position.PositionFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Position.PositionFilter"/>.
            Set the positionIncrement of all tokens to the "positionIncrement", except the first return token which retains its
            original positionIncrement value. The default positionIncrement value is zero.
            <code>
            &lt;fieldType name="text_position" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.PositionFilterFactory" positionIncrement="0"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Position.PositionFilter"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Position.PositionFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Position.PositionFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Pt.PortugueseAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Portuguese.
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.Pt.PortugueseAnalyzer"/>:
            <list type="bullet">
                <item><description> As of 3.6, PortugueseLightStemFilter is used for less aggressive stemming.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Pt.PortugueseAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Portuguese stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Pt.PortugueseAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Pt.PortugueseAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Pt.PortugueseAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Pt.PortugueseAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.Pt.PortugueseAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Pt.PortugueseAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Pt.PortugueseAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Pt.PortugueseAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>
                    , <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.Pt.PortugueseLightStemFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Pt.PortugueseLightStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Pt.PortugueseLightStemmer"/> to stem 
            Portuguese words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Pt.PortugueseLightStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Pt.PortugueseLightStemFilter"/>.
            <code>
            &lt;fieldType name="text_ptlgtstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.PortugueseLightStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Pt.PortugueseLightStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Pt.PortugueseLightStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Pt.PortugueseLightStemmer">
            <summary>
            Light Stemmer for Portuguese
            <para>
            This stemmer implements the "UniNE" algorithm in:
            <c>Light Stemming Approaches for the French, Portuguese, German and Hungarian Languages</c>
            Jacques Savoy
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Pt.PortugueseMinimalStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Pt.PortugueseMinimalStemmer"/> to stem 
            Portuguese words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Pt.PortugueseMinimalStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Pt.PortugueseMinimalStemFilter"/>.
            <code>
            &lt;fieldType name="text_ptminstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.PortugueseMinimalStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Pt.PortugueseMinimalStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Pt.PortugueseMinimalStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Pt.PortugueseMinimalStemmer">
            <summary>
            Minimal Stemmer for Portuguese
            <para>
            This follows the "RSLP-S" algorithm presented in:
            <c>A study on the Use of Stemming for Monolingual Ad-Hoc Portuguese
            Information Retrieval</c> (Orengo, et al)
            which is just the plural reduction step of the RSLP
            algorithm from <c>A Stemming Algorithm for the Portuguese Language</c>,
            Orengo et al.
            </para>
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Pt.RSLPStemmerBase"/>
        </member>
        <member name="T:Lucene.Net.Analysis.Pt.PortugueseStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Pt.PortugueseStemmer"/> to stem 
            Portuguese words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Pt.PortugueseStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Pt.PortugueseStemFilter"/>. 
            <code>
            &lt;fieldType name="text_ptstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.PortugueseStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Pt.PortugueseStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Pt.PortugueseStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Pt.PortugueseStemmer">
            <summary>
            Portuguese stemmer implementing the RSLP (Removedor de Sufixos da Lingua Portuguesa)
            algorithm. This is sometimes also referred to as the Orengo stemmer.
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Pt.RSLPStemmerBase"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Pt.PortugueseStemmer.Stem(System.Char[],System.Int32)">
            <param name="s"> buffer, oversized to at least <code>len+1</code> </param>
            <param name="len"> initial valid length of buffer </param>
            <returns> new valid length, stemmed </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Pt.RSLPStemmerBase">
            <summary>
            Base class for stemmers that use a set of RSLP-like stemming steps.
            <para>
            RSLP (Removedor de Sufixos da Lingua Portuguesa) is an algorithm designed
            originally for stemming the Portuguese language, described in the paper
            <c>A Stemming Algorithm for the Portuguese Language</c>, Orengo et. al.
            </para>
            <para>
            Since this time a plural-only modification (RSLP-S) as well as a modification
            for the Galician language have been implemented. This class parses a configuration
            file that describes <see cref="T:Lucene.Net.Analysis.Pt.RSLPStemmerBase.Step"/>s, where each <see cref="T:Lucene.Net.Analysis.Pt.RSLPStemmerBase.Step"/> contains a set of <see cref="T:Lucene.Net.Analysis.Pt.RSLPStemmerBase.Rule"/>s.
            </para>
            <para>
            The general rule format is: 
            <code>{ "suffix", N, "replacement", { "exception1", "exception2", ...}}</code>
            where:
            <list type="bullet">
              <item><description><c>suffix</c> is the suffix to be removed (such as "inho").</description></item>
              <item><description><c>N</c> is the min stem size, where stem is defined as the candidate stem 
                  after removing the suffix (but before appending the replacement!)</description></item>
              <item><description><c>replacement</c> is an optimal string to append after removing the suffix.
                  This can be the empty string.</description></item>
              <item><description><c>exceptions</c> is an optional list of exceptions, patterns that should 
                  not be stemmed. These patterns can be specified as whole word or suffix (ends-with) 
                  patterns, depending upon the exceptions format flag in the step header.</description></item>
            </list>
            </para>
            <para>
            A step is an ordered list of rules, with a structure in this format:
            <blockquote>{ "name", N, B, { "cond1", "cond2", ... }
                          ... rules ... };
            </blockquote>
            where:
            <list type="bullet">
              <item><description><c>name</c> is a name for the step (such as "Plural").</description></item>
              <item><description><c>N</c> is the min word size. Words that are less than this length bypass
                  the step completely, as an optimization. Note: N can be zero, in this case this 
                  implementation will automatically calculate the appropriate value from the underlying 
                  rules.</description></item>
              <item><description><c>B</c> is a "boolean" flag specifying how exceptions in the rules are matched.
                  A value of 1 indicates whole-word pattern matching, a value of 0 indicates that 
                  exceptions are actually suffixes and should be matched with ends-with.</description></item>
              <item><description><c>conds</c> are an optional list of conditions to enter the step at all. If
                  the list is non-empty, then a word must end with one of these conditions or it will
                  bypass the step completely as an optimization.</description></item>
            </list>
            </para>
            <a href="http://www.inf.ufrgs.br/~viviane/rslp/index.htm">RSLP description</a>
            @lucene.internal
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Pt.RSLPStemmerBase.Rule">
            <summary>
            A basic rule, with no exceptions.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Pt.RSLPStemmerBase.Rule.#ctor(System.String,System.Int32,System.String)">
            <summary>
            Create a rule. </summary>
            <param name="suffix"> suffix to remove </param>
            <param name="min"> minimum stem length </param>
            <param name="replacement"> replacement string </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Pt.RSLPStemmerBase.Rule.Matches(System.Char[],System.Int32)">
            <returns> true if the word matches this rule. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Pt.RSLPStemmerBase.Rule.Replace(System.Char[],System.Int32)">
            <returns> new valid length of the string after firing this rule. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Pt.RSLPStemmerBase.RuleWithSetExceptions">
            <summary>
            A rule with a set of whole-word exceptions.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Pt.RSLPStemmerBase.RuleWithSuffixExceptions">
            <summary>
            A rule with a set of exceptional suffixes.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Pt.RSLPStemmerBase.Step">
            <summary>
            A step containing a list of rules.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Pt.RSLPStemmerBase.Step.#ctor(System.String,Lucene.Net.Analysis.Pt.RSLPStemmerBase.Rule[],System.Int32,System.String[])">
            <summary>
            Create a new step </summary>
            <param name="name"> Step's name. </param>
            <param name="rules"> an ordered list of rules. </param>
            <param name="min"> minimum word size. if this is 0 it is automatically calculated. </param>
            <param name="suffixes"> optional list of conditional suffixes. may be null. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Pt.RSLPStemmerBase.Step.Apply(System.Char[],System.Int32)">
            <returns> new valid length of the string after applying the entire step. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Pt.RSLPStemmerBase.Parse(System.Type,System.String)">
            <summary>
            Parse a resource file into an RSLP stemmer description. </summary>
            <returns> a Map containing the named <see cref="T:Lucene.Net.Analysis.Pt.RSLPStemmerBase.Step"/>s in this description. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Query.QueryAutoStopWordAnalyzer">
            <summary>
            An <see cref="T:Lucene.Net.Analysis.Analyzer"/> used primarily at query time to wrap another analyzer and provide a layer of protection
            which prevents very common words from being passed into queries. 
            <para>
            For very large indexes the cost
            of reading TermDocs for a very common word can be  high. This analyzer was created after experience with
            a 38 million doc index which had a term in around 50% of docs and was causing TermQueries for 
            this term to take 2 seconds.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Query.QueryAutoStopWordAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Analyzer,Lucene.Net.Index.IndexReader)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Query.QueryAutoStopWordAnalyzer"/> with stopwords calculated for all
            indexed fields from terms with a document frequency percentage greater than
            <see cref="F:Lucene.Net.Analysis.Query.QueryAutoStopWordAnalyzer.defaultMaxDocFreqPercent"/>
            </summary>
            <param name="matchVersion"> Version to be used in <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/> </param>
            <param name="delegate"> <see cref="T:Lucene.Net.Analysis.Analyzer"/> whose <see cref="T:Lucene.Net.Analysis.TokenStream"/> will be filtered </param>
            <param name="indexReader"> <see cref="T:Lucene.Net.Index.IndexReader"/> to identify the stopwords from </param>
            <exception cref="T:System.IO.IOException"> Can be thrown while reading from the <see cref="T:Lucene.Net.Index.IndexReader"/> </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Query.QueryAutoStopWordAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Analyzer,Lucene.Net.Index.IndexReader,System.Int32)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Query.QueryAutoStopWordAnalyzer"/> with stopwords calculated for all
            indexed fields from terms with a document frequency greater than the given
            <paramref name="maxDocFreq"/>
            </summary>
            <param name="matchVersion"> Version to be used in <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/> </param>
            <param name="delegate"> <see cref="T:Lucene.Net.Analysis.Analyzer"/> whose <see cref="T:Lucene.Net.Analysis.TokenStream"/> will be filtered </param>
            <param name="indexReader"> <see cref="T:Lucene.Net.Index.IndexReader"/> to identify the stopwords from </param>
            <param name="maxDocFreq"> Document frequency terms should be above in order to be stopwords </param>
            <exception cref="T:System.IO.IOException"> Can be thrown while reading from the <see cref="T:Lucene.Net.Index.IndexReader"/> </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Query.QueryAutoStopWordAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Analyzer,Lucene.Net.Index.IndexReader,System.Single)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Query.QueryAutoStopWordAnalyzer"/> with stopwords calculated for all
            indexed fields from terms with a document frequency percentage greater than
            the given <paramref name="maxPercentDocs"/>
            </summary>
            <param name="matchVersion"> Version to be used in <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/> </param>
            <param name="delegate"> <see cref="T:Lucene.Net.Analysis.Analyzer"/> whose <see cref="T:Lucene.Net.Analysis.TokenStream"/> will be filtered </param>
            <param name="indexReader"> <see cref="T:Lucene.Net.Index.IndexReader"/> to identify the stopwords from </param>
            <param name="maxPercentDocs"> The maximum percentage (between 0.0 and 1.0) of index documents which
                                 contain a term, after which the word is considered to be a stop word </param>
            <exception cref="T:System.IO.IOException"> Can be thrown while reading from the <see cref="T:Lucene.Net.Index.IndexReader"/> </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Query.QueryAutoStopWordAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Analyzer,Lucene.Net.Index.IndexReader,System.Collections.Generic.ICollection{System.String},System.Single)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Query.QueryAutoStopWordAnalyzer"/> with stopwords calculated for the
            given selection of fields from terms with a document frequency percentage
            greater than the given <paramref name="maxPercentDocs"/>
            </summary>
            <param name="matchVersion"> Version to be used in <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/> </param>
            <param name="delegate"> <see cref="T:Lucene.Net.Analysis.Analyzer"/> whose <see cref="T:Lucene.Net.Analysis.TokenStream"/> will be filtered </param>
            <param name="indexReader"> <see cref="T:Lucene.Net.Index.IndexReader"/> to identify the stopwords from </param>
            <param name="fields"> Selection of fields to calculate stopwords for </param>
            <param name="maxPercentDocs"> The maximum percentage (between 0.0 and 1.0) of index documents which
                                 contain a term, after which the word is considered to be a stop word </param>
            <exception cref="T:System.IO.IOException"> Can be thrown while reading from the <see cref="T:Lucene.Net.Index.IndexReader"/> </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Query.QueryAutoStopWordAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Analyzer,Lucene.Net.Index.IndexReader,System.Collections.Generic.ICollection{System.String},System.Int32)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Query.QueryAutoStopWordAnalyzer"/> with stopwords calculated for the
            given selection of fields from terms with a document frequency greater than
            the given <paramref name="maxDocFreq"/>
            </summary>
            <param name="matchVersion"> Version to be used in <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/> </param>
            <param name="delegate"> Analyzer whose TokenStream will be filtered </param>
            <param name="indexReader"> <see cref="T:Lucene.Net.Index.IndexReader"/> to identify the stopwords from </param>
            <param name="fields"> Selection of fields to calculate stopwords for </param>
            <param name="maxDocFreq"> Document frequency terms should be above in order to be stopwords </param>
            <exception cref="T:System.IO.IOException"> Can be thrown while reading from the <see cref="T:Lucene.Net.Index.IndexReader"/> </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Query.QueryAutoStopWordAnalyzer.GetStopWords(System.String)">
            <summary>
            Provides information on which stop words have been identified for a field
            </summary>
            <param name="fieldName"> The field for which stop words identified in "addStopWords"
                             method calls will be returned </param>
            <returns> the stop words identified for a field </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Query.QueryAutoStopWordAnalyzer.GetStopWords">
            <summary>
            Provides information on which stop words have been identified for all fields
            </summary>
            <returns> the stop words (as terms) </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Reverse.ReverseStringFilter">
            <summary>
            Reverse token string, for example "country" => "yrtnuoc".
            <para>
            If <see cref="F:Lucene.Net.Analysis.Reverse.ReverseStringFilter.marker"/> is supplied, then tokens will be also prepended by
            that character. For example, with a marker of &#x5C;u0001, "country" =>
            "&#x5C;u0001yrtnuoc". This is useful when implementing efficient leading
            wildcards search.
            </para>
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.Reverse.ReverseStringFilter"/>, or when using any of
            its static methods:
            <list type="bullet">
                <item><description> As of 3.1, supplementary characters are handled correctly</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Reverse.ReverseStringFilter.START_OF_HEADING_MARKER">
            <summary>
            Example marker character: U+0001 (START OF HEADING) 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Reverse.ReverseStringFilter.INFORMATION_SEPARATOR_MARKER">
            <summary>
            Example marker character: U+001F (INFORMATION SEPARATOR ONE)
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Reverse.ReverseStringFilter.PUA_EC00_MARKER">
            <summary>
            Example marker character: U+EC00 (PRIVATE USE AREA: EC00) 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Reverse.ReverseStringFilter.RTL_DIRECTION_MARKER">
            <summary>
            Example marker character: U+200F (RIGHT-TO-LEFT MARK)
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Reverse.ReverseStringFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Reverse.ReverseStringFilter"/> that reverses all tokens in the 
            supplied <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            <para>
            The reversed tokens will not be marked. 
            </para>
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="in"> <see cref="T:Lucene.Net.Analysis.TokenStream"/> to filter </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Reverse.ReverseStringFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream,System.Char)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Reverse.ReverseStringFilter"/> that reverses and marks all tokens in the
            supplied <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            <para>
            The reversed tokens will be prepended (marked) by the <paramref name="marker"/>
            character.
            </para>
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="in"> <see cref="T:Lucene.Net.Analysis.TokenStream"/> to filter </param>
            <param name="marker"> A character used to mark reversed tokens </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Reverse.ReverseStringFilter.Reverse(Lucene.Net.Util.LuceneVersion,System.String)">
            <summary>
            Reverses the given input string
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="input"> the string to reverse </param>
            <returns> the given input string in reversed order </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Reverse.ReverseStringFilter.Reverse(Lucene.Net.Util.LuceneVersion,System.Char[])">
            <summary>
            Reverses the given input buffer in-place </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="buffer"> the input char array to reverse </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Reverse.ReverseStringFilter.Reverse(Lucene.Net.Util.LuceneVersion,System.Char[],System.Int32)">
            <summary>
            Partially reverses the given input buffer in-place from offset 0
            up to the given length. </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="buffer"> the input char array to reverse </param>
            <param name="len"> the length in the buffer up to where the
                   buffer should be reversed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Reverse.ReverseStringFilter.ReverseUnicode3(System.Char[],System.Int32,System.Int32)">
            @deprecated (3.1) Remove this when support for 3.0 indexes is no longer needed. 
        </member>
        <member name="M:Lucene.Net.Analysis.Reverse.ReverseStringFilter.Reverse(Lucene.Net.Util.LuceneVersion,System.Char[],System.Int32,System.Int32)">
            <summary>
            Partially reverses the given input buffer in-place from the given offset
            up to the given length. </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="buffer"> the input char array to reverse </param>
            <param name="start"> the offset from where to reverse the buffer </param>
            <param name="len"> the length in the buffer up to where the
                   buffer should be reversed </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Reverse.ReverseStringFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Reverse.ReverseStringFilter"/>.
            <code>
            &lt;fieldType name="text_rvsstr" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.ReverseStringFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            
            @since solr 1.4
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Reverse.ReverseStringFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Reverse.ReverseStringFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Ro.RomanianAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Romanian.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Ro.RomanianAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Romanian stopwords. </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Ro.RomanianAnalyzer.STOPWORDS_COMMENT">
            <summary>
            The comment character in the stopwords file.  
            All lines prefixed with this will be ignored.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Ro.RomanianAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Ro.RomanianAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Ro.RomanianAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ro.RomanianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.Ro.RomanianAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Ro.RomanianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Ro.RomanianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Ro.RomanianAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Ru.RussianAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Russian language. 
            <para>
            Supports an external list of stopwords (words that
            will not be indexed at all).
            A default set of stopwords is used unless an alternative list is specified.
            </para>
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.Ru.RussianAnalyzer"/>:
            <list type="bullet">
                <item><description> As of 3.1, <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> is used, Snowball stemming is done with
                   <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/>, and Snowball stopwords are used by default.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Ru.RussianAnalyzer.RUSSIAN_STOP_WORDS_30">
            <summary>
            List of typical Russian stopwords. (for backwards compatibility) </summary>
            @deprecated (3.1) Remove this for LUCENE 5.0 
        </member>
        <member name="F:Lucene.Net.Analysis.Ru.RussianAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Russian stopwords. </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Ru.RussianAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET_30">
            @deprecated (3.1) remove this for Lucene 5.0 
        </member>
        <member name="P:Lucene.Net.Analysis.Ru.RussianAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop-words set.
            </summary>
            <returns> an unmodifiable instance of the default stop-words set. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Ru.RussianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words
            </summary>
            <param name="matchVersion">
                     lucene compatibility version </param>
            <param name="stopwords">
                     a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Ru.RussianAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words
            </summary>
            <param name="matchVersion">
                     lucene compatibility version </param>
            <param name="stopwords">
                     a stopword set </param>
            <param name="stemExclusionSet"> a set of words not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Ru.RussianAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            used to tokenize all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from a <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>
                    , <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided, and <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/> </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Ru.RussianLetterTokenizer">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.Ru.RussianLetterTokenizer"/> is a <see cref="T:Lucene.Net.Analysis.Tokenizer"/> that extends <see cref="T:Lucene.Net.Analysis.Core.LetterTokenizer"/>
            by also allowing the basic Latin digits 0-9.
            <para>
            <a name="version"/>
            You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/> compatibility when creating
            <see cref="T:Lucene.Net.Analysis.Ru.RussianLetterTokenizer"/>:
            <ul>
            <li>As of 3.1, <see cref="T:Lucene.Net.Analysis.Util.CharTokenizer"/> uses an int based API to normalize and
            detect token characters. See <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.IsTokenChar(System.Int32)"/> and
            <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.Normalize(System.Int32)"/> for details.</li>
            </ul>
            </para>
            </summary>
            @deprecated (3.1) Use <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> instead, which has the same functionality.
            This filter will be removed in Lucene 5.0  
        </member>
        <member name="M:Lucene.Net.Analysis.Ru.RussianLetterTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader)">
            <summary>
            Construct a new <see cref="T:Lucene.Net.Analysis.Ru.RussianLetterTokenizer"/>.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="in">
                     the input to split up into tokens </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Ru.RussianLetterTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader)">
            <summary>
            Construct a new RussianLetterTokenizer using a given
            <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/>.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="factory">
                     the attribute factory to use for this <see cref="T:Lucene.Net.Analysis.Tokenizer"/> </param>
            <param name="in">
                     the input to split up into tokens </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Ru.RussianLetterTokenizer.IsTokenChar(System.Int32)">
            <summary>
            Collects only characters which satisfy
            <see cref="M:J2N.Character.IsLetter(System.Int32)"/>.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Ru.RussianLetterTokenizerFactory">
            @deprecated Use <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizerFactory"/> instead.
             This tokenizer has no Russian-specific functionality. 
        </member>
        <member name="M:Lucene.Net.Analysis.Ru.RussianLetterTokenizerFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Ru.RussianLetterTokenizerFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Ru.RussianLightStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Ru.RussianLightStemmer"/> to stem Russian
            words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Ru.RussianLightStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Ru.RussianLightStemFilter"/>.
            <code>
            &lt;fieldType name="text_rulgtstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.RussianLightStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Ru.RussianLightStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Ru.RussianLightStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Ru.RussianLightStemmer">
            <summary>
            Light Stemmer for Russian.
            <para>
            This stemmer implements the following algorithm:
            <c>Indexing and Searching Strategies for the Russian Language.</c>
            Ljiljana Dolamic and Jacques Savoy.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Shingle.ShingleAnalyzerWrapper">
            <summary>
            A ShingleAnalyzerWrapper wraps a <see cref="T:Lucene.Net.Analysis.Shingle.ShingleFilter"/> around another <see cref="T:Lucene.Net.Analysis.Analyzer"/>.
            <para>
            A shingle is another name for a token based n-gram.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleAnalyzerWrapper.#ctor(Lucene.Net.Analysis.Analyzer,System.Int32,System.Int32,System.String,System.Boolean,System.Boolean,System.String)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Shingle.ShingleAnalyzerWrapper"/>
            </summary>
            <param name="delegate"> <see cref="T:Lucene.Net.Analysis.Analyzer"/> whose <see cref="T:Lucene.Net.Analysis.TokenStream"/> is to be filtered </param>
            <param name="minShingleSize"> Min shingle (token ngram) size </param>
            <param name="maxShingleSize"> Max shingle size </param>
            <param name="tokenSeparator"> Used to separate input stream tokens in output shingles </param>
            <param name="outputUnigrams"> Whether or not the filter shall pass the original
                   tokens to the output stream </param>
            <param name="outputUnigramsIfNoShingles"> Overrides the behavior of outputUnigrams==false for those
                   times when no shingles are available (because there are fewer than
                   minShingleSize tokens in the input stream)?
                   Note that if outputUnigrams==true, then unigrams are always output,
                   regardless of whether any shingles are available. </param>
            <param name="fillerToken"> filler token to use when positionIncrement is more than 1 </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleAnalyzerWrapper.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Wraps <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/>. 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleAnalyzerWrapper.#ctor(Lucene.Net.Util.LuceneVersion,System.Int32,System.Int32)">
            <summary>
            Wraps <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/>. 
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Shingle.ShingleAnalyzerWrapper.MaxShingleSize">
            <summary>
            The max shingle (token ngram) size
            </summary>
            <returns> The max shingle (token ngram) size </returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Shingle.ShingleAnalyzerWrapper.MinShingleSize">
            <summary>
            The min shingle (token ngram) size
            </summary>
            <returns> The min shingle (token ngram) size </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Shingle.ShingleFilter">
            <summary>
            <para>A <see cref="T:Lucene.Net.Analysis.Shingle.ShingleFilter"/> constructs shingles (token n-grams) from a token stream.
            In other words, it creates combinations of tokens as a single token.
            
            </para>
            <para>For example, the sentence "please divide this sentence into shingles"
            might be tokenized into shingles "please divide", "divide this",
            "this sentence", "sentence into", and "into shingles".
            
            </para>
            <para>This filter handles position increments > 1 by inserting filler tokens
            (tokens with termtext "_"). It does not handle a position increment of 0.
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.DEFAULT_FILLER_TOKEN">
            <summary>
            filler token for when positionIncrement is more than 1
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.DEFAULT_MAX_SHINGLE_SIZE">
            <summary>
            default maximum shingle size is 2.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.DEFAULT_MIN_SHINGLE_SIZE">
            <summary>
            default minimum shingle size is 2.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.DEFAULT_TOKEN_TYPE">
            <summary>
            default token type attribute value is "shingle" 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.DEFAULT_TOKEN_SEPARATOR">
            <summary>
            The default string to use when joining adjacent tokens to form a shingle
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.inputWindow">
            <summary>
            The sequence of input stream tokens (or filler tokens, if necessary)
            that will be composed to form output shingles.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.gramSize">
            <summary>
            The number of input tokens in the next output token.  This is the "n" in
            "token n-grams".
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.gramBuilder">
            <summary>
            Shingle and unigram text is composed here.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.tokenType">
            <summary>
            The token type attribute value to use - default is "shingle"
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.tokenSeparator">
            <summary>
            The string to use when joining adjacent tokens to form a shingle
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.fillerToken">
            <summary>
            The string to insert for each position at which there is no token
            (i.e., when position increment is greater than one).
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.outputUnigrams">
            <summary>
            By default, we output unigrams (individual tokens) as well as shingles
            (token n-grams).
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.outputUnigramsIfNoShingles">
            <summary>
            By default, we don't override behavior of outputUnigrams.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.maxShingleSize">
            <summary>
            maximum shingle size (number of tokens)
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.minShingleSize">
            <summary>
            minimum shingle size (number of tokens)
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.numFillerTokensToInsert">
            <summary>
            The remaining number of filler tokens to be inserted into the input stream
            from which shingles are composed, to handle position increments greater
            than one.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.nextInputStreamToken">
            <summary>
            When the next input stream token has a position increment greater than
            one, it is stored in this field until sufficient filler tokens have been
            inserted to account for the position increment. 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.isNextInputStreamToken">
            <summary>
            Whether or not there is a next input stream token.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.isOutputHere">
            <summary>
            Whether at least one unigram or shingle has been output at the current 
            position.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.noShingleOutput">
            <summary>
            true if no shingles have been output yet (for outputUnigramsIfNoShingles).
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Shingle.ShingleFilter.endState">
            <summary>
            Holds the State after input.end() was called, so we can
            restore it in our end() impl.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Int32,System.Int32)">
            <summary>
            Constructs a <see cref="T:Lucene.Net.Analysis.Shingle.ShingleFilter"/> with the specified shingle size from the
            <see cref="T:Lucene.Net.Analysis.TokenStream"/> <paramref name="input"/>
            </summary>
            <param name="input"> input stream </param>
            <param name="minShingleSize"> minimum shingle size produced by the filter. </param>
            <param name="maxShingleSize"> maximum shingle size produced by the filter. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.Int32)">
            <summary>
            Constructs a <see cref="T:Lucene.Net.Analysis.Shingle.ShingleFilter"/> with the specified shingle size from the
            <see cref="T:Lucene.Net.Analysis.TokenStream"/> <paramref name="input"/>
            </summary>
            <param name="input"> input stream </param>
            <param name="maxShingleSize"> maximum shingle size produced by the filter. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Construct a <see cref="T:Lucene.Net.Analysis.Shingle.ShingleFilter"/> with default shingle size: 2.
            </summary>
            <param name="input"> input stream </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.String)">
            <summary>
            Construct a <see cref="T:Lucene.Net.Analysis.Shingle.ShingleFilter"/> with the specified token type for shingle tokens
            and the default shingle size: 2
            </summary>
            <param name="input"> input stream </param>
            <param name="tokenType"> token type for shingle tokens </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleFilter.SetTokenType(System.String)">
            <summary>
            Set the type of the shingle tokens produced by this filter.
            (default: "shingle")
            </summary>
            <param name="tokenType"> token tokenType </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleFilter.SetOutputUnigrams(System.Boolean)">
            <summary>
            Shall the output stream contain the input tokens (unigrams) as well as
            shingles? (default: true.)
            </summary>
            <param name="outputUnigrams"> Whether or not the output stream shall contain
            the input tokens (unigrams) </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleFilter.SetOutputUnigramsIfNoShingles(System.Boolean)">
            <summary>
            <para>Shall we override the behavior of outputUnigrams==false for those
            times when no shingles are available (because there are fewer than
            minShingleSize tokens in the input stream)? (default: false.)
            </para>
            <para>Note that if outputUnigrams==true, then unigrams are always output,
            regardless of whether any shingles are available.
            
            </para>
            </summary>
            <param name="outputUnigramsIfNoShingles"> Whether or not to output a single
            unigram when no shingles are available. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleFilter.SetMaxShingleSize(System.Int32)">
            <summary>
            Set the max shingle size (default: 2)
            </summary>
            <param name="maxShingleSize"> max size of output shingles </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleFilter.SetMinShingleSize(System.Int32)">
            <summary>
            <para>Set the min shingle size (default: 2).
            </para>
            <para>This method requires that the passed in minShingleSize is not greater
            than maxShingleSize, so make sure that maxShingleSize is set before
            calling this method.
            </para>
            <para>The unigram output option is independent of the min shingle size.
            
            </para>
            </summary>
            <param name="minShingleSize"> min size of output shingles </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleFilter.SetTokenSeparator(System.String)">
            <summary>
            Sets the string to use when joining adjacent tokens to form a shingle </summary>
            <param name="tokenSeparator"> used to separate input stream tokens in output shingles </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleFilter.SetFillerToken(System.String)">
            <summary>
            Sets the string to insert for each position at which there is no token
            (i.e., when position increment is greater than one).
            </summary>
            <param name="fillerToken"> string to insert at each position where there is no token </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleFilter.GetNextToken(Lucene.Net.Analysis.Shingle.ShingleFilter.InputWindowToken)">
            <summary>
            <para>Get the next token from the input stream.
            </para>
            <para>If the next token has <c>positionIncrement > 1</c>,
            <c>positionIncrement - 1</c> <see cref="F:Lucene.Net.Analysis.Shingle.ShingleFilter.fillerToken"/>s are
            inserted first.
            </para>
            </summary>
            <param name="target"> Where to put the new token; if null, a new instance is created. </param>
            <returns> On success, the populated token; null otherwise </returns>
            <exception cref="T:System.IO.IOException"> if the input stream has a problem </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleFilter.ShiftInputWindow">
            <summary>
            <para>Fills <see cref="F:Lucene.Net.Analysis.Shingle.ShingleFilter.inputWindow"/> with input stream tokens, if available, 
            shifting to the right if the window was previously full.
            </para>
            <para>
            Resets <see cref="F:Lucene.Net.Analysis.Shingle.ShingleFilter.gramSize"/> to its minimum value.
            </para>
            </summary>
            <exception cref="T:System.IO.IOException"> if there's a problem getting the next token </exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Shingle.ShingleFilter.CircularSequence">
            <summary>
            <para>An instance of this class is used to maintain the number of input
            stream tokens that will be used to compose the next unigram or shingle:
            <see cref="F:Lucene.Net.Analysis.Shingle.ShingleFilter.gramSize"/>.
            </para>
            <para><code>gramSize</code> will take on values from the circular sequence
            <b>{ [ 1, ] <see cref="F:Lucene.Net.Analysis.Shingle.ShingleFilter.minShingleSize"/> [ , ... , <see cref="F:Lucene.Net.Analysis.Shingle.ShingleFilter.maxShingleSize"/> ] }</b>.
            </para>
            <para>1 is included in the circular sequence only if 
            <see cref="F:Lucene.Net.Analysis.Shingle.ShingleFilter.outputUnigrams"/> = true.
            </para>
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Shingle.ShingleFilter.CircularSequence.Value">
            <returns> the current value. </returns>
            <seealso cref="M:Lucene.Net.Analysis.Shingle.ShingleFilter.CircularSequence.Advance"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleFilter.CircularSequence.Advance">
            <summary>
            <para>Increments this circular number's value to the next member in the
            circular sequence
            <code>gramSize</code> will take on values from the circular sequence
            <b>{ [ 1, ] <see cref="F:Lucene.Net.Analysis.Shingle.ShingleFilter.minShingleSize"/> [ , ... , <see cref="F:Lucene.Net.Analysis.Shingle.ShingleFilter.maxShingleSize"/> ] }</b>.
            </para>
            <para>1 is included in the circular sequence only if 
            <see cref="F:Lucene.Net.Analysis.Shingle.ShingleFilter.outputUnigrams"/> = true.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleFilter.CircularSequence.Reset">
            <summary>
            <para>Sets this circular number's value to the first member of the 
            circular sequence
            </para>
            <para><code>gramSize</code> will take on values from the circular sequence
            <b>{ [ 1, ] <see cref="F:Lucene.Net.Analysis.Shingle.ShingleFilter.minShingleSize"/> [ , ... , <see cref="F:Lucene.Net.Analysis.Shingle.ShingleFilter.maxShingleSize"/> ] }</b>.
            </para>
            <para>1 is included in the circular sequence only if 
            <see cref="F:Lucene.Net.Analysis.Shingle.ShingleFilter.outputUnigrams"/> = true.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleFilter.CircularSequence.AtMinValue">
            <summary>
            <para>Returns true if the current value is the first member of the circular
            sequence.
            </para>
            <para>If <see cref="F:Lucene.Net.Analysis.Shingle.ShingleFilter.outputUnigrams"/> = true, the first member of the circular
            sequence will be 1; otherwise, it will be <see cref="F:Lucene.Net.Analysis.Shingle.ShingleFilter.minShingleSize"/>.
            
            </para>
            </summary>
            <returns> true if the current value is the first member of the circular
             sequence; false otherwise </returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Shingle.ShingleFilter.CircularSequence.PreviousValue">
            <returns> the value this instance had before the last <see cref="M:Lucene.Net.Analysis.Shingle.ShingleFilter.CircularSequence.Advance"/> call </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Shingle.ShingleFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Shingle.ShingleFilter"/>.
            <code>
            &lt;fieldType name="text_shingle" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.ShingleFilterFactory" minShingleSize="2" maxShingleSize="2"
                        outputUnigrams="true" outputUnigramsIfNoShingles="false" tokenSeparator=" " fillerToken="_"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Shingle.ShingleFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Shingle.ShingleFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter">
            <summary>
            Attempts to parse the <see cref="M:Lucene.Net.Analysis.TokenAttributes.CharTermAttribute.ToString"/> as a Date using either the 
            <see cref="M:System.DateTime.TryParse(System.String,System.IFormatProvider,System.Globalization.DateTimeStyles,System.DateTime@)"/> or 
            <see cref="M:System.DateTime.TryParseExact(System.String,System.String[],System.IFormatProvider,System.Globalization.DateTimeStyles,System.DateTime@)"/> methods.
            If a format is passed, <see cref="M:System.DateTime.TryParseExact(System.String,System.String[],System.IFormatProvider,System.Globalization.DateTimeStyles,System.DateTime@)"/> 
            will be used, and the format must strictly match one of the specified formats as specified in the MSDN documentation.
            If the value is a Date, it will add it to the sink.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter.#ctor">
            <summary>
            Creates a new instance of <see cref="T:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter"/> using the current culture and <see cref="F:System.Globalization.DateTimeStyles.None"/>.
            Loosely matches standard DateTime formats using <see cref="M:System.DateTime.TryParse(System.String,System.IFormatProvider,System.Globalization.DateTimeStyles,System.DateTime@)"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter.#ctor(System.IFormatProvider)">
            <summary>
            Creates a new instance of <see cref="T:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter"/> using the supplied culture and <see cref="F:System.Globalization.DateTimeStyles.None"/>.
            Loosely matches standard DateTime formats using <see cref="M:System.DateTime.TryParse(System.String,System.IFormatProvider,System.Globalization.DateTimeStyles,System.DateTime@)"/>.
            </summary>
            <param name="culture">An object that supplies culture-specific format information</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter.#ctor(System.String)">
            <summary>
            Creates a new instance of <see cref="T:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter"/> using the current culture and <see cref="F:System.Globalization.DateTimeStyles.None"/>.
            Strictly matches the supplied DateTime formats using <see cref="M:System.DateTime.TryParseExact(System.String,System.String,System.IFormatProvider,System.Globalization.DateTimeStyles,System.DateTime@)"/>.
            </summary>
            <param name="format">The allowable format of the <see cref="M:Lucene.Net.Analysis.TokenAttributes.CharTermAttribute.ToString"/>.
            If supplied, it must match the format of the date exactly to get a match.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter.#ctor(System.String[])">
            <summary>
            Creates a new instance of <see cref="T:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter"/>  using the current culture and <see cref="F:System.Globalization.DateTimeStyles.None"/>.
            Strictly matches the supplied DateTime formats using <see cref="M:System.DateTime.TryParseExact(System.String,System.String[],System.IFormatProvider,System.Globalization.DateTimeStyles,System.DateTime@)"/>.
            </summary>
            <param name="formats">An array of allowable formats of the <see cref="M:Lucene.Net.Analysis.TokenAttributes.CharTermAttribute.ToString"/>.
            If supplied, one of them must match the format of the date exactly to get a match.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter.#ctor(System.IFormatProvider,System.Globalization.DateTimeStyles)">
            <summary>
            Creates a new instance of <see cref="T:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter"/>  using the supplied culture and <see cref="T:System.Globalization.DateTimeStyles"/>.
            Loosely matches standard DateTime formats using <see cref="M:System.DateTime.TryParse(System.String,System.IFormatProvider,System.Globalization.DateTimeStyles,System.DateTime@)"/>.
            </summary>
            <param name="culture">An object that supplies culture-specific format information</param>
            <param name="style">A bitwise combination of enumeration values that indicates the permitted format of s. 
            A typical value to specify is <see cref="F:System.Globalization.DateTimeStyles.None"/></param>
        </member>
        <member name="M:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter.#ctor(System.String,System.IFormatProvider)">
            <summary>
            Creates a new instance of <see cref="T:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter"/>  using the supplied format, culture and <see cref="F:System.Globalization.DateTimeStyles.None"/>.
            Strictly matches the supplied DateTime formats using <see cref="M:System.DateTime.TryParseExact(System.String,System.String,System.IFormatProvider,System.Globalization.DateTimeStyles,System.DateTime@)"/>.
            </summary>
            <param name="format">The allowable format of the <see cref="M:Lucene.Net.Analysis.TokenAttributes.CharTermAttribute.ToString"/>.
            If supplied, it must match the format of the date exactly to get a match.</param>
            <param name="culture">An object that supplies culture-specific format information</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter.#ctor(System.String[],System.IFormatProvider)">
            <summary>
            Creates a new instance of <see cref="T:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter"/>  using the supplied formats, culture and <see cref="F:System.Globalization.DateTimeStyles.None"/>.
            Strictly matches the supplied DateTime formats using <see cref="M:System.DateTime.TryParseExact(System.String,System.String[],System.IFormatProvider,System.Globalization.DateTimeStyles,System.DateTime@)"/>.
            </summary>
            <param name="formats">An array of allowable formats of the <see cref="M:Lucene.Net.Analysis.TokenAttributes.CharTermAttribute.ToString"/>.
            If supplied, one of them must match the format of the date exactly to get a match.</param>
            <param name="culture">An object that supplies culture-specific format information</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter.#ctor(System.String,System.IFormatProvider,System.Globalization.DateTimeStyles)">
            <summary>
            Creates a new instance of <see cref="T:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter"/> using the supplied format, culture and <see cref="T:System.Globalization.DateTimeStyles"/>.
            Strictly matches the supplied DateTime formats using <see cref="M:System.DateTime.TryParseExact(System.String,System.String,System.IFormatProvider,System.Globalization.DateTimeStyles,System.DateTime@)"/>.
            </summary>
            <param name="format">The allowable format of the <see cref="M:Lucene.Net.Analysis.TokenAttributes.CharTermAttribute.ToString"/>.
            If supplied, it must match the format of the date exactly to get a match.</param>
            <param name="culture">An object that supplies culture-specific format information</param>
            <param name="style">A bitwise combination of enumeration values that indicates the permitted format of s. 
            A typical value to specify is <see cref="F:System.Globalization.DateTimeStyles.None"/></param>
        </member>
        <member name="M:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter.#ctor(System.String[],System.IFormatProvider,System.Globalization.DateTimeStyles)">
            <summary>
            Creates a new instance of <see cref="T:Lucene.Net.Analysis.Sinks.DateRecognizerSinkFilter"/> using the supplied formats, culture and <see cref="T:System.Globalization.DateTimeStyles"/>.
            Strictly matches the supplied DateTime formats using <see cref="M:System.DateTime.TryParseExact(System.String,System.String[],System.IFormatProvider,System.Globalization.DateTimeStyles,System.DateTime@)"/>.
            </summary>
            <param name="formats">An array of allowable formats of the <see cref="M:Lucene.Net.Analysis.TokenAttributes.CharTermAttribute.ToString"/>.
            If supplied, one of them must match the format of the date exactly to get a match.</param>
            <param name="culture">An object that supplies culture-specific format information</param>
            <param name="style">A bitwise combination of enumeration values that indicates the permitted format of s. 
            A typical value to specify is <see cref="F:System.Globalization.DateTimeStyles.None"/></param>
        </member>
        <member name="T:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter">
            <summary>
            This TokenFilter provides the ability to set aside attribute states
            that have already been analyzed.  This is useful in situations where multiple fields share
            many common analysis steps and then go their separate ways.
            <para/>
            It is also useful for doing things like entity extraction or proper noun analysis as
            part of the analysis workflow and saving off those tokens for use in another field.
            <para/>
            <code>
            TeeSinkTokenFilter source1 = new TeeSinkTokenFilter(new WhitespaceTokenizer(version, reader1));
            TeeSinkTokenFilter.SinkTokenStream sink1 = source1.NewSinkTokenStream();
            TeeSinkTokenFilter.SinkTokenStream sink2 = source1.NewSinkTokenStream();
            
            TeeSinkTokenFilter source2 = new TeeSinkTokenFilter(new WhitespaceTokenizer(version, reader2));
            source2.AddSinkTokenStream(sink1);
            source2.AddSinkTokenStream(sink2);
            
            TokenStream final1 = new LowerCaseFilter(version, source1);
            TokenStream final2 = source2;
            TokenStream final3 = new EntityDetect(sink1);
            TokenStream final4 = new URLDetect(sink2);
            
            d.Add(new TextField("f1", final1, Field.Store.NO));
            d.Add(new TextField("f2", final2, Field.Store.NO));
            d.Add(new TextField("f3", final3, Field.Store.NO));
            d.Add(new TextField("f4", final4, Field.Store.NO));
            </code>
            In this example, <c>sink1</c> and <c>sink2</c> will both get tokens from both
            <c>reader1</c> and <c>reader2</c> after whitespace tokenizer
            and now we can further wrap any of these in extra analysis, and more "sources" can be inserted if desired.
            It is important, that tees are consumed before sinks (in the above example, the field names must be
            less the sink's field names). If you are not sure, which stream is consumed first, you can simply
            add another sink and then pass all tokens to the sinks at once using <see cref="M:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter.ConsumeAllTokens"/>.
            This <see cref="T:Lucene.Net.Analysis.TokenFilter"/> is exhausted after this. In the above example, change
            the example above to:
            <code>
            ...
            TokenStream final1 = new LowerCaseFilter(version, source1.NewSinkTokenStream());
            TokenStream final2 = source2.NewSinkTokenStream();
            sink1.ConsumeAllTokens();
            sink2.ConsumeAllTokens();
            ...
            </code>
            In this case, the fields can be added in any order, because the sources are not used anymore and all sinks are ready.
            <para>Note, the EntityDetect and URLDetect TokenStreams are for the example and do not currently exist in Lucene.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Instantiates a new <see cref="T:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter.NewSinkTokenStream">
            <summary>
            Returns a new <see cref="T:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter.SinkTokenStream"/> that receives all tokens consumed by this stream.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter.NewSinkTokenStream(Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter.SinkFilter)">
            <summary>
            Returns a new <see cref="T:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter.SinkTokenStream"/> that receives all tokens consumed by this stream
            that pass the supplied filter. </summary>
            <seealso cref="T:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter.SinkFilter"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter.AddSinkTokenStream(Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter.SinkTokenStream)">
            <summary>
            Adds a <see cref="T:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter.SinkTokenStream"/> created by another <see cref="T:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter"/>
            to this one. The supplied stream will also receive all consumed tokens.
            This method can be used to pass tokens from two different tees to one sink.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter.ConsumeAllTokens">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter"/> passes all tokens to the added sinks
            when itself is consumed. To be sure, that all tokens from the input
            stream are passed to the sinks, you can call this methods.
            This instance is exhausted after this, but all sinks are instant available.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter.SinkFilter">
            <summary>
            A filter that decides which <see cref="T:Lucene.Net.Util.AttributeSource"/> states to store in the sink.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter.SinkFilter.Accept(Lucene.Net.Util.AttributeSource)">
            <summary>
            Returns true, iff the current state of the passed-in <see cref="T:Lucene.Net.Util.AttributeSource"/> shall be stored
            in the sink. 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter.SinkFilter.Reset">
            <summary>
            Called by <see cref="M:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter.SinkTokenStream.Reset"/>. This method does nothing by default
            and can optionally be overridden.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Sinks.TeeSinkTokenFilter.SinkTokenStream">
            <summary>
            <see cref="T:Lucene.Net.Analysis.TokenStream"/> output from a tee with optional filtering.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Sinks.TokenRangeSinkFilter">
            <summary>
            Counts the tokens as they go by and saves to the internal list those between the range of lower and upper, exclusive of upper
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Sinks.TokenTypeSinkFilter">
            <summary>
            Adds a token to the sink if it has a specific type.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Snowball.SnowballAnalyzer">
            <summary>
            Filters <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> with <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, 
            <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/> and <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/>.
            
            Available stemmers are listed in org.tartarus.snowball.ext.  The name of a
            stemmer is the part of the class name before "Stemmer", e.g., the stemmer in
            <see cref="T:Lucene.Net.Tartarus.Snowball.Ext.EnglishStemmer"/> is named "English".
            
            <para><b>NOTE</b>: This class uses the same <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            dependent settings as <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/>, with the following addition:
            <list type="bullet">
              <item><description> As of 3.1, uses <see cref="T:Lucene.Net.Analysis.Tr.TurkishLowerCaseFilter"/> for Turkish language.</description></item>
            </list>
            </para> </summary>
            @deprecated (3.1) Use the language-specific analyzer in modules/analysis instead. 
            This analyzer will be removed in Lucene 5.0 
        </member>
        <member name="M:Lucene.Net.Analysis.Snowball.SnowballAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,System.String)">
            <summary>
            Builds the named analyzer with no stop words. </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Snowball.SnowballAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,System.String,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds the named analyzer with the given stop words. </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Snowball.SnowballAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Constructs a <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered by a 
               <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, a <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, a <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
               and a <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/> 
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Snowball.SnowballFilter">
            <summary>
            A filter that stems words using a Snowball-generated stemmer.
            
            Available stemmers are listed in Lucene.Net.Tartarus.Snowball.Ext.
            <para><b>NOTE</b>: <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/> expects lowercased text.
            <list type="bullet">
                <item><description>For the Turkish language, see <see cref="T:Lucene.Net.Analysis.Tr.TurkishLowerCaseFilter"/>.</description></item>
                <item><description>For other languages, see <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>.</description></item>
            </list>
            </para>
            
            <para>
            Note: This filter is aware of the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/>. To prevent
            certain terms from being passed to the stemmer
            <see cref="P:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute.IsKeyword"/> should be set to <c>true</c>
            in a previous <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            
            Note: For including the original term as well as the stemmed version, see
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.KeywordRepeatFilterFactory"/>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Snowball.SnowballFilter.#ctor(Lucene.Net.Analysis.TokenStream,System.String)">
            <summary>
            Construct the named stemming filter.
            
            Available stemmers are listed in Lucene.Net.Tartarus.Snowball.Ext.
            The name of a stemmer is the part of the class name before "Stemmer",
            e.g., the stemmer in <see cref="T:Lucene.Net.Tartarus.Snowball.Ext.EnglishStemmer"/> is named "English".
            </summary>
            <param name="in"> the input tokens to stem </param>
            <param name="name"> the name of a stemmer </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Snowball.SnowballFilter.IncrementToken">
            <summary>
            Returns the next input <see cref="T:Lucene.Net.Analysis.Token"/>, after being stemmed </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Snowball.SnowballPorterFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/>, with configurable language
            <para>
            Note: Use of the "Lovins" stemmer is not recommended, as it is implemented with reflection.
            <code>
            &lt;fieldType name="text_snowballstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.SnowballPorterFilterFactory" protected="protectedkeyword.txt" language="English"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Snowball.SnowballPorterFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Snowball.SnowballPorterFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.ClassicAnalyzer">
            <summary>
            Filters <see cref="T:Lucene.Net.Analysis.Standard.ClassicTokenizer"/> with <see cref="T:Lucene.Net.Analysis.Standard.ClassicFilter"/>, 
            <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/> and <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>, using a list of
            English stop words.
            
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.Standard.ClassicAnalyzer"/>:
            <list type="bullet">
                <item><description> As of 3.1, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/> correctly handles Unicode 4.0
                    supplementary characters in stopwords</description></item>
                <item><description> As of 2.9, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/> preserves position
                   increments</description></item>
                <item><description> As of 2.4, <see cref="T:Lucene.Net.Analysis.Token"/>s incorrectly identified as acronyms
                   are corrected (see <a href="https://issues.apache.org/jira/browse/LUCENE-1068">LUCENE-1068</a>)</description></item>
            </list>
            
            <see cref="T:Lucene.Net.Analysis.Standard.ClassicAnalyzer"/> was named <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/> in Lucene versions prior to 3.1. 
            As of 3.1, <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/> implements Unicode text segmentation,
            as specified by UAX#29.
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicAnalyzer.DEFAULT_MAX_TOKEN_LENGTH">
            <summary>
            Default maximum allowed token length </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicAnalyzer.STOP_WORDS_SET">
            <summary>
            An unmodifiable set containing some common English words that are usually not
            useful for searching. 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. </summary>
            <param name="matchVersion"> Lucene compatibility version - See <see cref="T:Lucene.Net.Analysis.Standard.ClassicAnalyzer"/> </param>
            <param name="stopWords"> stop words  </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words (<see cref="F:Lucene.Net.Analysis.Standard.ClassicAnalyzer.STOP_WORDS_SET"/>).
            </summary>
            <param name="matchVersion"> Lucene compatibility version - See <see cref="T:Lucene.Net.Analysis.Standard.ClassicAnalyzer"/> </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader)">
            <summary>
            Builds an analyzer with the stop words from the given reader. </summary>
            <seealso cref="M:Lucene.Net.Analysis.Util.WordlistLoader.GetWordSet(System.IO.TextReader,Lucene.Net.Util.LuceneVersion)"/>
            <param name="matchVersion"> Lucene compatibility version - See <see cref="T:Lucene.Net.Analysis.Standard.ClassicAnalyzer"/> </param>
            <param name="stopwords"> <see cref="T:System.IO.TextReader"/> to read stop words from  </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.ClassicAnalyzer.MaxTokenLength">
            <summary>
            Gets or sets maximum allowed token length.  If a token is seen
            that exceeds this length then it is discarded.  This
            setting only takes effect the next time tokenStream or
            tokenStream is called.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.ClassicFilter">
            <summary>
            Normalizes tokens extracted with <see cref="T:Lucene.Net.Analysis.Standard.ClassicTokenizer"/>. </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Construct filtering <paramref name="in"/>. </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicFilter.IncrementToken">
            <summary>
            Returns the next token in the stream, or null at EOS.
            <para>Removes <c>'s</c> from the end of words.
            </para>
            <para>Removes dots from acronyms.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.ClassicFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Standard.ClassicFilter"/>.
            <code>
            &lt;fieldType name="text_clssc" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.ClassicTokenizerFactory"/&gt;
                &lt;filter class="solr.ClassicFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Standard.ClassicFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.ClassicTokenizer">
            <summary>
            A grammar-based tokenizer constructed with JFlex (and then ported to .NET)
            
            <para> This should be a good tokenizer for most European-language documents:
            
            <list type="bullet">
                <item><description>Splits words at punctuation characters, removing punctuation. However, a 
                    dot that's not followed by whitespace is considered part of a token.</description></item>
                <item><description>Splits words at hyphens, unless there's a number in the token, in which case
                    the whole token is interpreted as a product number and is not split.</description></item>
                <item><description>Recognizes email addresses and internet hostnames as one token.</description></item>
            </list>
            
            </para>
            <para>Many applications have specific tokenizer needs.  If this tokenizer does
            not suit your application, please consider copying this source code
            directory to your project and maintaining your own grammar-based tokenizer.
            
            <see cref="T:Lucene.Net.Analysis.Standard.ClassicTokenizer"/> was named <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> in Lucene versions prior to 3.1.
            As of 3.1, <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> implements Unicode text segmentation,
            as specified by UAX#29.
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizer.scanner">
            <summary>
            A private instance of the JFlex-constructed scanner </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizer.TOKEN_TYPES">
            <summary>
            String token types that correspond to token type int constants </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.ClassicTokenizer.MaxTokenLength">
            <summary>
            Set the max allowed token length.  Any token longer
             than this is skipped. 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader)">
            <summary>
            Creates a new instance of the <see cref="T:Lucene.Net.Analysis.Standard.ClassicTokenizer"/>.  Attaches
            the <paramref name="input"/> to the newly created JFlex scanner.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="input"> The input reader
            
            See http://issues.apache.org/jira/browse/LUCENE-1068 </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Standard.ClassicTokenizer"/> with a given <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/> 
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.ClassicTokenizerFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Standard.ClassicTokenizer"/>.
            <code>
            &lt;fieldType name="text_clssc" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.ClassicTokenizerFactory" maxTokenLength="120"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicTokenizerFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Standard.ClassicTokenizerFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl">
            <summary>
            This class implements the classic lucene <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> up until 3.0 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.YYEOF">
            <summary>
            This character denotes the end of file </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.ZZ_BUFFERSIZE">
            <summary>
            initial size of the lookahead buffer </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.YYINITIAL">
            <summary>
            lexical states </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.ZZ_LEXSTATE">
            <summary>
            ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
            ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
                             at the beginning of a line
            l is of the form l = 2*k, k a non negative integer
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.ZZ_CMAP_PACKED">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.ZZ_CMAP">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.ZZ_ACTION">
            <summary>
            Translates DFA states to action switch labels.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.ZZ_ROWMAP">
            <summary>
            Translates a state to a row index in the transition table
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.ZZ_TRANS">
            <summary>
            The transition table of the DFA
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.ZZ_ATTRIBUTE">
            <summary>
            ZZ_ATTRIBUTE[aState] contains the attributes of state <c>aState</c>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.zzReader">
            <summary>
            the input device </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.zzState">
            <summary>
            the current state of the DFA </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.zzLexicalState">
            <summary>
            the current lexical state </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.zzBuffer">
            <summary>
            this buffer contains the current text to be matched and is
               the source of the YyText string 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.zzMarkedPos">
            <summary>
            the textposition at the last accepting state </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.zzCurrentPos">
            <summary>
            the current text position in the buffer </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.zzStartRead">
            <summary>
            startRead marks the beginning of the YyText string in the buffer </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.zzEndRead">
            <summary>
            endRead marks the last character in the buffer, that has been read
               from input 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.yyline">
            <summary>
            number of newlines encountered up to the start of the matched text </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.yyChar">
            <summary>
            the number of characters up to the start of the matched text </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.yycolumn">
            <summary>
            the number of characters from the last newline up to the start of the 
            matched text
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.zzAtBOL">
            <summary>
            zzAtBOL == true &lt;=&gt; the scanner is currently at the beginning of a line
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.zzAtEOF">
            <summary>
            zzAtEOF == true &lt;=&gt; the scanner is at the EOF </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.zzEOFDone">
            <summary>
            denotes if the user-EOF-code has already been executed </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.GetText(Lucene.Net.Analysis.TokenAttributes.ICharTermAttribute)">
            <summary>
            Fills ICharTermAttribute with the current token text.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.#ctor(System.IO.TextReader)">
            <summary>
            Creates a new scanner
            </summary>
            <param name="in">  the TextReader to read input from. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.ZzUnpackCMap(System.String)">
            <summary>
            Unpacks the compressed character translation table.
            </summary>
            <param name="packed">   the packed character translation table </param>
            <returns>         the unpacked character translation table </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.ZzRefill">
            <summary>
            Refills the input buffer.
            </summary>
            <returns>      <c>false</c>, iff there was new input.
            </returns>
            <exception cref="T:System.IO.IOException">  if any I/O-Error occurs </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.YyClose">
            <summary>
            Disposes the input stream.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.YyReset(System.IO.TextReader)">
            <summary>
            Resets the scanner to read from a new input stream.
            Does not close the old reader.
            <para/>
            All internal variables are reset, the old input stream 
            <b>cannot</b> be reused (internal buffer is discarded and lost).
            Lexical state is set to <see cref="F:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.YYINITIAL"/>.
            <para/>
            Internal scan buffer is resized down to its initial length, if it has grown.
            </summary>
            <param name="reader">   the new input stream  </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.YyState">
            <summary>
            Returns the current lexical state.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.YyBegin(System.Int32)">
            <summary>
            Enters a new lexical state
            </summary>
            <param name="newState"> the new lexical state </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.YyText">
            <summary>
            Returns the text matched by the current regular expression.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.YyCharAt(System.Int32)">
            <summary>
            Returns the character at position <tt>pos</tt> from the 
            matched text. 
            
            It is equivalent to YyText[pos], but faster
            </summary>
            <param name="pos"> the position of the character to fetch. 
                       A value from 0 to YyLength-1.
            </param>
            <returns> the character at position pos </returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.YyLength">
            <summary>
            Returns the length of the matched text region.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.ZzScanError(System.Int32)">
            <summary>
            Reports an error that occured while scanning.
            <para/>
            In a wellformed scanner (no or only correct usage of 
            YyPushBack(int) and a match-all fallback rule) this method 
            will only be called with things that "Can't Possibly Happen".
            If this method is called, something is seriously wrong
            (e.g. a JFlex bug producing a faulty scanner etc.).
            <para/>
            Usual syntax/scanner level error handling should be done
            in error fallback rules.
            </summary>
            <param name="errorCode">  the code of the errormessage to display </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.YyPushBack(System.Int32)">
            <summary>
            Pushes the specified amount of characters back into the input stream.
            
            They will be read again by then next call of the scanning method
            </summary>
            <param name="number">  the number of characters to be read again.
                           This number must not be greater than YyLength! </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.ClassicTokenizerImpl.GetNextToken">
            <summary>
            Resumes scanning until the next regular expression is matched,
            the end of input is encountered or an I/O-Error occurs.
            </summary>
            <returns>      the next token </returns>
            <exception cref="T:System.IO.IOException">  if any I/O-Error occurs </exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.StandardAnalyzer">
            <summary>
            Filters <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> with <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, 
            <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/> and <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>, using a list of
            English stop words.
            
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/>:
            <list type="bullet">
              <item><description> As of 3.4, Hiragana and Han characters are no longer wrongly split
                   from their combining characters. If you use a previous version number,
                   you get the exact broken behavior for backwards compatibility.</description></item>
              <item><description> As of 3.1, <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> implements Unicode text segmentation,
                   and <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/> correctly handles Unicode 4.0 supplementary characters
                   in stopwords.  <see cref="T:Lucene.Net.Analysis.Standard.ClassicTokenizer"/> and <see cref="T:Lucene.Net.Analysis.Standard.ClassicAnalyzer"/> 
                   are the pre-3.1 implementations of <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> and
                   <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/>.</description></item>
              <item><description> As of 2.9, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/> preserves position increments</description></item>
              <item><description> As of 2.4, <see cref="T:Lucene.Net.Analysis.Token"/>s incorrectly identified as acronyms
                   are corrected (see <a href="https://issues.apache.org/jira/browse/LUCENE-1068">LUCENE-1068</a>)</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardAnalyzer.DEFAULT_MAX_TOKEN_LENGTH">
            <summary>
            Default maximum allowed token length </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardAnalyzer.STOP_WORDS_SET">
            <summary>
            An unmodifiable set containing some common English words that are usually not
            useful for searching. 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. </summary>
            <param name="matchVersion"> Lucene compatibility version - See <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/> </param>
            <param name="stopWords"> stop words  </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words (<see cref="F:Lucene.Net.Analysis.Standard.StandardAnalyzer.STOP_WORDS_SET"/>). </summary>
            <param name="matchVersion"> Lucene compatibility version - See <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/> </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader)">
            <summary>
            Builds an analyzer with the stop words from the given reader. </summary>
            <seealso cref="M:Lucene.Net.Analysis.Util.WordlistLoader.GetWordSet(System.IO.TextReader,Lucene.Net.Util.LuceneVersion)"/>
            <param name="matchVersion"> Lucene compatibility version - See <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/> </param>
            <param name="stopwords"> <see cref="T:System.IO.TextReader"/> to read stop words from  </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.StandardAnalyzer.MaxTokenLength">
            <summary>
            Set maximum allowed token length.  If a token is seen
            that exceeds this length then it is discarded.  This
            setting only takes effect the next time tokenStream or
            tokenStream is called.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.StandardFilter">
            <summary>
            Normalizes tokens extracted with <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/>.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.StandardFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>.
            <code>
            &lt;fieldType name="text_stndrd" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.StandardFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Standard.StandardFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.StandardTokenizer">
            <summary>
            A grammar-based tokenizer constructed with JFlex.
            <para>
            As of Lucene version 3.1, this class implements the Word Break rules from the
            Unicode Text Segmentation algorithm, as specified in 
            <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>.
            <p/>
            </para>
            <para>Many applications have specific tokenizer needs.  If this tokenizer does
            not suit your application, please consider copying this source code
            directory to your project and maintaining your own grammar-based tokenizer.
            
            </para>
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/>:
            <list type="bullet">
                <item><description> As of 3.4, Hiragana and Han characters are no longer wrongly split
                    from their combining characters. If you use a previous version number,
                    you get the exact broken behavior for backwards compatibility.</description></item>
                <item><description> As of 3.1, StandardTokenizer implements Unicode text segmentation.
                    If you use a previous version number, you get the exact behavior of
                    <see cref="T:Lucene.Net.Analysis.Standard.ClassicTokenizer"/> for backwards compatibility.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizer.scanner">
            <summary>
            A private instance of the JFlex-constructed scanner </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizer.APOSTROPHE">
            @deprecated (3.1) 
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizer.ACRONYM">
            @deprecated (3.1) 
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizer.COMPANY">
            @deprecated (3.1) 
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizer.HOST">
            @deprecated (3.1) 
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizer.CJ">
            @deprecated (3.1) 
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizer.ACRONYM_DEP">
            @deprecated (3.1) 
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizer.TOKEN_TYPES">
            <summary>
            String token types that correspond to token type int constants </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.StandardTokenizer.MaxTokenLength">
            <summary>
            Set the max allowed token length.  Any token longer
             than this is skipped. 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader)">
            <summary>
            Creates a new instance of the <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/>.  Attaches
            the <paramref name="input"/> to the newly created JFlex-generated (then ported to .NET) scanner.
            </summary>
            <param name="matchVersion"> Lucene compatibility version - See <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> </param>
            <param name="input"> The input reader
            
            See http://issues.apache.org/jira/browse/LUCENE-1068 </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> with a given <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/> 
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.StandardTokenizerFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/>. 
            <code>
            &lt;fieldType name="text_stndrd" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory" maxTokenLength="255"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code> 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardTokenizerFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizerFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.StandardTokenizerImpl">
            <summary>
            This class implements Word Break rules from the Unicode Text Segmentation 
            algorithm, as specified in 
            <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>. 
            <para/>
            Tokens produced are of the following types:
            <list type="bullet">
                <item><description>&lt;ALPHANUM&gt;: A sequence of alphabetic and numeric characters</description></item>
                <item><description>&lt;NUM&gt;: A number</description></item>
                <item><description>&lt;SOUTHEAST_ASIAN&gt;: A sequence of characters from South and Southeast
                    Asian languages, including Thai, Lao, Myanmar, and Khmer</description></item>
                <item><description>&lt;IDEOGRAPHIC&gt;: A single CJKV ideographic character</description></item>
                <item><description>&lt;HIRAGANA&gt;: A single hiragana character</description></item>
                <item><description>&lt;KATAKANA&gt;: A sequence of katakana characters</description></item>
                <item><description>&lt;HANGUL&gt;: A sequence of Hangul characters</description></item>
            </list>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.YYEOF">
            <summary>
            This character denotes the end of file </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.ZZ_BUFFERSIZE">
            <summary>
            initial size of the lookahead buffer </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.YYINITIAL">
            <summary>
            lexical states </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.ZZ_LEXSTATE">
            <summary>
            ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
            ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
                             at the beginning of a line
            l is of the form l = 2*k, k a non negative integer
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.ZZ_CMAP_PACKED">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.ZZ_CMAP">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.ZZ_ACTION">
            <summary>
            Translates DFA states to action switch labels.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.ZZ_ROWMAP">
            <summary>
            Translates a state to a row index in the transition table
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.ZZ_TRANS">
            <summary>
            The transition table of the DFA
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.ZZ_ATTRIBUTE">
            <summary>
            ZZ_ATTRIBUTE[aState] contains the attributes of state <code>aState</code>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.zzReader">
            <summary>
            the input device </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.zzState">
            <summary>
            the current state of the DFA </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.zzLexicalState">
            <summary>
            the current lexical state </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.zzBuffer">
            <summary>
            this buffer contains the current text to be matched and is
               the source of the YyText() string 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.zzMarkedPos">
            <summary>
            the textposition at the last accepting state </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.zzCurrentPos">
            <summary>
            the current text position in the buffer </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.zzStartRead">
            <summary>
            startRead marks the beginning of the YyText() string in the buffer </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.zzEndRead">
            <summary>
            endRead marks the last character in the buffer, that has been read
               from input 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.yyline">
            <summary>
            number of newlines encountered up to the start of the matched text </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.yyChar">
            <summary>
            the number of characters up to the start of the matched text </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.yycolumn">
            <summary>
            the number of characters from the last newline up to the start of the 
            matched text
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.zzAtBOL">
            <summary>
            zzAtBOL == true &lt;=&gt; the scanner is currently at the beginning of a line
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.zzAtEOF">
            <summary>
            zzAtEOF == true &lt;=&gt; the scanner is at the EOF </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.zzEOFDone">
            <summary>
            denotes if the user-EOF-code has already been executed </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.WORD_TYPE">
            <summary>
            Alphanumeric sequences </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.NUMERIC_TYPE">
            <summary>
            Numbers </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.SOUTH_EAST_ASIAN_TYPE">
            <summary>
            Chars in class \p{Line_Break = Complex_Context} are from South East Asian
            scripts (Thai, Lao, Myanmar, Khmer, etc.).  Sequences of these are kept 
            together as as a single token rather than broken up, because the logic
            required to break them at word boundaries is too complex for UAX#29.
            <para>
            See Unicode Line Breaking Algorithm: http://www.unicode.org/reports/tr14/#SA
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.GetText(Lucene.Net.Analysis.TokenAttributes.ICharTermAttribute)">
            <summary>
            Fills CharTermAttribute with the current token text.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.#ctor(System.IO.TextReader)">
            <summary>
            Creates a new scanner
            </summary>
            <param name="in">  the TextReader to read input from. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.ZzUnpackCMap(System.String)">
            <summary>
            Unpacks the compressed character translation table.
            </summary>
            <param name="packed">   the packed character translation table </param>
            <returns>         the unpacked character translation table </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.ZzRefill">
            <summary>
            Refills the input buffer.
            </summary>
            <returns>      <code>false</code>, iff there was new input.
            </returns>
            <exception cref="T:System.IO.IOException">  if any I/O-Error occurs </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.YyClose">
            <summary>
            Disposes the input stream.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.YyReset(System.IO.TextReader)">
            <summary>
            Resets the scanner to read from a new input stream.
            Does not close the old reader.
            
            All internal variables are reset, the old input stream 
            <b>cannot</b> be reused (internal buffer is discarded and lost).
            Lexical state is set to <see cref="F:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.YYINITIAL"/>.
            
            Internal scan buffer is resized down to its initial length, if it has grown.
            </summary>
            <param name="reader">   the new input stream  </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.YyState">
            <summary>
            Returns the current lexical state.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.YyBegin(System.Int32)">
            <summary>
            Enters a new lexical state
            </summary>
            <param name="newState"> the new lexical state </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.YyText">
            <summary>
            Returns the text matched by the current regular expression.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.YyCharAt(System.Int32)">
            <summary>
            Returns the character at position <paramref name="pos"/> from the 
            matched text. 
            
            It is equivalent to YyText[pos], but faster
            </summary>
            <param name="pos"> the position of the character to fetch. 
                       A value from 0 to YyLength-1.
            </param>
            <returns> the character at position pos </returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.YyLength">
            <summary>
            Returns the length of the matched text region.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.ZzScanError(System.Int32)">
            <summary>
            Reports an error that occured while scanning.
            <para/>
            In a wellformed scanner (no or only correct usage of 
            YyPushBack(int) and a match-all fallback rule) this method 
            will only be called with things that "Can't Possibly Happen".
            If this method is called, something is seriously wrong
            (e.g. a JFlex bug producing a faulty scanner etc.).
            <para/>
            Usual syntax/scanner level error handling should be done
            in error fallback rules.
            </summary>
            <param name="errorCode">  the code of the errormessage to display </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.YyPushBack(System.Int32)">
            <summary>
            Pushes the specified amount of characters back into the input stream.
            
            They will be read again by then next call of the scanning method
            </summary>
            <param name="number">  the number of characters to be read again.
                           This number must not be greater than YyLength! </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.StandardTokenizerImpl.GetNextToken">
            <summary>
            Resumes scanning until the next regular expression is matched,
            the end of input is encountered or an I/O-Error occurs.
            </summary>
            <returns>      the next token </returns>
            <exception cref="T:System.IO.IOException">  if any I/O-Error occurs </exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.IStandardTokenizerInterface">
            <summary>
            Internal interface for supporting versioned grammars.
            @lucene.internal 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.IStandardTokenizerInterface.GetText(Lucene.Net.Analysis.TokenAttributes.ICharTermAttribute)">
            <summary>
            Copies the matched text into the <see cref="T:Lucene.Net.Analysis.TokenAttributes.ICharTermAttribute"/>
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.IStandardTokenizerInterface.YyChar">
            <summary>
            Returns the current position.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.IStandardTokenizerInterface.YyReset(System.IO.TextReader)">
            <summary>
            Resets the scanner to read from a new input stream.
            Does not close the old reader.
            <para/>
            All internal variables are reset, the old input stream 
            <b>cannot</b> be reused (internal buffer is discarded and lost).
            Lexical state is set to <c>YYINITIAL</c>.
            </summary>
            <param name="reader">   the new input stream  </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.IStandardTokenizerInterface.YyLength">
            <summary>
            Returns the length of the matched text region.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.IStandardTokenizerInterface.GetNextToken">
            <summary>
            Resumes scanning until the next regular expression is matched,
            the end of input is encountered or an I/O-Error occurs.
            </summary>
            <returns>      the next token, <see cref="F:Lucene.Net.Analysis.Standard.StandardTokenizerInterface.YYEOF"/> on end of stream </returns>
            <exception cref="T:System.IO.IOException">  if any I/O-Error occurs </exception>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.StandardTokenizerInterface.YYEOF">
            <summary>
            This character denotes the end of file </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31">
            <summary>
            This class implements StandardTokenizer, except with a bug 
            (https://issues.apache.org/jira/browse/LUCENE-3358) where Han and Hiragana
            characters would be split from combining characters:
            
            @deprecated This class is only for exact backwards compatibility
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.YYEOF">
            <summary>This character denotes the end of file</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.ZZ_BUFFERSIZE">
            <summary>initial size of the lookahead buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.YYINITIAL">
            <summary>lexical states</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.ZZ_LEXSTATE">
            <summary>
            ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
            ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
                at the beginning of a line
            l is of the form l = 2*k, k a non negative integer
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.ZZ_CMAP_PACKED">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.ZZ_CMAP">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.ZZ_ACTION">
            <summary>
            Translates DFA states to action switch labels.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.ZZ_ROWMAP">
            <summary>
            Translates a state to a row index in the transition table
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.ZZ_TRANS">
            <summary>
            The transition table of the DFA
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.ZZ_ATTRIBUTE">
            <summary>
            ZZ_ATTRIBUTE[aState] contains the attributes of state <c>aState</c>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.zzReader">
            <summary>the input device</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.zzState">
            <summary>the current state of the DFA</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.zzLexicalState">
            <summary>the current lexical state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.zzBuffer">
            <summary>
            this buffer contains the current text to be matched and is
            the source of the YyText() string
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.zzMarkedPos">
            <summary>the textposition at the last accepting state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.zzCurrentPos">
            <summary>the current text position in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.zzStartRead">
            <summary>startRead marks the beginning of the YyText string in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.zzEndRead">
            <summary>
            endRead marks the last character in the buffer, that has been read
            from input
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.yyline">
            <summary>number of newlines encountered up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.yyChar">
            <summary>the number of characters up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.yycolumn">
            <summary>
            the number of characters from the last newline up to the start of the 
            matched text
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.zzAtBOL">
            <summary>zzAtBOL == true &lt;=&gt; the scanner is currently at the beginning of a line</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.zzAtEOF">
            <summary>zzAtEOF == true &lt;=&gt; the scanner is at the EOF</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.zzEOFDone">
            <summary>denotes if the user-EOF-code has already been executed</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.WORD_TYPE">
            <summary>Alphanumeric sequences</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.NUMERIC_TYPE">
            <summary>Numbers</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.SOUTH_EAST_ASIAN_TYPE">
            <summary>
            Chars in class \p{Line_Break = Complex_Context} are from South East Asian
            scripts (Thai, Lao, Myanmar, Khmer, etc.).  Sequences of these are kept 
            together as as a single token rather than broken up, because the logic
            required to break them at word boundaries is too complex for UAX#29.
            <para/>
            See Unicode Line Breaking Algorithm: http://www.unicode.org/reports/tr14/#SA
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.GetText(Lucene.Net.Analysis.TokenAttributes.ICharTermAttribute)">
            <summary>Fills ICharTermAttribute with the current token text.</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.#ctor(System.IO.TextReader)">
            <summary>
            Creates a new scanner
            </summary>
            <param name="in">the TextReader to read input from.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.ZzUnpackCMap(System.String)">
            <summary>
            Unpacks the compressed character translation table.
            </summary>
            <param name="packed">the packed character translation table</param>
            <returns>the unpacked character translation table</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.ZzRefill">
            <summary>
            Refills the input buffer.
            </summary>
            <returns><c>false</c>, iff there was new input.</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.YyClose">
            <summary>
            Disposes the input stream.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.YyReset(System.IO.TextReader)">
            <summary>
            Resets the scanner to read from a new input stream.
            Does not close the old reader.
            <para/>
            All internal variables are reset, the old input stream 
            <b>cannot</b> be reused (internal buffer is discarded and lost).
            Lexical state is set to <see cref="F:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.YYINITIAL"/>.
            Internal scan buffer is resized down to its initial length, if it has grown.
            </summary>
            <param name="reader">the new input stream </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.YyState">
            <summary>
            Returns the current lexical state.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.YyBegin(System.Int32)">
            <summary>
            Enters a new lexical state
            </summary>
            <param name="newState">the new lexical state</param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.YyText">
            <summary>
            Returns the text matched by the current regular expression.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.YyCharAt(System.Int32)">
            <summary>
            Returns the character at position <paramref name="pos"/> from the 
            matched text.
            <para/>
            It is equivalent to YyText[pos], but faster
            </summary>
            <param name="pos">
            the position of the character to fetch.
            A value from 0 to YyLength-1.
            </param>
            <returns>the character at position pos</returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.YyLength">
            <summary>
            Returns the length of the matched text region.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.ZzScanError(System.Int32)">
            <summary>
            Reports an error that occured while scanning.
            <para/>
            In a wellformed scanner (no or only correct usage of 
            YyPushBack(int) and a match-all fallback rule) this method 
            will only be called with things that "Can't Possibly Happen".
            If this method is called, something is seriously wrong
            (e.g. a JFlex bug producing a faulty scanner etc.).
            Usual syntax/scanner level error handling should be done
            in error fallback rules.
            </summary>
            <param name="errorCode">the code of the errormessage to display</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.YyPushBack(System.Int32)">
            <summary>
            Pushes the specified amount of characters back into the input stream.
            <para/>
            They will be read again by then next call of the scanning method
            </summary>
            <param name="number">
            the number of characters to be read again.
            This number must not be greater than YyLength!
            </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.StandardTokenizerImpl31.GetNextToken">
            <summary>
            Resumes scanning until the next regular expression is matched,
            the end of input is encountered or an I/O-Error occurs.
            </summary>
            <returns>the next token</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31">
            <summary>
            This class implements UAX29URLEmailTokenizer, except with a bug 
            (https://issues.apache.org/jira/browse/LUCENE-3358) where Han and Hiragana
            characters would be split from combining characters:
            @deprecated This class is only for exact backwards compatibility
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.YYEOF">
            <summary>This character denotes the end of file</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.ZZ_BUFFERSIZE">
            <summary>initial size of the lookahead buffer </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.YYINITIAL">
            <summary>lexical states</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.ZZ_LEXSTATE">
            <summary>
            ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
            ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
                             at the beginning of a line
            l is of the form l = 2*k, k a non negative integer
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.ZZ_CMAP_PACKED">
            <summary>Translates characters to character classes</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.ZZ_CMAP">
            <summary>Translates characters to character classes</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.ZZ_ACTION">
            <summary>Translates DFA states to action switch labels.</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.ZZ_TRANS">
            <summary>The transition table of the DFA</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.ZZ_UNKNOWN_ERROR">
            <summary>error codes</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.ZZ_ERROR_MSG">
            <summary>error messages for the codes above</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.ZZ_ATTRIBUTE">
            <summary>ZZ_ATTRIBUTE[aState] contains the attributes of state <c>aState</c></summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.zzReader">
            <summary>the input device</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.zzState">
            <summary>the current state of the DFA</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.zzLexicalState">
            <summary>the current lexical state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.zzBuffer">
            <summary>
            this buffer contains the current text to be matched and is
            the source of the YyText string
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.zzMarkedPos">
            <summary>the textposition at the last accepting state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.zzCurrentPos">
            <summary>the current text position in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.zzStartRead">
            <summary>startRead marks the beginning of the YyText string in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.zzEndRead">
            <summary>
            endRead marks the last character in the buffer, that has been read
            from input
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.yyline">
            <summary>number of newlines encountered up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.yychar">
            <summary>the number of characters up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.yycolumn">
            <summary>
            the number of characters from the last newline up to the start of the 
            matched text
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.zzAtBOL">
            <summary>zzAtBOL == true &lt;=&gt; the scanner is currently at the beginning of a line</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.zzAtEOF">
            <summary>zzAtEOF == true &lt;=&gt; the scanner is at the EOF</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.zzEOFDone">
            <summary>denotes if the user-EOF-code has already been executed</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.WORD_TYPE">
            <summary>Alphanumeric sequences</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.NUMERIC_TYPE">
            <summary>Numbers</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.SOUTH_EAST_ASIAN_TYPE">
            <summary>
            Chars in class \p{Line_Break = Complex_Context} are from South East Asian
            scripts (Thai, Lao, Myanmar, Khmer, etc.).  Sequences of these are kept 
            together as as a single token rather than broken up, because the logic
            required to break them at word boundaries is too complex for UAX#29.
            <para/>
            See Unicode Line Breaking Algorithm: http://www.unicode.org/reports/tr14/#SA
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.GetText(Lucene.Net.Analysis.TokenAttributes.ICharTermAttribute)">
            <summary>Fills CharTermAttribute with the current token text.</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.#ctor(System.IO.TextReader)">
            <summary>
            Creates a new scanner
            </summary>
            <param name="in">the TextReader to read input from.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.ZzUnpackCMap(System.String)">
            <summary>
            Unpacks the compressed character translation table.
            </summary>
            <param name="packed">the packed character translation table</param>
            <returns>the unpacked character translation table</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.ZzRefill">
            <summary>
            Refills the input buffer.
            </summary>
            <returns><code>false</code>, iff there was new input.</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.YyClose">
            <summary>Closes the input stream.</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.YyReset(System.IO.TextReader)">
            <summary>
            Resets the scanner to read from a new input stream.
            Does not close the old reader.
            
            All internal variables are reset, the old input stream 
            <b>cannot</b> be reused (internal buffer is discarded and lost).
            Lexical state is set to <see cref="F:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.YYINITIAL"/>.
            
            Internal scan buffer is resized down to its initial length, if it has grown.
            </summary>
            <param name="reader">the new input stream </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.YyState">
            <summary>
            Returns the current lexical state.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.YyBegin(System.Int32)">
            <summary>
            Enters a new lexical state
            </summary>
            <param name="newState">the new lexical state</param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.YyText">
            <summary>Returns the text matched by the current regular expression.</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.YyCharAt(System.Int32)">
            <summary>
            Returns the character at position <tt>pos</tt> from the 
            matched text. 
            
            It is equivalent to YyText[pos], but faster
            </summary>
            <param name="pos">the position of the character to fetch. 
            A value from 0 to YyLength-1.</param>
            <returns>the character at position pos</returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.YyLength">
            <summary>Returns the length of the matched text region.</summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.ZzScanError(System.Int32)">
            <summary>
            Reports an error that occured while scanning.
            
            In a wellformed scanner (no or only correct usage of 
            YyPushBack(int) and a match-all fallback rule) this method 
            will only be called with things that "Can't Possibly Happen".
            If this method is called, something is seriously wrong
            (e.g. a JFlex bug producing a faulty scanner etc.).
            
            Usual syntax/scanner level error handling should be done
            in error fallback rules.
            </summary>
            <param name="errorCode">the code of the errormessage to display</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.YyPushBack(System.Int32)">
            <summary>
            Pushes the specified amount of characters back into the input stream.
            
            They will be read again by then next call of the scanning method
            </summary>
            <param name="number">the number of characters to be read again.
            This number must not be greater than YyLength!</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std31.UAX29URLEmailTokenizerImpl31.GetNextToken">
            <summary>
            Resumes scanning until the next regular expression is matched,
            the end of input is encountered or an I/O-Error occurs.
            </summary>
            <returns>      the next token </returns>
            <exception cref="T:System.IO.IOException">  if any I/O-Error occurs </exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34">
            <summary>
            This class implements StandardTokenizer using Unicode 6.0.0.
            
            @deprecated This class is only for exact backwards compatibility
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.YYEOF">
            <summary>This character denotes the end of file</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.ZZ_BUFFERSIZE">
            <summary>initial size of the lookahead buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.YYINITIAL">
            <summary>lexical states</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.ZZ_LEXSTATE">
            <summary>
            ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
            ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
                at the beginning of a line
            l is of the form l = 2*k, k a non negative integer
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.ZZ_CMAP_PACKED">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.ZZ_CMAP">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.ZZ_ACTION">
            <summary>
            Translates DFA states to action switch labels.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.ZZ_ROWMAP">
            <summary>
            Translates a state to a row index in the transition table
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.ZZ_TRANS">
            <summary>
            The transition table of the DFA
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.ZZ_ATTRIBUTE">
            <summary>
            ZZ_ATTRIBUTE[aState] contains the attributes of state <c>aState</c>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.zzReader">
            <summary>the input device</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.zzState">
            <summary>the current state of the DFA</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.zzLexicalState">
            <summary>the current lexical state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.zzBuffer">
            <summary>
            this buffer contains the current text to be matched and is
            the source of the YyText string
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.zzMarkedPos">
            <summary>the textposition at the last accepting state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.zzCurrentPos">
            <summary>the current text position in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.zzStartRead">
            <summary>startRead marks the beginning of the YyText string in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.zzEndRead">
            <summary>
            endRead marks the last character in the buffer, that has been read
            from input
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.yyline">
            <summary>number of newlines encountered up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.yyChar">
            <summary>the number of characters up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.yycolumn">
            <summary>
            the number of characters from the last newline up to the start of the 
            matched text
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.zzAtBOL">
            <summary>
            zzAtBOL == true &lt;=&gt; the scanner is currently at the beginning of a line
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.zzAtEOF">
            <summary>zzAtEOF == true &lt;=&gt; the scanner is at the EOF</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.zzEOFDone">
            <summary>denotes if the user-EOF-code has already been executed</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.WORD_TYPE">
            <summary>Alphanumeric sequences</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.NUMERIC_TYPE">
            <summary>Numbers</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.SOUTH_EAST_ASIAN_TYPE">
            <summary>
            Chars in class \p{Line_Break = Complex_Context} are from South East Asian
            scripts (Thai, Lao, Myanmar, Khmer, etc.).  Sequences of these are kept 
            together as as a single token rather than broken up, because the logic
            required to break them at word boundaries is too complex for UAX#29.
            <para/>
            See Unicode Line Breaking Algorithm: http://www.unicode.org/reports/tr14/#SA
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.GetText(Lucene.Net.Analysis.TokenAttributes.ICharTermAttribute)">
            <summary>
            Fills ICharTermAttribute with the current token text.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.#ctor(System.IO.TextReader)">
            <summary>
            Creates a new scanner
            </summary>
            <param name="in">the TextReader to read input from.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.ZzUnpackCMap(System.String)">
            <summary>
            Unpacks the compressed character translation table.
            </summary>
            <param name="packed">the packed character translation table</param>
            <returns>the unpacked character translation table</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.ZzRefill">
            <summary>
            Refills the input buffer.
            </summary>
            <returns><c>false</c>, iff there was new input.</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.YyClose">
            <summary>
            Disposes the input stream.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.YyReset(System.IO.TextReader)">
            <summary>
            Resets the scanner to read from a new input stream.
            Does not close the old reader.
            <para/>
            All internal variables are reset, the old input stream 
            <b>cannot</b> be reused (internal buffer is discarded and lost).
            Lexical state is set to <see cref="F:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.YYINITIAL"/>.
            <para/>
            Internal scan buffer is resized down to its initial length, if it has grown.
            </summary>
            <param name="reader">the new input stream </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.YyState">
            <summary>
            Returns the current lexical state.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.YyBegin(System.Int32)">
            <summary>
            Enters a new lexical state
            </summary>
            <param name="newState">the new lexical state</param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.YyText">
            <summary>
            Returns the text matched by the current regular expression.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.YyCharAt(System.Int32)">
            <summary>
            Returns the character at position <paramref name="pos"/> from the 
            matched text. 
            <para/>
            It is equivalent to YyText[pos], but faster
            </summary>
            <param name="pos">
            the position of the character to fetch.
            A value from 0 to YyLength-1.
            </param>
            <returns>the character at position pos</returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.YyLength">
            <summary>
            Returns the length of the matched text region.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.ZzScanError(System.Int32)">
            <summary>
            Reports an error that occured while scanning.
            <para/>
            In a wellformed scanner (no or only correct usage of 
            YyPushBack(int) and a match-all fallback rule) this method 
            will only be called with things that "Can't Possibly Happen".
            If this method is called, something is seriously wrong
            Usual syntax/scanner level error handling should be done
            in error fallback rules.
            </summary>
            <param name="errorCode">the code of the errormessage to display</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.YyPushBack(System.Int32)">
            <summary>
            Pushes the specified amount of characters back into the input stream.
            <para/>
            They will be read again by then next call of the scanning method
            </summary>
            <param name="number">
            the number of characters to be read again.
            This number must not be greater than YyLength!
            </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.StandardTokenizerImpl34.GetNextToken">
            <summary>
            Resumes scanning until the next regular expression is matched,
            the end of input is encountered or an I/O-Error occurs.
            </summary>
            <returns>the next token</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34">
            <summary>
            This class implements UAX29URLEmailTokenizer, except with a bug
            (https://issues.apache.org/jira/browse/LUCENE-3880) where "mailto:"
            URI scheme prepended to an email address will disrupt recognition
            of the email address.
            
            @deprecated This class is only for exact backwards compatibility
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.YYEOF">
            <summary>This character denotes the end of file</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.ZZ_BUFFERSIZE">
            <summary>initial size of the lookahead buffer </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.YYINITIAL">
            <summary>lexical states</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.ZZ_LEXSTATE">
            <summary>
            ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
            ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
                at the beginning of a line
            l is of the form l = 2*k, k a non negative integer
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.ZZ_CMAP_PACKED">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.ZZ_CMAP">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.ZZ_ACTION">
            <summary>
            Translates DFA states to action switch labels.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.ZZ_ROWMAP">
            <summary>
            Translates a state to a row index in the transition table
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.ZZ_TRANS">
            <summary>
            The transition table of the DFA
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.ZZ_ATTRIBUTE">
            <summary>
            ZZ_ATTRIBUTE[aState] contains the attributes of state <c>aState</c>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.zzReader">
            <summary>the input device</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.zzState">
            <summary>the current state of the DFA</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.zzLexicalState">
            <summary>the current lexical state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.zzBuffer">
            <summary>
            this buffer contains the current text to be matched and is
            the source of the YyText string
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.zzMarkedPos">
            <summary>the textposition at the last accepting state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.zzCurrentPos">
            <summary>the current text position in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.zzStartRead">
            <summary>startRead marks the beginning of the YyText string in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.zzEndRead">
            <summary>
            endRead marks the last character in the buffer, that has been read
            from input
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.yyline">
            <summary>number of newlines encountered up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.yychar">
            <summary>the number of characters up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.yycolumn">
            <summary>
            the number of characters from the last newline up to the start of the
            matched text
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.zzAtBOL">
            <summary>
            zzAtBOL == true &lt;=&gt; the scanner is currently at the beginning of a line
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.zzAtEOF">
            <summary>zzAtEOF == true &lt;=&gt; the scanner is at the EOF</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.zzEOFDone">
            <summary>denotes if the user-EOF-code has already been executed</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.WORD_TYPE">
            <summary>Alphanumeric sequences</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.NUMERIC_TYPE">
            <summary>Numbers</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.SOUTH_EAST_ASIAN_TYPE">
            <summary>
            Chars in class \p{Line_Break = Complex_Context} are from South East Asian
            scripts (Thai, Lao, Myanmar, Khmer, etc.).  Sequences of these are kept 
            together as as a single token rather than broken up, because the logic
            required to break them at word boundaries is too complex for UAX#29.
            <para/>
            See Unicode Line Breaking Algorithm: http://www.unicode.org/reports/tr14/#SA
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.GetText(Lucene.Net.Analysis.TokenAttributes.ICharTermAttribute)">
            <summary>
            Fills ICharTermAttribute with the current token text.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.#ctor(System.IO.TextReader)">
            <summary>
            Creates a new scanner
            </summary>
            <param name="in">the TextReader to read input from.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.ZzUnpackCMap(System.String)">
            <summary>
            Unpacks the compressed character translation table.
            </summary>
            <param name="packed">the packed character translation table</param>
            <returns>the unpacked character translation table</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.ZzRefill">
            <summary>
            Refills the input buffer.
            </summary>
            <returns><c>false</c>, iff there was new input.</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.YyClose">
            <summary>
            Disposes the input stream.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.YyReset(System.IO.TextReader)">
            <summary>
            Resets the scanner to read from a new input stream.
            Does not close the old reader.
            <para/>
            All internal variables are reset, the old input stream 
            <b>cannot</b> be reused (internal buffer is discarded and lost).
            Lexical state is set to <see cref="F:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.YYINITIAL"/>.
            <para/>
            Internal scan buffer is resized down to its initial length, if it has grown.
            </summary>
            <param name="reader">the new input stream </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.YyState">
            <summary>
            Returns the current lexical state.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.YyBegin(System.Int32)">
            <summary>
            Enters a new lexical state
            </summary>
            <param name="newState">the new lexical state</param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.YyText">
            <summary>
            Returns the text matched by the current regular expression.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.YyCharAt(System.Int32)">
            <summary>
            Returns the character at position <paramref name="pos"/> from the 
            matched text.
            <para/>
            It is equivalent to YyText[pos], but faster
            </summary>
            <param name="pos">
            the position of the character to fetch. 
            A value from 0 to YyLength-1.
            </param>
            <returns>the character at position pos</returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.YyLength">
            <summary>
            Returns the length of the matched text region.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.ZzScanError(System.Int32)">
            <summary>
            Reports an error that occured while scanning.
            <para/>
            In a wellformed scanner (no or only correct usage of 
            YyPushBack(int) and a match-all fallback rule) this method 
            will only be called with things that "Can't Possibly Happen".
            If this method is called, something is seriously wrong
            (e.g. a JFlex bug producing a faulty scanner etc.).
            <para/>
            Usual syntax/scanner level error handling should be done
            in error fallback rules.
            </summary>
            <param name="errorCode">the code of the errormessage to display</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.YyPushBack(System.Int32)">
            <summary>
            Pushes the specified amount of characters back into the input stream.
            <para/>
            They will be read again by then next call of the scanning method
            </summary>
            <param name="number">
            the number of characters to be read again.
            This number must not be greater than YyLength!
            </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std34.UAX29URLEmailTokenizerImpl34.GetNextToken">
            <summary>
            Resumes scanning until the next regular expression is matched,
            the end of input is encountered or an I/O-Error occurs.
            </summary>
            <returns>the next token</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36">
            <summary>
            This class implements UAX29URLEmailTokenizer using Unicode 6.0.0.
            @deprecated This class is only for exact backwards compatibility
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.YYEOF">
            <summary>This character denotes the end of file</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.ZZ_BUFFERSIZE">
            <summary>initial size of the lookahead buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.YYINITIAL">
            <summary>lexical states</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.ZZ_LEXSTATE">
            <summary>
            ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
            ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
                at the beginning of a line
            l is of the form l = 2*k, k a non negative integer
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.ZZ_CMAP_PACKED">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.ZZ_CMAP">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.ZZ_ACTION">
            <summary>
            Translates DFA states to action switch labels.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.ZZ_ROWMAP">
            <summary>
            Translates a state to a row index in the transition table
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.ZZ_TRANS">
            <summary>
            The transition table of the DFA
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.ZZ_ATTRIBUTE">
            <summary>
            ZZ_ATTRIBUTE[aState] contains the attributes of state <c>aState</c>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.zzReader">
            <summary>the input device</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.zzState">
            <summary>the current state of the DFA</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.zzLexicalState">
            <summary>the current lexical state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.zzBuffer">
            <summary>
            this buffer contains the current text to be matched and is
            the source of the YyText string 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.zzMarkedPos">
            <summary>the textposition at the last accepting state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.zzCurrentPos">
            <summary>the current text position in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.zzStartRead">
            <summary>startRead marks the beginning of the YyText string in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.zzEndRead">
            <summary>
            endRead marks the last character in the buffer, that has been read
            from input
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.yyline">
            <summary>number of newlines encountered up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.yychar">
            <summary>the number of characters up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.yycolumn">
            <summary>
            the number of characters from the last newline up to the start of the 
            matched text
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.zzAtBOL">
            <summary>
            zzAtBOL == true &lt;=&gt; the scanner is currently at the beginning of a line
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.zzAtEOF">
            <summary>zzAtEOF == true &lt;=&gt; the scanner is at the EOF</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.zzEOFDone">
            <summary>denotes if the user-EOF-code has already been executed</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.WORD_TYPE">
            <summary>Alphanumeric sequences</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.NUMERIC_TYPE">
            <summary>Numbers</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.SOUTH_EAST_ASIAN_TYPE">
            <summary>
            Chars in class \p{Line_Break = Complex_Context} are from South East Asian
            scripts (Thai, Lao, Myanmar, Khmer, etc.).  Sequences of these are kept 
            together as as a single token rather than broken up, because the logic
            required to break them at word boundaries is too complex for UAX#29.
            <para/>
            See Unicode Line Breaking Algorithm: http://www.unicode.org/reports/tr14/#SA
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.GetText(Lucene.Net.Analysis.TokenAttributes.ICharTermAttribute)">
            <summary>
            Fills ICharTermAttribute with the current token text.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.#ctor(System.IO.TextReader)">
            <summary>
            Creates a new scanner
            </summary>
            <param name="in">the TextReader to read input from.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.ZzUnpackCMap(System.String)">
            <summary>
            Unpacks the compressed character translation table.
            </summary>
            <param name="packed">the packed character translation table</param>
            <returns>the unpacked character translation table</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.ZzRefill">
            <summary>
            Refills the input buffer.
            </summary>
            <returns><c>false</c>, iff there was new input.</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.YyClose">
            <summary>
            Disposes the input stream.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.YyReset(System.IO.TextReader)">
            <summary>
            Resets the scanner to read from a new input stream.
            Does not close the old reader.
            <para/>
            All internal variables are reset, the old input stream 
            <b>cannot</b> be reused (internal buffer is discarded and lost).
            Lexical state is set to <see cref="F:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.YYINITIAL"/>.
            <para/>
            Internal scan buffer is resized down to its initial length, if it has grown.
            </summary>
            <param name="reader">the new input stream </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.YyState">
            <summary>
            Returns the current lexical state.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.YyBegin(System.Int32)">
            <summary>
            Enters a new lexical state
            </summary>
            <param name="newState">the new lexical state</param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.YyText">
            <summary>
            Returns the text matched by the current regular expression.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.YyCharAt(System.Int32)">
            <summary>
            Returns the character at position <paramref name="pos"/> from the 
            matched text.
            <para/>
            It is equivalent to YyText[pos], but faster
            </summary>
            <param name="pos">
            the position of the character to fetch. 
            A value from 0 to YyLength-1.
            </param>
            <returns>the character at position pos</returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.YyLength">
            <summary>
            Returns the length of the matched text region.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.ZzScanError(System.Int32)">
            <summary>
            Reports an error that occured while scanning.
            <para/>
            In a wellformed scanner (no or only correct usage of 
            YyPushBack(int) and a match-all fallback rule) this method 
            will only be called with things that "Can't Possibly Happen".
            If this method is called, something is seriously wrong
            (e.g. a JFlex bug producing a faulty scanner etc.).
            <para/>
            Usual syntax/scanner level error handling should be done
            in error fallback rules.
            </summary>
            <param name="errorCode">the code of the errormessage to display</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.YyPushBack(System.Int32)">
            <summary>
            Pushes the specified amount of characters back into the input stream.
            <para/>
            They will be read again by then next call of the scanning method
            </summary>
            <param name="number">
            the number of characters to be read again.
            This number must not be greater than YyLength!
            </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std36.UAX29URLEmailTokenizerImpl36.GetNextToken">
            <summary>
            Resumes scanning until the next regular expression is matched,
            the end of input is encountered or an I/O-Error occurs.
            </summary>
            <returns>the next token</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40">
            <summary>
            This class implements StandardTokenizer using Unicode 6.1.0.
            
            @deprecated This class is only for exact backwards compatibility
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.YYEOF">
            <summary>This character denotes the end of file</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.ZZ_BUFFERSIZE">
            <summary>initial size of the lookahead buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.YYINITIAL">
            <summary>lexical states</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.ZZ_LEXSTATE">
            <summary>
            ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
            ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
                at the beginning of a line
            l is of the form l = 2*k, k a non negative integer
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.ZZ_CMAP_PACKED">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.ZZ_CMAP">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.ZZ_ACTION">
            <summary>
            Translates DFA states to action switch labels.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.ZZ_ROWMAP">
            <summary>
            Translates a state to a row index in the transition table
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.ZZ_TRANS">
            <summary>
            The transition table of the DFA
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.ZZ_ATTRIBUTE">
            <summary>
            ZZ_ATTRIBUTE[aState] contains the attributes of state <c>aState</c>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.zzReader">
            <summary>the input device</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.zzState">
            <summary>the current state of the DFA</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.zzLexicalState">
            <summary>the current lexical state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.zzBuffer">
            <summary>
            this buffer contains the current text to be matched and is
            the source of the YyText string 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.zzMarkedPos">
            <summary>the textposition at the last accepting state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.zzCurrentPos">
            <summary>the current text position in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.zzStartRead">
            <summary>startRead marks the beginning of the YyText string in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.zzEndRead">
            <summary>
            endRead marks the last character in the buffer, that has been read
            from input
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.yyline">
            <summary>number of newlines encountered up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.yyChar">
            <summary>the number of characters up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.yycolumn">
            <summary>
            the number of characters from the last newline up to the start of the
            matched text
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.zzAtBOL">
            <summary>
            zzAtBOL == true &lt;=&gt; the scanner is currently at the beginning of a line
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.zzAtEOF">
            <summary>zzAtEOF == true &lt;=&gt; the scanner is at the EOF</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.zzEOFDone">
            <summary>denotes if the user-EOF-code has already been executed</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.WORD_TYPE">
            <summary>Alphanumeric sequences</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.NUMERIC_TYPE">
            <summary>Numbers</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.SOUTH_EAST_ASIAN_TYPE">
            <summary>
            Chars in class \p{Line_Break = Complex_Context} are from South East Asian
            scripts (Thai, Lao, Myanmar, Khmer, etc.).  Sequences of these are kept 
            together as as a single token rather than broken up, because the logic
            required to break them at word boundaries is too complex for UAX#29.
            <para/>
            See Unicode Line Breaking Algorithm: http://www.unicode.org/reports/tr14/#SA
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.GetText(Lucene.Net.Analysis.TokenAttributes.ICharTermAttribute)">
            <summary>
            Fills ICharTermAttribute with the current token text.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.#ctor(System.IO.TextReader)">
            <summary>
            Creates a new scanner
            </summary>
            <param name="in">the TextReader to read input from.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.ZzUnpackCMap(System.String)">
            <summary>
            Unpacks the compressed character translation table.
            </summary>
            <param name="packed">the packed character translation table</param>
            <returns>the unpacked character translation table</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.ZzRefill">
            <summary>
            Refills the input buffer.
            </summary>
            <returns><c>false</c>, iff there was new input.</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.YyClose">
            <summary>
            Disposes the input stream.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.YyReset(System.IO.TextReader)">
            <summary>
            Resets the scanner to read from a new input stream.
            Does not close the old reader.
            <para/>
            All internal variables are reset, the old input stream 
            <b>cannot</b> be reused (internal buffer is discarded and lost).
            Lexical state is set to <see cref="F:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.YYINITIAL"/>.
            <para/>
            Internal scan buffer is resized down to its initial length, if it has grown.
            </summary>
            <param name="reader">the new input stream </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.YyState">
            <summary>
            Returns the current lexical state.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.YyBegin(System.Int32)">
            <summary>
            Enters a new lexical state
            </summary>
            <param name="newState">the new lexical state</param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.YyText">
            <summary>
            Returns the text matched by the current regular expression.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.YyCharAt(System.Int32)">
            <summary>
            Returns the character at position <paramref name="pos"/> from the 
            matched text.
            <para/>
            It is equivalent to YyText[pos], but faster
            </summary>
            <param name="pos">
            the position of the character to fetch. 
            A value from 0 to YyLength-1.
            </param>
            <returns>the character at position pos</returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.YyLength">
            <summary>
            Returns the length of the matched text region.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.ZzScanError(System.Int32)">
            <summary>
            Reports an error that occured while scanning.
            <para/>
            In a wellformed scanner (no or only correct usage of 
            YyPushBack(int) and a match-all fallback rule) this method 
            will only be called with things that "Can't Possibly Happen".
            If this method is called, something is seriously wrong
            (e.g. a JFlex bug producing a faulty scanner etc.).
            <para/>
            Usual syntax/scanner level error handling should be done
            in error fallback rules.
            </summary>
            <param name="errorCode">the code of the errormessage to display</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.YyPushBack(System.Int32)">
            <summary>
            Pushes the specified amount of characters back into the input stream.
            <para/>
            They will be read again by then next call of the scanning method
            </summary>
            <param name="number">
            the number of characters to be read again.
            This number must not be greater than YyLength!
            </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.StandardTokenizerImpl40.GetNextToken">
            <summary>
            Resumes scanning until the next regular expression is matched,
            the end of input is encountered or an I/O-Error occurs.
            </summary>
            <returns>the next token</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40">
            <summary>
            This class implements <see cref="T:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizer"/> using Unicode 6.1.0.
            @deprecated This class is only for exact backwards compatibility
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.YYEOF">
            <summary>This character denotes the end of file</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.ZZ_BUFFERSIZE">
            <summary>initial size of the lookahead buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.YYINITIAL">
            <summary>lexical states</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.ZZ_LEXSTATE">
            <summary>
            ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
            ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
                at the beginning of a line
            l is of the form l = 2*k, k a non negative integer
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.ZZ_CMAP_PACKED">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.ZZ_CMAP">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.ZZ_ACTION">
            <summary>
            Translates DFA states to action switch labels.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.ZZ_ROWMAP">
            <summary>
            Translates a state to a row index in the transition table
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.ZZ_TRANS">
            <summary>
            The transition table of the DFA
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.ZZ_ATTRIBUTE">
            <summary>
            ZZ_ATTRIBUTE[aState] contains the attributes of state <c>aState</c>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.zzReader">
            <summary>the input device</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.zzState">
            <summary>the current state of the DFA</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.zzLexicalState">
            <summary>the current lexical state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.zzBuffer">
            <summary>
            this buffer contains the current text to be matched and is
            the source of the YyText string 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.zzMarkedPos">
            <summary>the textposition at the last accepting state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.zzCurrentPos">
            <summary>the current text position in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.zzStartRead">
            <summary>startRead marks the beginning of the YyText string in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.zzEndRead">
            <summary>
            endRead marks the last character in the buffer, that has been read
            from input
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.yyline">
            <summary>number of newlines encountered up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.yychar">
            <summary>the number of characters up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.yycolumn">
            <summary>
            the number of characters from the last newline up to the start of the
            matched text
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.zzAtBOL">
            <summary>
            zzAtBOL == true &lt;=&gt; the scanner is currently at the beginning of a line
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.zzAtEOF">
            <summary>zzAtEOF == true &lt;=&gt; the scanner is at the EOF</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.zzEOFDone">
            <summary>denotes if the user-EOF-code has already been executed</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.WORD_TYPE">
            <summary>Alphanumeric sequences</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.NUMERIC_TYPE">
            <summary>Numbers</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.SOUTH_EAST_ASIAN_TYPE">
            <summary>
            Chars in class \p{Line_Break = Complex_Context} are from South East Asian
            scripts (Thai, Lao, Myanmar, Khmer, etc.).  Sequences of these are kept 
            together as as a single token rather than broken up, because the logic
            required to break them at word boundaries is too complex for UAX#29.
            <para/>
            See Unicode Line Breaking Algorithm: http://www.unicode.org/reports/tr14/#SA
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.GetText(Lucene.Net.Analysis.TokenAttributes.ICharTermAttribute)">
            <summary>
            Fills ICharTermAttribute with the current token text.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.#ctor(System.IO.TextReader)">
            <summary>
            Creates a new scanner
            </summary>
            <param name="in">the TextReader to read input from.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.ZzUnpackCMap(System.String)">
            <summary>
            Unpacks the compressed character translation table.
            </summary>
            <param name="packed">the packed character translation table</param>
            <returns>the unpacked character translation table</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.ZzRefill">
            <summary>
            Refills the input buffer.
            </summary>
            <returns><c>false</c>, iff there was new input.</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.YyClose">
            <summary>
            Disposes the input stream.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.YyReset(System.IO.TextReader)">
            <summary>
            Resets the scanner to read from a new input stream.
            Does not close the old reader.
            <para/>
            All internal variables are reset, the old input stream 
            <b>cannot</b> be reused (internal buffer is discarded and lost).
            Lexical state is set to <see cref="F:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.YYINITIAL"/>.
            <para/>
            Internal scan buffer is resized down to its initial length, if it has grown.
            </summary>
            <param name="reader">the new input stream </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.YyState">
            <summary>
            Returns the current lexical state.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.YyBegin(System.Int32)">
            <summary>
            Enters a new lexical state
            </summary>
            <param name="newState">the new lexical state</param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.YyText">
            <summary>
            Returns the text matched by the current regular expression.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.YyCharAt(System.Int32)">
            <summary>
            Returns the character at position <paramref name="pos"/> from the 
            matched text.
            <para/>
            It is equivalent to YyText[pos], but faster
            </summary>
            <param name="pos">
            the position of the character to fetch. 
            A value from 0 to YyLength-1.
            </param>
            <returns>the character at position pos</returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.YyLength">
            <summary>
            Returns the length of the matched text region.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.ZzScanError(System.Int32)">
            <summary>
            Reports an error that occured while scanning.
            <para/>
            In a wellformed scanner (no or only correct usage of 
            YyPushBack(int) and a match-all fallback rule) this method 
            will only be called with things that "Can't Possibly Happen".
            If this method is called, something is seriously wrong
            (e.g. a JFlex bug producing a faulty scanner etc.).
            <para/>
            Usual syntax/scanner level error handling should be done
            in error fallback rules.
            </summary>
            <param name="errorCode">the code of the errormessage to display</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.YyPushBack(System.Int32)">
            <summary>
            Pushes the specified amount of characters back into the input stream.
            <para/>
            They will be read again by then next call of the scanning method
            </summary>
            <param name="number">
            the number of characters to be read again.
            This number must not be greater than YyLength!
            </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.Std40.UAX29URLEmailTokenizerImpl40.GetNextToken">
            <summary>
            Resumes scanning until the next regular expression is matched,
            the end of input is encountered or an I/O-Error occurs.
            </summary>
            <returns>the next token</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.UAX29URLEmailAnalyzer">
            <summary>
            Filters <see cref="T:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizer"/>
            with <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>,
            <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/> and
            <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>, using a list of
            English stop words.
            
            <para>
              You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
              compatibility when creating <see cref="T:Lucene.Net.Analysis.Standard.UAX29URLEmailAnalyzer"/>
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailAnalyzer.DEFAULT_MAX_TOKEN_LENGTH">
            <summary>
            Default maximum allowed token length </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailAnalyzer.STOP_WORDS_SET">
            <summary>
            An unmodifiable set containing some common English words that are usually not
            useful for searching. 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. </summary>
            <param name="matchVersion"> Lucene version to match - See <see cref="T:Lucene.Net.Analysis.Standard.UAX29URLEmailAnalyzer"/> </param>
            <param name="stopWords"> stop words  </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words (<see cref="F:Lucene.Net.Analysis.Standard.UAX29URLEmailAnalyzer.STOP_WORDS_SET"/>.
            </summary>
            <param name="matchVersion"> Lucene version to match - See <see cref="T:Lucene.Net.Analysis.Standard.UAX29URLEmailAnalyzer"/> </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader)">
            <summary>
            Builds an analyzer with the stop words from the given reader. </summary>
            <seealso cref="M:Lucene.Net.Analysis.Util.WordlistLoader.GetWordSet(System.IO.TextReader,Lucene.Net.Util.LuceneVersion)"/>
            <param name="matchVersion"> Lucene version to match - See <see cref="T:Lucene.Net.Analysis.Standard.UAX29URLEmailAnalyzer"/> </param>
            <param name="stopwords"> <see cref="T:System.IO.TextReader"/> to read stop words from  </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.UAX29URLEmailAnalyzer.MaxTokenLength">
            <summary>
            Set maximum allowed token length.  If a token is seen
            that exceeds this length then it is discarded.  This
            setting only takes effect the next time tokenStream or
            tokenStream is called.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizer">
            <summary>
            This class implements Word Break rules from the Unicode Text Segmentation 
            algorithm, as specified in                 `
            <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a> 
            URLs and email addresses are also tokenized according to the relevant RFCs.
            <para/>
            Tokens produced are of the following types:
            <list type="bullet">
                <item><description>&lt;ALPHANUM&gt;: A sequence of alphabetic and numeric characters</description></item>
                <item><description>&lt;NUM&gt;: A number</description></item>
                <item><description>&lt;URL&gt;: A URL</description></item>
                <item><description>&lt;EMAIL&gt;: An email address</description></item>
                <item><description>&lt;SOUTHEAST_ASIAN&gt;: A sequence of characters from South and Southeast
                    Asian languages, including Thai, Lao, Myanmar, and Khmer</description></item>
                <item><description>&lt;IDEOGRAPHIC&gt;: A single CJKV ideographic character</description></item>
                <item><description>&lt;HIRAGANA&gt;: A single hiragana character</description></item>
            </list>
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizer"/>:
            <list type="bullet">
                <item><description> As of 3.4, Hiragana and Han characters are no longer wrongly split
                    from their combining characters. If you use a previous version number,
                    you get the exact broken behavior for backwards compatibility.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizer.scanner">
            <summary>
            A private instance of the JFlex-constructed scanner </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizer.TOKEN_TYPES">
            <summary>
            String token types that correspond to token type int constants </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizer.MaxTokenLength">
            <summary>
            Set the max allowed token length.  Any token longer
             than this is skipped. 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader)">
            <summary>
            Creates a new instance of the <see cref="T:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizer"/>.  Attaches
            the <paramref name="input"/> to the newly created JFlex scanner.
            </summary>
            <param name="matchVersion"> Lucene compatibility version </param>
            <param name="input"> The input reader </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizer"/> with a given <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizer.Init(Lucene.Net.Util.LuceneVersion)">
            <summary>
            LUCENENET specific: This method was added in .NET to prevent having to repeat code in the constructors.
            </summary>
            <param name="matchVersion"></param>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizer"/>. 
            <code>
            &lt;fieldType name="text_urlemail" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.UAX29URLEmailTokenizerFactory" maxTokenLength="255"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code> 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl">
            <summary>
            This class implements Word Break rules from the Unicode Text Segmentation 
            algorithm, as specified in 
            <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a> 
            URLs and email addresses are also tokenized according to the relevant RFCs.
            <para/>
            Tokens produced are of the following types:
            <list type="bullet">
                <item><description>&lt;ALPHANUM&gt;: A sequence of alphabetic and numeric characters</description></item>
                <item><description>&lt;NUM&gt;: A number</description></item>
                <item><description>&lt;URL&gt;: A URL</description></item>
                <item><description>&lt;EMAIL&gt;: An email address</description></item>
                <item><description>&lt;SOUTHEAST_ASIAN&gt;: A sequence of characters from South and Southeast
                    Asian languages, including Thai, Lao, Myanmar, and Khmer</description></item>
                <item><description>&lt;IDEOGRAPHIC&gt;: A single CJKV ideographic character</description></item>
                <item><description>&lt;HIRAGANA&gt;: A single hiragana character</description></item>
                <item><description>&lt;KATAKANA&gt;: A sequence of katakana characters</description></item>
                <item><description>&lt;HANGUL&gt;: A sequence of Hangul characters</description></item>
            </list>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.YYEOF">
            <summary>This character denotes the end of file</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.ZZ_BUFFERSIZE">
            <summary>initial size of the lookahead buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.YYINITIAL">
            <summary>lexical states</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.ZZ_LEXSTATE">
            <summary>
            ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
            ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
                at the beginning of a line
            l is of the form l = 2*k, k a non negative integer
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.ZZ_CMAP_PACKED">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.ZZ_CMAP">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.ZZ_ACTION">
            <summary>
            Translates DFA states to action switch labels.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.ZZ_ROWMAP">
            <summary>
            Translates a state to a row index in the transition table
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.ZZ_TRANS">
            <summary>
            The transition table of the DFA
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.ZZ_ATTRIBUTE">
            <summary>
            ZZ_ATTRIBUTE[aState] contains the attributes of state <c>aState</c>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.zzReader">
            <summary>the input device</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.zzState">
            <summary>the current state of the DFA</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.zzLexicalState">
            <summary>the current lexical state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.zzBuffer">
            <summary>
            this buffer contains the current text to be matched and is
            the source of the YyText string 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.zzMarkedPos">
            <summary>the textposition at the last accepting state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.zzCurrentPos">
            <summary>the current text position in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.zzStartRead">
            <summary>startRead marks the beginning of the YyText string in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.zzEndRead">
            <summary>
            endRead marks the last character in the buffer, that has been read
            from input
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.yyline">
            <summary>number of newlines encountered up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.yychar">
            <summary>the number of characters up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.yycolumn">
            <summary>
            the number of characters from the last newline up to the start of the
            matched text
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.zzAtBOL">
            <summary>
            zzAtBOL == true &lt;=&gt; the scanner is currently at the beginning of a line
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.zzAtEOF">
            <summary>zzAtEOF == true &lt;=&gt; the scanner is at the EOF</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.zzEOFDone">
            <summary>denotes if the user-EOF-code has already been executed</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.WORD_TYPE">
            <summary>Alphanumeric sequences</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.NUMERIC_TYPE">
            <summary>Numbers</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.SOUTH_EAST_ASIAN_TYPE">
            <summary>
            Chars in class \p{Line_Break = Complex_Context} are from South East Asian
            scripts (Thai, Lao, Myanmar, Khmer, etc.).  Sequences of these are kept 
            together as as a single token rather than broken up, because the logic
            required to break them at word boundaries is too complex for UAX#29.
            <para/>
            See Unicode Line Breaking Algorithm: http://www.unicode.org/reports/tr14/#SA
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.GetText(Lucene.Net.Analysis.TokenAttributes.ICharTermAttribute)">
            <summary>
            Fills ICharTermAttribute with the current token text.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.#ctor(System.IO.TextReader)">
            <summary>
            Creates a new scanner
            </summary>
            <param name="in">the TextReader to read input from.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.ZzUnpackCMap(System.String)">
            <summary>
            Unpacks the compressed character translation table.
            </summary>
            <param name="packed">the packed character translation table</param>
            <returns>the unpacked character translation table</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.ZzRefill">
            <summary>
            Refills the input buffer.
            </summary>
            <returns><c>false</c>, iff there was new input.</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.YyClose">
            <summary>
            Disposes the input stream.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.YyReset(System.IO.TextReader)">
            <summary>
            Resets the scanner to read from a new input stream.
            Does not close the old reader.
            <para/>
            All internal variables are reset, the old input stream 
            <b>cannot</b> be reused (internal buffer is discarded and lost).
            Lexical state is set to <see cref="F:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.YYINITIAL"/>.
            <para/>
            Internal scan buffer is resized down to its initial length, if it has grown.
            </summary>
            <param name="reader">the new input stream </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.YyState">
            <summary>
            Returns the current lexical state.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.YyBegin(System.Int32)">
            <summary>
            Enters a new lexical state
            </summary>
            <param name="newState">the new lexical state</param>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.YyText">
            <summary>
            Returns the text matched by the current regular expression.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.YyCharAt(System.Int32)">
            <summary>
            Returns the character at position <paramref name="pos"/> from the 
            matched text.
            <para/>
            It is equivalent to YyText[pos], but faster
            </summary>
            <param name="pos">
            the position of the character to fetch. 
            A value from 0 to YyLength-1.
            </param>
            <returns>the character at position pos</returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.YyLength">
            <summary>
            Returns the length of the matched text region.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.ZzScanError(System.Int32)">
            <summary>
            Reports an error that occured while scanning.
            <para/>
            In a wellformed scanner (no or only correct usage of 
            YyPushBack(int) and a match-all fallback rule) this method 
            will only be called with things that "Can't Possibly Happen".
            If this method is called, something is seriously wrong
            (e.g. a JFlex bug producing a faulty scanner etc.).
            <para/>
            Usual syntax/scanner level error handling should be done
            in error fallback rules.
            </summary>
            <param name="errorCode">the code of the errormessage to display</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.YyPushBack(System.Int32)">
            <summary>
            Pushes the specified amount of characters back into the input stream.
            <para/>
            They will be read again by then next call of the scanning method
            </summary>
            <param name="number">
            the number of characters to be read again.
            This number must not be greater than YyLength!
            </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Standard.UAX29URLEmailTokenizerImpl.GetNextToken">
            <summary>
            Resumes scanning until the next regular expression is matched,
            the end of input is encountered or an I/O-Error occurs.
            </summary>
            <returns>the next token</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Sv.SwedishAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Swedish.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Sv.SwedishAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Swedish stopwords. </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Sv.SwedishAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Sv.SwedishAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Sv.SwedishAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Sv.SwedishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.Sv.SwedishAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Sv.SwedishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Sv.SwedishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Sv.SwedishAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/>, <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem exclusion set is
                    provided and <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Sv.SwedishLightStemFilter">
            <summary>
            A <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that applies <see cref="T:Lucene.Net.Analysis.Sv.SwedishLightStemmer"/> to stem Swedish
            words.
            <para>
            To prevent terms from being stemmed use an instance of
            <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> or a custom <see cref="T:Lucene.Net.Analysis.TokenFilter"/> that sets
            the <see cref="T:Lucene.Net.Analysis.TokenAttributes.KeywordAttribute"/> before this <see cref="T:Lucene.Net.Analysis.TokenStream"/>.
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Sv.SwedishLightStemFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Sv.SwedishLightStemFilter"/>.
            <code>
            &lt;fieldType name="text_svlgtstem" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.SwedishLightStemFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Sv.SwedishLightStemFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Sv.SwedishLightStemFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Sv.SwedishLightStemmer">
            <summary>
            Light Stemmer for Swedish.
            <para>
            This stemmer implements the algorithm described in:
            <c>Report on CLEF-2003 Monolingual Tracks</c>
            Jacques Savoy
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Synonym.FSTSynonymFilterFactory.LoadSynonyms(Lucene.Net.Analysis.Util.IResourceLoader,System.String,System.Boolean,Lucene.Net.Analysis.Analyzer)">
            <summary>
            Load synonyms with the given <see cref="T:Lucene.Net.Analysis.Synonym.SynonymMap.Parser"/> class.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Synonym.SlowSynonymFilter">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Synonym.SynonymFilter"/> handles multi-token synonyms with variable position increment offsets.
            <para>
            The matched tokens from the input stream may be optionally passed through (includeOrig=true)
            or discarded.  If the original tokens are included, the position increments may be modified
            to retain absolute positions after merging with the synonym tokenstream.
            </para>
            <para>
            Generated synonyms will start at the same position as the first matched source token.
            </para>
            </summary>
            @deprecated (3.4) use SynonymFilterFactory instead. only for precise index backwards compatibility. this factory will be removed in Lucene 5.0 
        </member>
        <member name="T:Lucene.Net.Analysis.Synonym.SlowSynonymFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Synonym.SlowSynonymFilter"/> (only used with luceneMatchVersion &lt; 3.4)
            <code>
            &lt;fieldType name="text_synonym" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="false"
                        expand="true" tokenizerFactory="solr.WhitespaceTokenizerFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
            @deprecated (3.4) use SynonymFilterFactory instead. only for precise index backwards compatibility. this factory will be removed in Lucene 5.0 
        </member>
        <member name="M:Lucene.Net.Analysis.Synonym.SlowSynonymFilterFactory.LoadRules(System.String,Lucene.Net.Analysis.Util.IResourceLoader)">
            <returns> a list of all rules </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Synonym.SlowSynonymFilterFactory.SplitSmart(System.String,System.String,System.Boolean)">
            <summary>
            Splits a backslash escaped string on the separator.
            <para/>
            Current backslash escaping supported:
            <para/> \n \t \r \b \f are escaped the same as a .NET string
            <para/> Other characters following a backslash are produced verbatim (\c => c)
            </summary>
            <param name="s">  the string to split </param>
            <param name="separator"> the separator to split on </param>
            <param name="decode"> decode backslash escaping </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Synonym.SlowSynonymMap">
            <summary>
            Mapping rules for use with <see cref="T:Lucene.Net.Analysis.Synonym.SlowSynonymFilter"/>
            </summary>
            @deprecated (3.4) use <see cref="T:Lucene.Net.Analysis.Synonym.SynonymFilterFactory"/> instead. only for precise index backwards compatibility. this factory will be removed in Lucene 5.0 
        </member>
        <member name="P:Lucene.Net.Analysis.Synonym.SlowSynonymMap.Submap">
            <summary>
            @lucene.internal </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Synonym.SlowSynonymMap.Synonyms">
            <summary>
            @lucene.internal </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Synonym.SlowSynonymMap.Add(System.Collections.Generic.IList{System.String},System.Collections.Generic.IList{Lucene.Net.Analysis.Token},System.Boolean,System.Boolean)">
            <param name="singleMatch">  <see cref="T:System.Collections.Generic.IList`1"/>, the sequence of strings to match </param>
            <param name="replacement">  <see cref="T:System.Collections.Generic.IList`1"/> the list of tokens to use on a match </param>
            <param name="includeOrig">  sets a flag on this mapping signaling the generation of matched tokens in addition to the replacement tokens </param>
            <param name="mergeExisting"> merge the replacement tokens with any other mappings that exist </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Synonym.SlowSynonymMap.MakeTokens(System.Collections.Generic.IList{System.String})">
            <summary>
            Produces a <see cref="T:System.Collections.Generic.IList`1"/> from a <see cref="T:System.Collections.Generic.IList`1"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Synonym.SlowSynonymMap.MergeTokens(System.Collections.Generic.IList{Lucene.Net.Analysis.Token},System.Collections.Generic.IList{Lucene.Net.Analysis.Token})">
            <summary>
            Merge two lists of tokens, producing a single list with manipulated positionIncrements so that
            the tokens end up at the same position.
            
            Example:  [a b] merged with [c d] produces [a/b c/d]  ('/' denotes tokens in the same position)
            Example:  [a,5 b,2] merged with [c d,4 e,4] produces [c a,5/d b,2 e,2]  (a,n means a has posInc=n)
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Synonym.SolrSynonymParser">
            <summary>
            Parser for the Solr synonyms format.
            <list type="bullet">
                <item><description> Blank lines and lines starting with '#' are comments.</description></item>
                <item><description> Explicit mappings match any token sequence on the LHS of "=>"
                    and replace with all alternatives on the RHS.  These types of mappings
                    ignore the expand parameter in the constructor.
                    Example:
                    <code>i-pod, i pod => ipod</code>
                </description></item>
                <item><description> Equivalent synonyms may be separated with commas and give
                    no explicit mapping.  In this case the mapping behavior will
                    be taken from the expand parameter in the constructor.  This allows
                    the same synonym file to be used in different synonym handling strategies.
                    Example:
                    <code>ipod, i-pod, i pod</code>
                </description></item>
                <item><description> Multiple synonym mapping entries are merged.
                    Example:
                    <code>
                        foo => foo bar
                        foo => baz
                        is equivalent to
                        foo => foo bar, baz
                    </code>
                </description></item>
            </list>
            @lucene.experimental
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Synonym.SynonymFilter">
            <summary>
            Matches single or multi word synonyms in a token stream.
            This token stream cannot properly handle position
            increments != 1, ie, you should place this filter before
            filtering out stop words.
            
            <para>Note that with the current implementation, parsing is
            greedy, so whenever multiple parses would apply, the rule
            starting the earliest and parsing the most tokens wins.
            For example if you have these rules:
                 
            <code>
              a -> x
              a b -> y
              b c d -> z
            </code>
            
            Then input <c>a b c d e</c> parses to <c>y b c
            d</c>, ie the 2nd rule "wins" because it started
            earliest and matched the most input tokens of other rules
            starting at that point.</para>
            
            <para>A future improvement to this filter could allow
            non-greedy parsing, such that the 3rd rule would win, and
            also separately allow multiple parses, such that all 3
            rules would match, perhaps even on a rule by rule
            basis.</para>
            
            <para><b>NOTE</b>: when a match occurs, the output tokens
            associated with the matching rule are "stacked" on top of
            the input stream (if the rule had
            <c>keepOrig=true</c>) and also on top of another
            matched rule's output tokens.  This is not a correct
            solution, as really the output should be an arbitrary
            graph/lattice.  For example, with the above match, you
            would expect an exact <see cref="T:Lucene.Net.Search.PhraseQuery"/> <c>"y b
            c"</c> to match the parsed tokens, but it will fail to
            do so.  This limitation is necessary because Lucene's
            <see cref="T:Lucene.Net.Analysis.TokenStream"/> (and index) cannot yet represent an arbitrary
            graph.</para>
            
            <para><b>NOTE</b>: If multiple incoming tokens arrive on the
            same position, only the first token at that position is
            used for parsing.  Subsequent tokens simply pass through
            and are not parsed.  A future improvement would be to
            allow these tokens to also be matched.</para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Synonym.SynonymFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Synonym.SynonymMap,System.Boolean)">
            <param name="input"> input tokenstream </param>
            <param name="synonyms"> synonym map </param>
            <param name="ignoreCase"> case-folds input for matching with <see cref="M:J2N.Character.ToLower(System.Int32)"/>.
                              Note, if you set this to true, its your responsibility to lowercase
                              the input entries when you create the <see cref="T:Lucene.Net.Analysis.Synonym.SynonymMap"/> </param>
        </member>
        <member name="T:Lucene.Net.Analysis.Synonym.SynonymFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Synonym.SynonymFilter"/>.
            <code>
            &lt;fieldType name="text_synonym" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
                &lt;filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" 
                        format="solr" ignoreCase="false" expand="true" 
                        tokenizerFactory="solr.WhitespaceTokenizerFactory"
                        [optional tokenizer factory parameters]/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            
            <para>
            An optional param name prefix of "tokenizerFactory." may be used for any 
            init params that the <see cref="T:Lucene.Net.Analysis.Synonym.SynonymFilterFactory"/> needs to pass to the specified 
            <see cref="T:Lucene.Net.Analysis.Util.TokenizerFactory"/>.  If the <see cref="T:Lucene.Net.Analysis.Util.TokenizerFactory"/> expects an init parameters with 
            the same name as an init param used by the <see cref="T:Lucene.Net.Analysis.Synonym.SynonymFilterFactory"/>, the prefix 
            is mandatory.
            </para>
            <para>
            The optional <c>format</c> parameter controls how the synonyms will be parsed:
            It supports the short names of <c>solr</c> for <see cref="T:Lucene.Net.Analysis.Synonym.SolrSynonymParser"/> 
            and <c>wordnet</c> for and <see cref="T:Lucene.Net.Analysis.Synonym.WordnetSynonymParser"/>, or your own 
            <see cref="T:Lucene.Net.Analysis.Synonym.SynonymMap.Parser"/> class name. The default is <c>solr</c>.
            A custom <see cref="T:Lucene.Net.Analysis.Synonym.SynonymMap.Parser"/> is expected to have a constructor taking:
            <list type="bullet">
                <item><description><c><see cref="T:System.Boolean"/> dedup</c> - true if duplicates should be ignored, false otherwise</description></item>
                <item><description><c><see cref="T:System.Boolean"/> expand</c> - true if conflation groups should be expanded, false if they are one-directional</description></item>
                <item><description><c><see cref="T:Lucene.Net.Analysis.Analyzer"/> analyzer</c> - an analyzer used for each raw synonym</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Synonym.SynonymFilterFactory.Delegator">
            <summary>
            Access to the delegator <see cref="T:Lucene.Net.Analysis.Util.TokenFilterFactory"/> for test verification
            </summary>
            @deprecated Method exists only for testing 4x, will be removed in 5.0
            @lucene.internal 
        </member>
        <member name="T:Lucene.Net.Analysis.Synonym.SynonymMap">
            <summary>
            A map of synonyms, keys and values are phrases.
            @lucene.experimental
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Synonym.SynonymMap.WORD_SEPARATOR">
            <summary>
            for multiword support, you must separate words with this separator </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Synonym.SynonymMap.Fst">
            <summary>
            map&lt;input word, list&lt;ord&gt;&gt; </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Synonym.SynonymMap.Words">
            <summary>
            map&lt;ord, outputword&gt; </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Synonym.SynonymMap.MaxHorizontalContext">
            <summary>
            maxHorizontalContext: maximum context we need on the tokenstream </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Synonym.SynonymMap.Builder">
            <summary>
            Builds an FSTSynonymMap.
            <para>
            Call <see cref="M:Lucene.Net.Analysis.Synonym.SynonymMap.Builder.Add(Lucene.Net.Util.CharsRef,Lucene.Net.Util.CharsRef,System.Boolean)"/> until you have added all the mappings, then call <see cref="M:Lucene.Net.Analysis.Synonym.SynonymMap.Builder.Build"/> to get an FSTSynonymMap
            @lucene.experimental
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Synonym.SynonymMap.Builder.#ctor(System.Boolean)">
            <summary>
            If dedup is true then identical rules (same input,
             same output) will be added only once. 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Synonym.SynonymMap.Builder.Join(System.String[],Lucene.Net.Util.CharsRef)">
            <summary>
            Sugar: just joins the provided terms with 
            <see cref="F:Lucene.Net.Analysis.Synonym.SynonymMap.WORD_SEPARATOR"/>. reuse and its chars
            must not be null. 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Synonym.SynonymMap.Builder.HasHoles(Lucene.Net.Util.CharsRef)">
            <summary>
            only used for asserting!
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Synonym.SynonymMap.Builder.Add(Lucene.Net.Util.CharsRef,Lucene.Net.Util.CharsRef,System.Boolean)">
            <summary>
            Add a phrase->phrase synonym mapping.
            Phrases are character sequences where words are
            separated with character zero (U+0000).  Empty words
            (two U+0000s in a row) are not allowed in the input nor
            the output!
            </summary>
            <param name="input"> input phrase </param>
            <param name="output"> output phrase </param>
            <param name="includeOrig"> true if the original should be included </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Synonym.SynonymMap.Builder.Build">
            <summary>
            Builds an <see cref="T:Lucene.Net.Analysis.Synonym.SynonymMap"/> and returns it.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Synonym.SynonymMap.Parser">
            <summary>
            Abstraction for parsing synonym files.
            
            @lucene.experimental
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Synonym.SynonymMap.Parser.Parse(System.IO.TextReader)">
            <summary>
            Parse the given input, adding synonyms to the inherited <see cref="T:Lucene.Net.Analysis.Synonym.SynonymMap.Builder"/>. </summary>
            <param name="in"> The input to parse </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Synonym.SynonymMap.Parser.Analyze(System.String,Lucene.Net.Util.CharsRef)">
            <summary>
            Sugar: analyzes the text with the analyzer and
            separates by <see cref="F:Lucene.Net.Analysis.Synonym.SynonymMap.WORD_SEPARATOR"/>.
            reuse and its chars must not be null. 
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Synonym.WordnetSynonymParser">
            <summary>
            Parser for wordnet prolog format
            <para>
            See http://wordnet.princeton.edu/man/prologdb.5WN.html for a description of the format.
            @lucene.experimental
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Tr.ApostropheFilter">
            <summary>
            Strips all characters after an apostrophe (including the apostrophe itself).
            <para>
            In Turkish, apostrophe is used to separate suffixes from proper names
            (continent, sea, river, lake, mountain, upland, proper names related to
            religion and mythology). This filter intended to be used before stem filters.
            For more information, see <a href="http://www.ipcsit.com/vol57/015-ICNI2012-M021.pdf">
            Role of Apostrophes in Turkish Information Retrieval</a>
            </para>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Tr.ApostropheFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Tr.ApostropheFilter"/>.
            <code>
            &lt;fieldType name="text_tr_lower_apostrophes" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.ApostropheFilterFactory"/&gt;
                &lt;filter class="solr.TurkishLowerCaseFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Tr.ApostropheFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Tr.ApostropheFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Tr.TurkishAnalyzer">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> for Turkish.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Tr.TurkishAnalyzer.DEFAULT_STOPWORD_FILE">
            <summary>
            File containing default Turkish stopwords. </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Tr.TurkishAnalyzer.STOPWORDS_COMMENT">
            <summary>
            The comment character in the stopwords file.  
            All lines prefixed with this will be ignored.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Tr.TurkishAnalyzer.DefaultStopSet">
            <summary>
            Returns an unmodifiable instance of the default stop words set. </summary>
            <returns> default stop words set. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Tr.TurkishAnalyzer.DefaultSetHolder">
            <summary>
            Atomically loads the <see cref="F:Lucene.Net.Analysis.Tr.TurkishAnalyzer.DefaultSetHolder.DEFAULT_STOP_SET"/> in a lazy fashion once the outer class 
            accesses the static final set the first time.;
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Tr.TurkishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Builds an analyzer with the default stop words: <see cref="F:Lucene.Net.Analysis.Tr.TurkishAnalyzer.DEFAULT_STOPWORD_FILE"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Tr.TurkishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Tr.TurkishAnalyzer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Builds an analyzer with the given stop words. If a non-empty stem exclusion set is
            provided this analyzer will add a <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> before
            stemming.
            </summary>
            <param name="matchVersion"> lucene compatibility version </param>
            <param name="stopwords"> a stopword set </param>
            <param name="stemExclusionSet"> a set of terms not to be stemmed </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Tr.TurkishAnalyzer.CreateComponents(System.String,System.IO.TextReader)">
            <summary>
            Creates a
            <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
            which tokenizes all the text in the provided <see cref="T:System.IO.TextReader"/>.
            </summary>
            <returns> A
                    <see cref="T:Lucene.Net.Analysis.TokenStreamComponents"/>
                    built from an <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> filtered with
                    <see cref="T:Lucene.Net.Analysis.Standard.StandardFilter"/>, <see cref="T:Lucene.Net.Analysis.Tr.TurkishLowerCaseFilter"/>,
                    <see cref="T:Lucene.Net.Analysis.Core.StopFilter"/>, <see cref="T:Lucene.Net.Analysis.Miscellaneous.SetKeywordMarkerFilter"/> if a stem
                    exclusion set is provided and <see cref="T:Lucene.Net.Analysis.Snowball.SnowballFilter"/>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Tr.TurkishLowerCaseFilter">
            <summary>
            Normalizes Turkish token text to lower case.
            <para>
            Turkish and Azeri have unique casing behavior for some characters. This
            filter applies Turkish lowercase rules. For more information, see <a
            href="http://en.wikipedia.org/wiki/Turkish_dotted_and_dotless_I"
            >http://en.wikipedia.org/wiki/Turkish_dotted_and_dotless_I</a>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Tr.TurkishLowerCaseFilter.#ctor(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Tr.TurkishLowerCaseFilter"/>, that normalizes Turkish token text 
            to lower case.
            </summary>
            <param name="in"> <see cref="T:Lucene.Net.Analysis.TokenStream"/> to filter </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Tr.TurkishLowerCaseFilter.IsBeforeDot(System.Char[],System.Int32,System.Int32)">
            <summary>
            lookahead for a combining dot above.
            other NSMs may be in between.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Tr.TurkishLowerCaseFilter.Delete(System.Char[],System.Int32,System.Int32)">
            <summary>
            delete a character in-place.
            rarely happens, only if <see cref="F:Lucene.Net.Analysis.Tr.TurkishLowerCaseFilter.COMBINING_DOT_ABOVE"/> is found after an i
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Tr.TurkishLowerCaseFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Tr.TurkishLowerCaseFilter"/>.
            <code>
            &lt;fieldType name="text_trlwr" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.TurkishLowerCaseFilterFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Tr.TurkishLowerCaseFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Tr.TurkishLowerCaseFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.AbstractAnalysisFactory">
            <summary>
            Abstract parent class for analysis factories <see cref="T:Lucene.Net.Analysis.Util.TokenizerFactory"/>,
            <see cref="T:Lucene.Net.Analysis.Util.TokenFilterFactory"/> and <see cref="T:Lucene.Net.Analysis.Util.CharFilterFactory"/>.
            <para>
            The typical lifecycle for a factory consumer is:
            <list type="bullet">
                <item><description>Create factory via its constructor (or via XXXFactory.ForName)</description></item>
                <item><description>(Optional) If the factory uses resources such as files, 
                    <see cref="M:Lucene.Net.Analysis.Util.IResourceLoaderAware.Inform(Lucene.Net.Analysis.Util.IResourceLoader)"/> is called to initialize those resources.</description></item>
                <item><description>Consumer calls create() to obtain instances.</description></item>
            </list>
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.originalArgs">
            <summary>
            The original args, before any processing </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.m_luceneMatchVersion">
            <summary>
            the luceneVersion arg </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Initialize this factory via a set of key-value pairs.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.AssureMatchVersion">
            <summary>
            this method can be called in the <see cref="M:Lucene.Net.Analysis.Util.TokenizerFactory.Create(System.IO.TextReader)"/>
            or <see cref="M:Lucene.Net.Analysis.Util.TokenFilterFactory.Create(Lucene.Net.Analysis.TokenStream)"/> methods,
            to inform user, that for this factory a <see cref="F:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.m_luceneMatchVersion"/> is required 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.RequireInt32(System.Collections.Generic.IDictionary{System.String,System.String},System.String)">
            <summary>
            NOTE: This was requireInt() in Lucene
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.GetInt32(System.Collections.Generic.IDictionary{System.String,System.String},System.String,System.Int32)">
            <summary>
            NOTE: This was getInt() in Lucene
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.RequireSingle(System.Collections.Generic.IDictionary{System.String,System.String},System.String)">
            <summary>
            NOTE: This was requireFloat() in Lucene
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.GetSingle(System.Collections.Generic.IDictionary{System.String,System.String},System.String,System.Single)">
            <summary>
            NOTE: This was getFloat() in Lucene
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.GetSet(System.Collections.Generic.IDictionary{System.String,System.String},System.String)">
            <summary>
            Returns whitespace- and/or comma-separated set of values, or null if none are found </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.GetPattern(System.Collections.Generic.IDictionary{System.String,System.String},System.String)">
            <summary>
            Compiles a pattern for the value of the specified argument key <paramref name="name"/> 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.GetCulture(System.Collections.Generic.IDictionary{System.String,System.String},System.String,System.Globalization.CultureInfo)">
            <summary>
            Gets a <see cref="T:System.Globalization.CultureInfo"/> value of the specified argument key <paramref name="name"/>.
            <para/>
            To specify the invariant culture, pass the string <c>"invariant"</c>.
            <para/>
            LUCENENET specific
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.GetWordSet(Lucene.Net.Analysis.Util.IResourceLoader,System.String,System.Boolean)">
            <summary>
            Returns as <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> from wordFiles, which
            can be a comma-separated list of filenames
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.GetLines(Lucene.Net.Analysis.Util.IResourceLoader,System.String)">
            <summary>
            Returns the resource's lines (with content treated as UTF-8)
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.GetSnowballWordSet(Lucene.Net.Analysis.Util.IResourceLoader,System.String,System.Boolean)">
            <summary>
            Same as <see cref="M:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.GetWordSet(Lucene.Net.Analysis.Util.IResourceLoader,System.String,System.Boolean)"/>,
            except the input is in snowball format. 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.SplitFileNames(System.String)">
            <summary>
            Splits file names separated by comma character.
            File names can contain comma characters escaped by backslash '\'
            </summary>
            <param name="fileNames"> the string containing file names </param>
            <returns> a list of file names with the escaping backslashed removed </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.AbstractAnalysisFactory.GetClassArg">
            <returns> the string used to specify the concrete class name in a serialized representation: the class arg.  
                    If the concrete class name was not specified via a class arg, returns <c>GetType().Name</c>. </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.AnalysisSPILoader`1">
            <summary>
            Helper class for loading named SPIs from classpath (e.g. Tokenizers, TokenStreams).
            @lucene.internal
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.AnalysisSPILoader`1.Reload">
             <summary>
             Reloads the internal SPI list.
             Changes to the service list are visible after the method ends, all
             iterators (e.g, from <see cref="P:Lucene.Net.Analysis.Util.AnalysisSPILoader`1.AvailableServices"/>,...) stay consistent.
            
             <para/><b>NOTE:</b> Only new service providers are added, existing ones are
             never removed or replaced.
            
             <para/><em>this method is expensive and should only be called for discovery
             of new service providers on the given classpath/classloader!</em>
             </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.BufferedCharFilter">
            <summary>
            LUCENENET specific class to mimic Java's BufferedReader (that is, a reader that is seekable) 
            so it supports Mark() and Reset() (which are part of the Java Reader class), but also 
            provide the Correct() method of BaseCharFilter.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Util.BufferedCharFilter.m_lock">
            <summary>
            The object used to synchronize access to the reader.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Util.BufferedCharFilter.buf">
             <summary>
             The characters that can be read and refilled in bulk. We maintain three
             indices into this buffer:
             <code>
             { X X X X X X X X X X X X - - }
             ^     ^             ^
             |     |             |
             mark   pos end
             </code>
             Pos points to the next readable character.End is one greater than the
             last readable character.When<c> pos == end</c>, the buffer is empty and
             must be <see cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.FillBuf"/> before characters can be read.
            
             <para/> Mark is the value pos will be set to on calls to 
             <see cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Reset"/>. Its value is in the range <c>[0...pos]</c>. If the mark is <c>-1</c>, the
             buffer cannot be reset.
             
             <para/> MarkLimit limits the distance between the mark and the pos.When this
             limit is exceeded, <see cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Reset"/> is permitted (but not required) to
             throw an exception. For shorter distances, <see cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Reset"/> shall not throw
             (unless the reader is closed).
             </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Util.BufferedCharFilter.isDisposing">
            <summary>
            LUCENENET specific to throw an exception if the user calls <see cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Close"/> instead of <see cref="M:System.IO.TextReader.Dispose"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.BufferedCharFilter.#ctor(System.IO.TextReader)">
            <summary>
            Creates a buffering character-input stream that uses a default-sized input buffer.
            </summary>
            <param name="in">A TextReader</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.BufferedCharFilter.#ctor(System.IO.TextReader,System.Int32)">
            <summary>
            Creates a buffering character-input stream that uses an input buffer of the specified size.
            </summary>
            <param name="in">A TextReader</param>
            <param name="size">Input-buffer size</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Dispose(System.Boolean)">
            <summary>
            Disposes this reader. This implementation closes the buffered source reader
            and releases the buffer. Nothing is done if this reader has already been
            disposed.
            </summary>
            <param name="disposing"></param>
            <exception cref="T:System.IO.IOException">if an error occurs while closing this reader.</exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.BufferedCharFilter.FillBuf">
            <summary>
            Populates the buffer with data. It is an error to call this method when
            the buffer still contains data; ie. if <c>pos &lt; end</c>.
            </summary>
            <returns>
            the number of bytes read into the buffer, or -1 if the end of the
            source stream has been reached.
            </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.BufferedCharFilter.EnsureOpen">
            <summary>
            Checks to make sure that the stream has not been closed
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.BufferedCharFilter.IsClosed">
            <summary>
            Indicates whether or not this reader is closed.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Mark(System.Int32)">
            <summary>
            Sets a mark position in this reader. The parameter <paramref name="markLimit"/>
            indicates how many characters can be read before the mark is invalidated.
            Calling <see cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Reset"/> will reposition the reader back to the marked
            position if <see cref="F:Lucene.Net.Analysis.Util.BufferedCharFilter.markLimit"/> has not been surpassed.
            </summary>
            <param name="markLimit">
            the number of characters that can be read before the mark is
            invalidated.
            </param>
            <exception cref="T:System.ArgumentOutOfRangeException">if <c>markLimit &lt; 0</c></exception>
            <exception cref="T:System.IO.IOException">if an error occurs while setting a mark in this reader.</exception>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.BufferedCharFilter.IsMarkSupported">
            <summary>
            Indicates whether this reader supports the <see cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Mark(System.Int32)"/> and
            <see cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Reset"/> methods. This implementation returns <c>true</c>.
            </summary>
            <seealso cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Mark(System.Int32)"/>
            <seealso cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Reset"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Read">
            <summary>
            Reads a single character from this reader and returns it with the two
            higher-order bytes set to 0. If possible, <see cref="T:Lucene.Net.Analysis.Util.BufferedCharFilter"/> returns a
            character from the buffer. If there are no characters available in the
            buffer, it fills the buffer and then returns a character. It returns -1
            if there are no more characters in the source reader.
            </summary>
            <returns>The character read or -1 if the end of the source reader has been reached.</returns>
            <exception cref="T:System.IO.IOException">If this reader is disposed or some other I/O error occurs.</exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Read(System.Char[],System.Int32,System.Int32)">
            <summary>
            Reads at most <paramref name="length"/> characters from this reader and stores them
            at <paramref name="offset"/> in the character array <paramref name="buffer"/>. Returns the
            number of characters actually read or -1 if the end of the source reader
            has been reached. If all the buffered characters have been used, a mark
            has not been set and the requested number of characters is larger than
            this readers buffer size, BufferedReader bypasses the buffer and simply
            places the results directly into <paramref name="buffer"/>.
            </summary>
            <param name="buffer">the character array to store the characters read.</param>
            <param name="offset">the initial position in <paramref name="buffer"/> to store the bytes read from this reader.</param>
            <param name="length">the maximum number of characters to read, must be non-negative.</param>
            <returns>number of characters read or -1 if the end of the source reader has been reached.</returns>
            <exception cref="T:System.ArgumentOutOfRangeException">
            if <c>offset &lt; 0</c> or <c>length &lt; 0</c>, or if
            <c>offset + length</c> is greater than the size of
            <paramref name="buffer"/>.
            </exception>
            <exception cref="T:System.IO.IOException">if this reader is disposed or some other I/O error occurs.</exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.BufferedCharFilter.ChompNewline">
            <summary>
            Peeks at the next input character, refilling the buffer if necessary. If
            this character is a newline character ("\n"), it is discarded.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.BufferedCharFilter.ReadLine">
            <summary>
            Returns the next line of text available from this reader. A line is
            represented by zero or more characters followed by <c>'\n'</c>,
            <c>'\r'</c>, <c>"\r\n"</c> or the end of the reader. The string does
            not include the newline sequence.
            </summary>
            <returns>The contents of the line or <c>null</c> if no characters were 
            read before the end of the reader has been reached.</returns>
            <exception cref="T:System.IO.IOException">if this reader is disposed or some other I/O error occurs.</exception>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.BufferedCharFilter.IsReady">
            <summary>
            Indicates whether this reader is ready to be read without blocking.
            </summary>
            <returns>
            <c>true</c> if this reader will not block when <see cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Read"/> is
            called, <c>false</c> if unknown or blocking will occur.
            </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Reset">
            <summary>
            Resets this reader's position to the last <see cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Mark(System.Int32)"/> location.
            Invocations of <see cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Read"/> and <see cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Skip(System.Int32)"/> will occur from this new
            location.
            </summary>
            <exception cref="T:System.IO.IOException">If this reader is disposed or no mark has been set.</exception>
            <seealso cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Mark(System.Int32)"/>
            <seealso cref="P:Lucene.Net.Analysis.Util.BufferedCharFilter.IsMarkSupported"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Skip(System.Int32)">
            <summary>
            Skips <paramref name="amount"/> characters in this reader. Subsequent
            <see cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Read"/>s will not return these characters unless <see cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Reset"/>
            is used. Skipping characters may invalidate a mark if <see cref="F:Lucene.Net.Analysis.Util.BufferedCharFilter.markLimit"/>
            is surpassed.
            </summary>
            <param name="amount">the maximum number of characters to skip.</param>
            <returns>the number of characters actually skipped.</returns>
            <exception cref="T:System.ArgumentOutOfRangeException">if <c>amount &lt; 0</c>.</exception>
            <exception cref="T:System.IO.IOException">If this reader is disposed or some other I/O error occurs.</exception>
            <seealso cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Mark(System.Int32)"/>
            <seealso cref="P:Lucene.Net.Analysis.Util.BufferedCharFilter.IsMarkSupported"/>
            <seealso cref="M:Lucene.Net.Analysis.Util.BufferedCharFilter.Reset"/>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.CharacterUtils">
            <summary>
            <see cref="T:Lucene.Net.Analysis.Util.CharacterUtils"/> provides a unified interface to Character-related
            operations to implement backwards compatible character operations based on a
            <see cref="T:Lucene.Net.Util.LuceneVersion"/> instance.
            
            @lucene.internal
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharacterUtils.GetInstance(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Returns a <see cref="T:Lucene.Net.Analysis.Util.CharacterUtils"/> implementation according to the given
            <see cref="T:Lucene.Net.Util.LuceneVersion"/> instance.
            </summary>
            <param name="matchVersion">
                     a version instance </param>
            <returns> a <see cref="T:Lucene.Net.Analysis.Util.CharacterUtils"/> implementation according to the given
                    <see cref="T:Lucene.Net.Util.LuceneVersion"/> instance. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharacterUtils.GetJava4Instance(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Return a <see cref="T:Lucene.Net.Analysis.Util.CharacterUtils"/> instance compatible with Java 1.4. </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharacterUtils.CodePointAt(System.String,System.Int32)">
            <summary>
            Returns the code point at the given index of the <see cref="T:System.String"/>.
            Depending on the <see cref="T:Lucene.Net.Util.LuceneVersion"/> passed to
            <see cref="M:Lucene.Net.Analysis.Util.CharacterUtils.GetInstance(Lucene.Net.Util.LuceneVersion)"/> this method mimics the behavior
            of <c>Character.CodePointAt(char[], int)</c> as it would have been
            available on a Java 1.4 JVM or on a later virtual machine version.
            </summary>
            <param name="seq">
                     a character sequence </param>
            <param name="offset">
                     the offset to the char values in the chars array to be converted
            </param>
            <returns> the Unicode code point at the given index </returns>
            <exception cref="T:System.NullReferenceException">
                      - if the sequence is null. </exception>
            <exception cref="T:System.ArgumentOutOfRangeException">
                      - if the value offset is negative or not less than the length of
                      the character sequence. </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharacterUtils.CodePointAt(J2N.Text.ICharSequence,System.Int32)">
            <summary>
            Returns the code point at the given index of the <see cref="T:J2N.Text.ICharSequence"/>.
            Depending on the <see cref="T:Lucene.Net.Util.LuceneVersion"/> passed to
            <see cref="M:Lucene.Net.Analysis.Util.CharacterUtils.GetInstance(Lucene.Net.Util.LuceneVersion)"/> this method mimics the behavior
            of <c>Character.CodePointAt(char[], int)</c> as it would have been
            available on a Java 1.4 JVM or on a later virtual machine version.
            </summary>
            <param name="seq">
                     a character sequence </param>
            <param name="offset">
                     the offset to the char values in the chars array to be converted
            </param>
            <returns> the Unicode code point at the given index </returns>
            <exception cref="T:System.NullReferenceException">
                      - if the sequence is null. </exception>
            <exception cref="T:System.ArgumentOutOfRangeException">
                      - if the value offset is negative or not less than the length of
                      the character sequence. </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharacterUtils.CodePointAt(System.Char[],System.Int32,System.Int32)">
            <summary>
            Returns the code point at the given index of the char array where only elements
            with index less than the limit are used.
            Depending on the <see cref="T:Lucene.Net.Util.LuceneVersion"/> passed to
            <see cref="M:Lucene.Net.Analysis.Util.CharacterUtils.GetInstance(Lucene.Net.Util.LuceneVersion)"/> this method mimics the behavior
            of <c>Character.CodePointAt(char[], int)</c> as it would have been
            available on a Java 1.4 JVM or on a later virtual machine version.
            </summary>
            <param name="chars">
                     a character array </param>
            <param name="offset">
                     the offset to the char values in the chars array to be converted </param>
            <param name="limit"> the index afer the last element that should be used to calculate
                   codepoint.  
            </param>
            <returns> the Unicode code point at the given index </returns>
            <exception cref="T:System.NullReferenceException">
                      - if the array is null. </exception>
            <exception cref="T:System.ArgumentOutOfRangeException">
                      - if the value offset is negative or not less than the length of
                      the char array. </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharacterUtils.CodePointCount(System.String)">
            <summary>
            Return the number of characters in <paramref name="seq"/>. </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharacterUtils.NewCharacterBuffer(System.Int32)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Util.CharacterUtils.CharacterBuffer"/> and allocates a <see cref="T:char[]"/>
            of the given bufferSize.
            </summary>
            <param name="bufferSize">
                     the internal char buffer size, must be <c>&gt;= 2</c> </param>
            <returns> a new <see cref="T:Lucene.Net.Analysis.Util.CharacterUtils.CharacterBuffer"/> instance. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharacterUtils.ToLower(System.Char[],System.Int32,System.Int32)">
            <summary>
            Converts each unicode codepoint to lowerCase via <see cref="M:J2N.Character.ToLower(System.Int32)"/> starting 
            at the given offset. </summary>
            <param name="buffer"> the char buffer to lowercase </param>
            <param name="offset"> the offset to start at </param>
            <param name="length"> the number of characters in the buffer to lower case </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharacterUtils.ToUpper(System.Char[],System.Int32,System.Int32)">
            <summary>
            Converts each unicode codepoint to UpperCase via <see cref="M:J2N.Character.ToUpper(System.Int32)"/> starting 
            at the given offset. </summary>
            <param name="buffer"> the char buffer to UPPERCASE </param>
            <param name="offset"> the offset to start at </param>
            <param name="length"> the number of characters in the buffer to lower case </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharacterUtils.ToCodePoints(System.Char[],System.Int32,System.Int32,System.Int32[],System.Int32)">
            <summary>
            Converts a sequence of .NET characters to a sequence of unicode code points. </summary>
             <returns> the number of code points written to the destination buffer  </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharacterUtils.ToChars(System.Int32[],System.Int32,System.Int32,System.Char[],System.Int32)">
            <summary>
            Converts a sequence of unicode code points to a sequence of .NET characters. </summary>
             <returns> the number of chars written to the destination buffer  </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharacterUtils.Fill(Lucene.Net.Analysis.Util.CharacterUtils.CharacterBuffer,System.IO.TextReader,System.Int32)">
            <summary>
            Fills the <see cref="T:Lucene.Net.Analysis.Util.CharacterUtils.CharacterBuffer"/> with characters read from the given
            reader <see cref="T:System.IO.TextReader"/>. This method tries to read <code>numChars</code>
            characters into the <see cref="T:Lucene.Net.Analysis.Util.CharacterUtils.CharacterBuffer"/>, each call to fill will start
            filling the buffer from offset <c>0</c> up to <paramref name="numChars"/>.
            In case code points can span across 2 java characters, this method may
            only fill <c>numChars - 1</c> characters in order not to split in
            the middle of a surrogate pair, even if there are remaining characters in
            the <see cref="T:System.IO.TextReader"/>.
            <para>
            Depending on the <see cref="T:Lucene.Net.Util.LuceneVersion"/> passed to
            <see cref="M:Lucene.Net.Analysis.Util.CharacterUtils.GetInstance(Lucene.Net.Util.LuceneVersion)"/> this method implements
            supplementary character awareness when filling the given buffer. For all
            <see cref="T:Lucene.Net.Util.LuceneVersion"/> &gt; 3.0 <see cref="M:Lucene.Net.Analysis.Util.CharacterUtils.Fill(Lucene.Net.Analysis.Util.CharacterUtils.CharacterBuffer,System.IO.TextReader,System.Int32)"/> guarantees
            that the given <see cref="T:Lucene.Net.Analysis.Util.CharacterUtils.CharacterBuffer"/> will never contain a high surrogate
            character as the last element in the buffer unless it is the last available
            character in the reader. In other words, high and low surrogate pairs will
            always be preserved across buffer boarders.
            </para>
            <para>
            A return value of <c>false</c> means that this method call exhausted
            the reader, but there may be some bytes which have been read, which can be
            verified by checking whether <c>buffer.Length &gt; 0</c>.
            </para>
            </summary>
            <param name="buffer">
                     the buffer to fill. </param>
            <param name="reader">
                     the reader to read characters from. </param>
            <param name="numChars">
                     the number of chars to read </param>
            <returns> <code>false</code> if and only if reader.read returned -1 while trying to fill the buffer </returns>
            <exception cref="T:System.IO.IOException">
                      if the reader throws an <see cref="T:System.IO.IOException"/>. </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharacterUtils.Fill(Lucene.Net.Analysis.Util.CharacterUtils.CharacterBuffer,System.IO.TextReader)">
            <summary>
            Convenience method which calls <c>Fill(buffer, reader, buffer.Buffer.Length)</c>. </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharacterUtils.OffsetByCodePoints(System.Char[],System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Return the index within <c>buf[start:start+count]</c> which is by <paramref name="offset"/>
            code points from <paramref name="index"/>. 
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.CharacterUtils.CharacterBuffer">
            <summary>
            A simple IO buffer to use with
            <see cref="M:Lucene.Net.Analysis.Util.CharacterUtils.Fill(Lucene.Net.Analysis.Util.CharacterUtils.CharacterBuffer,System.IO.TextReader)"/>.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharacterUtils.CharacterBuffer.Buffer">
            <summary>
            Returns the internal buffer
            </summary>
            <returns> the buffer </returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharacterUtils.CharacterBuffer.Offset">
            <summary>
            Returns the data offset in the internal buffer.
            </summary>
            <returns> the offset </returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharacterUtils.CharacterBuffer.Length">
            <summary>
            Return the length of the data in the internal buffer starting at
            <see cref="P:Lucene.Net.Analysis.Util.CharacterUtils.CharacterBuffer.Offset"/>
            </summary>
            <returns> the length </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharacterUtils.CharacterBuffer.Reset">
            <summary>
            Resets the CharacterBuffer. All internals are reset to its default
            values.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.CharArrayMap`1">
            <summary>
            A simple class that stores key <see cref="T:System.String"/>s as <see cref="T:char[]"/>'s in a
            hash table. Note that this is not a general purpose
            class.  For example, it cannot remove items from the
            map, nor does it resize its hash table to be smaller,
            etc.  It is designed to be quick to retrieve items
            by <see cref="T:char[]"/> keys without the necessity of converting
            to a <see cref="T:System.String"/> first.
            
            <a name="version"></a>
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap"/>:
            <list type="bullet">
              <item><description> As of 3.1, supplementary characters are
                  properly lowercased.</description></item>
            </list>
            Before 3.1 supplementary characters could not be
            lowercased correctly due to the lack of Unicode 4
            support in JDK 1.4. To use instances of
            <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap"/> with the behavior before Lucene
            3.1 pass a <see cref="T:Lucene.Net.Util.LuceneVersion"/> &lt; 3.1 to the constructors.
            </para>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Util.CharArrayMap`1.PLACEHOLDER">
            <summary>
            LUCENENET: Moved this from CharArraySet so it doesn't need to know the generic type of CharArrayMap
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.CharArrayMap`1.MapValue">
            <summary>
            LUCENENET SPECIFIC type used to act as a placeholder. Since <c>null</c>
            means that our value is not populated, we need an instance of something
            to indicate it is. Using an instance of <typeparamref name="TValue"/> would only work if
            we could constrain it with the new() constraint, which isn't possible because
            some types such as <see cref="T:System.String"/> don't have a default constructor.
            So, this is a workaround that allows any type regardless of the type of constructor.
            
            <para>
            Note also that we gain the ability to use value types for <typeparamref name="TValue"/>, but
            also create a difference in behavior from Java Lucene where the actual values 
            returned could be <c>null</c>.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.#ctor(Lucene.Net.Util.LuceneVersion,System.Int32,System.Boolean)">
            <summary>
            Create map with enough capacity to hold <paramref name="startSize"/> terms
            </summary>
            <param name="matchVersion">
                     lucene compatibility version - see <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> for details. </param>
            <param name="startSize">
                     the initial capacity </param>
            <param name="ignoreCase">
                     <c>false</c> if and only if the set should be case sensitive;
                     otherwise <c>true</c>. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.#ctor(Lucene.Net.Util.LuceneVersion,System.Collections.Generic.IDictionary{System.String,`0},System.Boolean)">
            <summary>
            Creates a map from the mappings in another map. 
            </summary>
            <param name="matchVersion">
                     compatibility match version see <a href="#version">Version
                     note</a> above for details. </param>
            <param name="c">
                     a map (<see cref="T:IDictionary{string, V}"/>) whose mappings to be copied </param>
            <param name="ignoreCase">
                     <c>false</c> if and only if the set should be case sensitive;
                     otherwise <c>true</c>. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.#ctor(Lucene.Net.Analysis.Util.CharArrayMap{`0})">
            <summary>
            Create set from the supplied map (used internally for readonly maps...)
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Add(System.Collections.Generic.KeyValuePair{System.String,`0})">
            <summary>
            Adds the <see cref="T:KeyValuePair{string, V}.Value"/> for the passed in <see cref="T:KeyValuePair{string, V}.Key"/>.
            Note that the <see cref="T:KeyValuePair{string, V}"/> instance is not added to the dictionary.
            </summary>
            <param name="item">A <see cref="T:KeyValuePair{string, V}"/> whose <see cref="T:KeyValuePair{string, V}.Value"/> 
            will be added for the corresponding <see cref="T:KeyValuePair{string, V}.Key"/>. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Add(System.String,`0)">
            <summary>
            Adds the <paramref name="value"/> for the passed in <paramref name="key"/>.
            </summary>
            <param name="key">The string-able type to be added/updated in the dictionary.</param>
            <param name="value">The corresponding value for the given <paramref name="key"/>.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Clear">
            <summary>
            Clears all entries in this map. This method is supported for reusing, but not 
            <see cref="M:System.Collections.Generic.IDictionary`2.Remove(`0)"/>. 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Contains(System.Collections.Generic.KeyValuePair{System.String,`0})">
            <summary>
            Not supported. 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.CopyTo(System.Collections.Generic.KeyValuePair{System.String,`0}[],System.Int32)">
            <summary>
            Copies all items in the current dictionary the <paramref name="array"/> starting at the <paramref name="arrayIndex"/>.
            The array is assumed to already be dimensioned to fit the elements in this dictionary; otherwise a <see cref="T:System.ArgumentOutOfRangeException"/>
            will be thrown.
            </summary>
            <param name="array">The array to copy the items into.</param>
            <param name="arrayIndex">A 32-bit integer that represents the index in <paramref name="array"/> at which copying begins.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.CopyTo(Lucene.Net.Analysis.Util.CharArrayMap{`0})">
            <summary>
            Copies all items in the current <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> to the passed in
            <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/>.
            </summary>
            <param name="map"></param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.ContainsKey(System.Char[],System.Int32,System.Int32)">
            <summary>
            <c>true</c> if the <paramref name="length"/> chars of <paramref name="text"/> starting at <paramref name="offset"/>
            are in the <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.Keys"/> 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.ContainsKey(System.Char[])">
            <summary>
            <c>true</c> if the entire <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.Keys"/> is the same as the 
            <paramref name="text"/> <see cref="T:char[]"/> being passed in; 
            otherwise <c>false</c>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.ContainsKey(System.String)">
            <summary>
            <c>true</c> if the <paramref name="text"/> <see cref="T:System.String"/> is in the <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.Keys"/>; 
            otherwise <c>false</c>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.ContainsKey(J2N.Text.ICharSequence)">
            <summary>
            <c>true</c> if the <paramref name="text"/> <see cref="T:J2N.Text.ICharSequence"/> is in the <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.Keys"/>; 
            otherwise <c>false</c>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.ContainsKey(System.Object)">
            <summary>
            <c>true</c> if the <paramref name="o"/> <see cref="M:System.Object.ToString"/> is in the <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.Keys"/>; 
            otherwise <c>false</c>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Get(System.Char[],System.Int32,System.Int32)">
            <summary>
            returns the value of the mapping of <paramref name="length"/> chars of <paramref name="text"/>
            starting at <paramref name="offset"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Get(System.Char[])">
            <summary>
            returns the value of the mapping of the chars inside this <paramref name="text"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Get(J2N.Text.ICharSequence)">
            <summary>
            returns the value of the mapping of the chars inside this <see cref="T:J2N.Text.ICharSequence"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Get(System.String)">
            <summary>
            returns the value of the mapping of the chars inside this <see cref="T:System.String"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Get(System.Object)">
            <summary>
            returns the value of the mapping of the chars inside this <see cref="M:System.Object.ToString"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.GetSlot(J2N.Text.ICharSequence)">
            <summary>
            Returns true if the <see cref="T:J2N.Text.ICharSequence"/> is in the set
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.GetSlot(System.String)">
            <summary>
            Returns true if the <see cref="T:System.String"/> is in the set
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(J2N.Text.ICharSequence,`0)">
            <summary>
            Add the given mapping.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.Object,`0)">
            <summary>
            Add the given mapping using the <see cref="M:System.Object.ToString"/> representation
            of <paramref name="o"/> in the <see cref="P:System.Globalization.CultureInfo.InvariantCulture"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)">
            <summary>
            Add the given mapping.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.Char[],`0)">
            <summary>
            Add the given mapping.
            If ignoreCase is true for this Set, the text array will be directly modified.
            The user should never modify this text array after calling this method.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.PutImpl(J2N.Text.ICharSequence,Lucene.Net.Analysis.Util.CharArrayMap{`0}.MapValue)">
            <summary>
            Add the given mapping.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.PutImpl(System.String,Lucene.Net.Analysis.Util.CharArrayMap{`0}.MapValue)">
            <summary>
            Add the given mapping.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.PutImpl(System.Char[],Lucene.Net.Analysis.Util.CharArrayMap{`0}.MapValue)">
            <summary>
            LUCENENET specific. Centralizes the logic between Put()
            implementations that accept a value and those that don't. This value is
            so we know whether or not the value was set, since we can't reliably do
            a check for <c>null</c> on the <typeparamref name="TValue"/> type.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.PutAll(System.Collections.Generic.IDictionary{System.Char[],`0})">
            <summary>
            This implementation enumerates over the specified <see cref="T:IDictionary{char[],TValue}"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.Char[],`0)"/> operation once for each entry.
            </summary>
            <param name="collection">A dictionary of values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.PutAll(System.Collections.Generic.IDictionary{System.String,`0})">
            <summary>
            This implementation enumerates over the specified <see cref="T:IDictionary{string,TValue}"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="collection">A dictionary of values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.PutAll(System.Collections.Generic.IDictionary{J2N.Text.ICharSequence,`0})">
            <summary>
            This implementation enumerates over the specified <see cref="T:IDictionary{ICharSequence,TValue}"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(J2N.Text.ICharSequence,`0)"/> operation once for each entry.
            </summary>
            <param name="collection">A dictionary of values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.PutAll(System.Collections.Generic.IDictionary{System.Object,`0})">
            <summary>
            This implementation enumerates over the specified <see cref="T:IDictionary{object,TValue}"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.Object,`0)"/> operation once for each entry.
            </summary>
            <param name="collection">A dictionary of values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.PutAll(System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.Char[],`0}})">
            <summary>
            This implementation enumerates over the specified <see cref="T:IEnumerable{KeyValuePair{char[],TValue}}"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.Char[],`0)"/> operation once for each entry.
            </summary>
            <param name="collection">The values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.PutAll(System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.String,`0}})">
            <summary>
            This implementation enumerates over the specified <see cref="T:IEnumerable{KeyValuePair{string,TValue}}"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="collection">The values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.PutAll(System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{J2N.Text.ICharSequence,`0}})">
            <summary>
            This implementation enumerates over the specified <see cref="T:IEnumerable{KeyValuePair{ICharSequence,TValue}}"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(J2N.Text.ICharSequence,`0)"/> operation once for each entry.
            </summary>
            <param name="collection">The values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.PutAll(System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.Object,`0}})">
            <summary>
            This implementation enumerates over the specified <see cref="T:IEnumerable{KeyValuePair{object,TValue}}"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.Object,`0)"/> operation once for each entry.
            </summary>
            <param name="collection">The values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Equals(System.Object)">
            <summary>
            LUCENENET Specific - test for value equality similar to how it is done in Java
            </summary>
            <param name="obj">Another dictionary to test the values of</param>
            <returns><c>true</c> if the given object is an <see cref="T:IDictionary{object, V}"/> that contains
            the same key value pairs as the current map</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.GetHashCode">
            <summary>
            LUCENENET Specific - override required by .NET because we override Equals
            to simulate Java's value equality checking.
            </summary>
            <returns></returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharArrayMap`1.MatchVersion">
            <summary>
            The Lucene version corresponding to the compatibility behavior 
            that this instance emulates
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.Char[])">
            <summary>
            Adds a placeholder with the given <paramref name="text"/> as the key.
            Primarily for internal use by <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(J2N.Text.ICharSequence)">
            <summary>
            Adds a placeholder with the given <paramref name="text"/> as the key.
            Primarily for internal use by <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String)">
            <summary>
            Adds a placeholder with the given <paramref name="text"/> as the key.
            Primarily for internal use by <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.Object)">
            <summary>
            Adds a placeholder with the given <paramref name="o"/> as the key.
            Primarily for internal use by <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.TryGetValue(System.Char[],System.Int32,System.Int32,`0@)">
            <summary>
            Gets the value associated with the specified key.
            </summary>
            <param name="key">The key of the value to get.</param>
            <param name="offset">The position of the <paramref name="key"/> where the target key begins.</param>
            <param name="length">The total length of the <paramref name="key"/>.</param>
            <param name="value">When this method returns, contains the value associated with the specified key, 
            if the key is found; otherwise, the default value for the type of the value parameter. 
            This parameter is passed uninitialized.</param>
            <returns><c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> contains an element with the specified key; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.TryGetValue(System.Char[],`0@)">
            <summary>
            Gets the value associated with the specified key.
            </summary>
            <param name="key">The key of the value to get.</param>
            <param name="value">When this method returns, contains the value associated with the specified key, 
            if the key is found; otherwise, the default value for the type of the value parameter. 
            This parameter is passed uninitialized.</param>
            <returns><c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> contains an element with the specified key; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.TryGetValue(J2N.Text.ICharSequence,`0@)">
            <summary>
            Gets the value associated with the specified key.
            </summary>
            <param name="key">The key of the value to get.</param>
            <param name="value">When this method returns, contains the value associated with the specified key, 
            if the key is found; otherwise, the default value for the type of the value parameter. 
            This parameter is passed uninitialized.</param>
            <returns><c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> contains an element with the specified key; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.TryGetValue(System.String,`0@)">
            <summary>
            Gets the value associated with the specified key.
            </summary>
            <param name="key">The key of the value to get.</param>
            <param name="value">When this method returns, contains the value associated with the specified key, 
            if the key is found; otherwise, the default value for the type of the value parameter. 
            This parameter is passed uninitialized.</param>
            <returns><c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> contains an element with the specified key; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.TryGetValue(System.Object,`0@)">
            <summary>
            Gets the value associated with the specified key.
            </summary>
            <param name="key">The key of the value to get.</param>
            <param name="value">When this method returns, contains the value associated with the specified key, 
            if the key is found; otherwise, the default value for the type of the value parameter. 
            This parameter is passed uninitialized.</param>
            <returns><c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> contains an element with the specified key; otherwise, <c>false</c>.</returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharArrayMap`1.Item(System.Char[],System.Int32,System.Int32)">
            <summary>
            Gets or sets the value associated with the specified key.
            </summary>
            <param name="key">The key of the value to get or set.</param>
            <param name="offset">The position of the <paramref name="key"/> where the target key begins.</param>
            <param name="length">The total length of the <paramref name="key"/>.</param>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharArrayMap`1.Item(System.Char[])">
            <summary>
            Gets or sets the value associated with the specified key.
            </summary>
            <param name="key">The key of the value to get or set.</param>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharArrayMap`1.Item(J2N.Text.ICharSequence)">
            <summary>
            Gets or sets the value associated with the specified key.
            </summary>
            <param name="key">The key of the value to get or set.</param>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharArrayMap`1.Item(System.String)">
            <summary>
            Gets or sets the value associated with the specified key.
            </summary>
            <param name="key">The key of the value to get or set.</param>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharArrayMap`1.Item(System.Object)">
            <summary>
            Gets or sets the value associated with the specified key.
            </summary>
            <param name="key">The key of the value to get or set.</param>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharArrayMap`1.Keys">
            <summary>
            Gets a collection containing the keys in the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/>.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharArrayMap`1.Values">
            <summary>
            Gets a collection containing the values in the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/>.
            This specialized collection can be enumerated in order to read its values and 
            overrides <see cref="M:System.Object.ToString"/> in order to display a string 
            representation of the values.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.CharArrayMap`1.KeyCollection">
            <summary>
            LUCENENET specific class used to break the infinite recursion when the
            CharArraySet iterates the keys of this dictionary via <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.OriginalKeySet"/>. 
            In Java, the keyset of the abstract base class was used to break the infinite recursion, 
            however this class doesn't have an abstract base class so that is not an option. 
            This class is just a facade around the keys (not another collection of keys), so it 
            doesn't consume any additional RAM while providing another "virtual" collection to iterate over.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.CharArrayMap`1.KeyCollection.KeyEnumerator">
            <summary>
            LUCENENET specific class to iterate the values in the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1.KeyCollection"/>.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.CharArrayMap`1.ValueCollection">
            <summary>
            LUCENENET specific class that represents the values in the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/>.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.CharArrayMap`1.ValueCollection.ValueEnumerator">
            <summary>
            LUCENENET specific class to enumerate the values in the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1.ValueCollection"/>.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharArrayMap`1.IsReadOnly">
            <summary>
            <c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> is read-only; otherwise <c>false</c>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.GetEnumerator">
            <summary>
            Returns an enumerator that iterates through the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.System#Collections#IEnumerable#GetEnumerator">
            <summary>
            Returns an enumerator that iterates through the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/>.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharArrayMap`1.Count">
            <summary>
            Gets the number of key/value pairs contained in the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.ToString">
            <summary>
            Returns a string that represents the current object. (Inherited from <see cref="T:System.Object"/>.)
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharArrayMap`1.OriginalKeySet">
            <summary>
            helper for <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to not produce endless recursion
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharArrayMap`1.KeySet">
            <summary>
            Returns an <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> view on the map's keys.
            The set will use the same <see cref="F:Lucene.Net.Analysis.Util.CharArrayMap`1.matchVersion"/> as this map. 
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.CharArrayMap`1.EntryIterator">
            <summary>
            public iterator class so efficient methods are exposed to users
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.EntryIterator.NextKey">
            <summary>
            gets the next key... do not modify the returned char[]
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.EntryIterator.NextKeyString">
            <summary>
            gets the next key as a newly created <see cref="T:System.String"/> object
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharArrayMap`1.EntryIterator.CurrentValue">
            <summary>
            returns the value associated with the current key
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.EntryIterator.SetValue(`0)">
            <summary>
            sets the value associated with the last key returned
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.CharArrayMap`1.EntrySet_">
            <summary>
            public EntrySet_ class so efficient methods are exposed to users
            
            NOTE: In .NET this was renamed to EntrySet_ because it conflicted with the
            method EntrySet(). Since there is also an extension method named <see cref="T:IDictionary{K,V}.EntrySet()"/> 
            that this class needs to override, changing the name of the method was not
            possible because the extension method would produce incorrect results if it were
            inadvertently called, leading to hard-to-diagnose bugs.
            
            Another difference between this set and the Java counterpart is that it implements
            <see cref="T:System.Collections.Generic.ICollection`1"/> rather than <see cref="T:System.Collections.Generic.ISet`1"/> so we don't have to implement
            a bunch of methods that we aren't really interested in. The <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.Keys"/> and <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.Values"/>
            properties both return <see cref="T:System.Collections.Generic.ICollection`1"/>, and while there is no <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.EntrySet"/> method
            or property in .NET, if there were it would certainly return <see cref="T:System.Collections.Generic.ICollection`1"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap`1.EmptyMap">
            <summary>
            Returns an empty, unmodifiable map. </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.ICharArrayMap">
            <summary>
            LUCENENET specific interface used so <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>
            can hold a reference to <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> without
            knowing its generic type.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap.Copy``1(Lucene.Net.Util.LuceneVersion,System.Collections.Generic.IDictionary{System.String,``0})">
            <summary>
            Returns a copy of the given map as a <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/>. If the given map
            is a <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> the ignoreCase property will be preserved.
            <para>
            <b>Note:</b> If you intend to create a copy of another <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> where
            the <see cref="T:Lucene.Net.Util.LuceneVersion"/> of the source map differs from its copy
            <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.#ctor(Lucene.Net.Util.LuceneVersion,System.Collections.Generic.IDictionary{System.String,`0},System.Boolean)"/> should be used instead.
            The <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap.Copy``1(Lucene.Net.Util.LuceneVersion,System.Collections.Generic.IDictionary{System.String,``0})"/> will preserve the <see cref="T:Lucene.Net.Util.LuceneVersion"/> of the
            source map if it is an instance of <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/>.
            </para>
            </summary>
            <param name="matchVersion">
                     compatibility match version see <a href="#version">Version
                     note</a> above for details. This argument will be ignored if the
                     given map is a <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/>. </param>
            <param name="map">
                     a map to copy </param>
            <returns> a copy of the given map as a <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/>. If the given map
                    is a <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> the ignoreCase property as well as the
                    <paramref name="matchVersion"/> will be of the given map will be preserved. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap.Copy``1(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.ICharArrayMap)">
            <summary>
            Used by <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to copy <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> without knowing 
            its generic type.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap.UnmodifiableMap``1(Lucene.Net.Analysis.Util.CharArrayMap{``0})">
            <summary>
            Returns an unmodifiable <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/>. This allows to provide
            unmodifiable views of internal map for "read-only" use.
            </summary>
            <param name="map">
                     a map for which the unmodifiable map is returned. </param>
            <returns> an new unmodifiable <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/>. </returns>
            <exception cref="T:System.ArgumentException">
                      if the given map is <c>null</c>. </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMap.UnmodifiableMap``1(Lucene.Net.Analysis.Util.ICharArrayMap)">
            <summary>
            Used by <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to create an <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap.UnmodifiableCharArrayMap`1"/> instance
            without knowing the type of <typeparamref name="TValue"/>.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.CharArrayMap.EmptyCharArrayMap`1">
            <summary>
            Empty <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap.UnmodifiableCharArrayMap`1"/> optimized for speed.
            Contains checks will always return <c>false</c> or throw
            NPE if necessary.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.CharArrayMapExtensions">
            <summary>
            LUCENENET specific extension methods for CharArrayMap
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.ContainsKey``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Boolean)">
            <summary>
            <c>true</c> if the <paramref name="key"/> <see cref="T:System.Boolean"/> is in the <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.KeySet"/>; 
            otherwise <c>false</c>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.ContainsKey``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Byte)">
            <summary>
            <c>true</c> if the <paramref name="key"/> <see cref="T:System.Byte"/> is in the <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.KeySet"/>; 
            otherwise <c>false</c>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.ContainsKey``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Char)">
            <summary>
            <c>true</c> if the <paramref name="key"/> <see cref="T:System.Char"/> is in the <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.KeySet"/>; 
            otherwise <c>false</c>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.ContainsKey``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Int32)">
            <summary>
            <c>true</c> if the <paramref name="key"/> <see cref="T:System.Int32"/> is in the <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.KeySet"/>; 
            otherwise <c>false</c>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.ContainsKey``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Int64)">
            <summary>
            <c>true</c> if the <paramref name="key"/> <see cref="T:System.Int64"/> is in the <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.KeySet"/>; 
            otherwise <c>false</c>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.ContainsKey``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.SByte)">
            <summary>
            <c>true</c> if the <paramref name="key"/> <see cref="T:System.SByte"/> is in the <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.KeySet"/>; 
            otherwise <c>false</c>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.ContainsKey``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Int16)">
            <summary>
            <c>true</c> if the <paramref name="key"/> <see cref="T:System.Int16"/> is in the <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.KeySet"/>; 
            otherwise <c>false</c>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.ContainsKey``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.UInt32)">
            <summary>
            <c>true</c> if the <paramref name="key"/> <see cref="T:System.UInt32"/> is in the <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.KeySet"/>; 
            otherwise <c>false</c>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.ContainsKey``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.UInt64)">
            <summary>
            <c>true</c> if the <paramref name="key"/> <see cref="T:System.UInt64"/> is in the <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.KeySet"/>; 
            otherwise <c>false</c>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.ContainsKey``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.UInt16)">
            <summary>
            <c>true</c> if the <paramref name="key"/> <see cref="T:System.UInt16"/> is in the <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.KeySet"/>; 
            otherwise <c>false</c>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Get``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Boolean)">
            <summary>
            returns the value of the mapping of the chars inside this <paramref name="text"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Get``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Byte)">
            <summary>
            returns the value of the mapping of the chars inside this <paramref name="text"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Get``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Char)">
            <summary>
            returns the value of the mapping of the chars inside this <paramref name="text"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Get``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Decimal)">
            <summary>
            returns the value of the mapping of the chars inside this <paramref name="text"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Get``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Double)">
            <summary>
            returns the value of the mapping of the chars inside this <paramref name="text"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Get``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Single)">
            <summary>
            returns the value of the mapping of the chars inside this <paramref name="text"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Get``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Int32)">
            <summary>
            returns the value of the mapping of the chars inside this <paramref name="text"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Get``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Int64)">
            <summary>
            returns the value of the mapping of the chars inside this <paramref name="text"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Get``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.SByte)">
            <summary>
            returns the value of the mapping of the chars inside this <paramref name="text"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Get``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Int16)">
            <summary>
            returns the value of the mapping of the chars inside this <paramref name="text"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Get``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.UInt32)">
            <summary>
            returns the value of the mapping of the chars inside this <paramref name="text"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Get``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.UInt64)">
            <summary>
            returns the value of the mapping of the chars inside this <paramref name="text"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Get``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.UInt16)">
            <summary>
            returns the value of the mapping of the chars inside this <paramref name="text"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Put``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Boolean,``0)">
            <summary>
            Add the given mapping.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Put``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Byte,``0)">
            <summary>
            Add the given mapping.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Put``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Char,``0)">
            <summary>
            Add the given mapping.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Put``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Int32,``0)">
            <summary>
            Add the given mapping.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Put``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Int64,``0)">
            <summary>
            Add the given mapping.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Put``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.SByte,``0)">
            <summary>
            Add the given mapping.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Put``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Int16,``0)">
            <summary>
            Add the given mapping.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Put``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.UInt32,``0)">
            <summary>
            Add the given mapping.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Put``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.UInt64,``0)">
            <summary>
            Add the given mapping.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.Put``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.UInt16,``0)">
            <summary>
            Add the given mapping.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IDictionary{System.Boolean,``0})">
            <summary>
            This implementation enumerates over the specified <paramref name="dictionary"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="dictionary">A dictionary of values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IDictionary{System.Byte,``0})">
            <summary>
            This implementation enumerates over the specified <paramref name="dictionary"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="dictionary">A dictionary of values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IDictionary{System.Char,``0})">
            <summary>
            This implementation enumerates over the specified <paramref name="dictionary"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="dictionary">A dictionary of values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IDictionary{System.Int32,``0})">
            <summary>
            This implementation enumerates over the specified <paramref name="dictionary"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="dictionary">A dictionary of values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IDictionary{System.Int64,``0})">
            <summary>
            This implementation enumerates over the specified <paramref name="dictionary"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="dictionary">A dictionary of values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IDictionary{System.SByte,``0})">
            <summary>
            This implementation enumerates over the specified <paramref name="dictionary"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="dictionary">A dictionary of values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IDictionary{System.Int16,``0})">
            <summary>
            This implementation enumerates over the specified <paramref name="dictionary"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="dictionary">A dictionary of values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IDictionary{System.UInt32,``0})">
            <summary>
            This implementation enumerates over the specified <paramref name="dictionary"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="dictionary">A dictionary of values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IDictionary{System.UInt64,``0})">
            <summary>
            This implementation enumerates over the specified <paramref name="dictionary"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="dictionary">A dictionary of values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IDictionary{System.UInt16,``0})">
            <summary>
            This implementation enumerates over the specified <paramref name="dictionary"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="dictionary">A dictionary of values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.Boolean,``0}})">
            <summary>
            This implementation enumerates over the specified <paramref name="collection"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="collection">The values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.Byte,``0}})">
            <summary>
            This implementation enumerates over the specified <paramref name="collection"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="collection">The values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.Char,``0}})">
            <summary>
            This implementation enumerates over the specified <paramref name="collection"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="collection">The values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.Int32,``0}})">
            <summary>
            This implementation enumerates over the specified <paramref name="collection"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="collection">The values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.Int64,``0}})">
            <summary>
            This implementation enumerates over the specified <paramref name="collection"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="collection">The values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.SByte,``0}})">
            <summary>
            This implementation enumerates over the specified <paramref name="collection"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="collection">The values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.Int16,``0}})">
            <summary>
            This implementation enumerates over the specified <paramref name="collection"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="collection">The values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.UInt32,``0}})">
            <summary>
            This implementation enumerates over the specified <paramref name="collection"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="collection">The values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.UInt64,``0}})">
            <summary>
            This implementation enumerates over the specified <paramref name="collection"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="collection">The values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.PutAll``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.UInt16,``0}})">
            <summary>
            This implementation enumerates over the specified <paramref name="collection"/>'s
            entries, and calls this map's <see cref="M:Lucene.Net.Analysis.Util.CharArrayMap`1.Put(System.String,`0)"/> operation once for each entry.
            </summary>
            <param name="map">this map</param>
            <param name="collection">The values to add/update in the current map.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.TryGetValue``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Boolean,``0@)">
            <summary>
            Gets the value associated with the specified key.
            </summary>
            <param name="map">this map</param>
            <param name="key">The key of the value to get.</param>
            <param name="value">When this method returns, contains the value associated with the specified key, 
            if the key is found; otherwise, the default value for the type of the value parameter. 
            This parameter is passed uninitialized.</param>
            <returns><c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> contains an element with the specified key; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.TryGetValue``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Byte,``0@)">
            <summary>
            Gets the value associated with the specified key.
            </summary>
            <param name="map">this map</param>
            <param name="key">The key of the value to get.</param>
            <param name="value">When this method returns, contains the value associated with the specified key, 
            if the key is found; otherwise, the default value for the type of the value parameter. 
            This parameter is passed uninitialized.</param>
            <returns><c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> contains an element with the specified key; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.TryGetValue``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Char,``0@)">
            <summary>
            Gets the value associated with the specified key.
            </summary>
            <param name="map">this map</param>
            <param name="key">The key of the value to get.</param>
            <param name="value">When this method returns, contains the value associated with the specified key, 
            if the key is found; otherwise, the default value for the type of the value parameter. 
            This parameter is passed uninitialized.</param>
            <returns><c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> contains an element with the specified key; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.TryGetValue``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Int32,``0@)">
            <summary>
            Gets the value associated with the specified key.
            </summary>
            <param name="map">this map</param>
            <param name="key">The key of the value to get.</param>
            <param name="value">When this method returns, contains the value associated with the specified key, 
            if the key is found; otherwise, the default value for the type of the value parameter. 
            This parameter is passed uninitialized.</param>
            <returns><c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> contains an element with the specified key; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.TryGetValue``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Int64,``0@)">
            <summary>
            Gets the value associated with the specified key.
            </summary>
            <param name="map">this map</param>
            <param name="key">The key of the value to get.</param>
            <param name="value">When this method returns, contains the value associated with the specified key, 
            if the key is found; otherwise, the default value for the type of the value parameter. 
            This parameter is passed uninitialized.</param>
            <returns><c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> contains an element with the specified key; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.TryGetValue``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.SByte,``0@)">
            <summary>
            Gets the value associated with the specified key.
            </summary>
            <param name="map">this map</param>
            <param name="key">The key of the value to get.</param>
            <param name="value">When this method returns, contains the value associated with the specified key, 
            if the key is found; otherwise, the default value for the type of the value parameter. 
            This parameter is passed uninitialized.</param>
            <returns><c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> contains an element with the specified key; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.TryGetValue``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.Int16,``0@)">
            <summary>
            Gets the value associated with the specified key.
            </summary>
            <param name="map">this map</param>
            <param name="key">The key of the value to get.</param>
            <param name="value">When this method returns, contains the value associated with the specified key, 
            if the key is found; otherwise, the default value for the type of the value parameter. 
            This parameter is passed uninitialized.</param>
            <returns><c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> contains an element with the specified key; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.TryGetValue``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.UInt32,``0@)">
            <summary>
            Gets the value associated with the specified key.
            </summary>
            <param name="map">this map</param>
            <param name="key">The key of the value to get.</param>
            <param name="value">When this method returns, contains the value associated with the specified key, 
            if the key is found; otherwise, the default value for the type of the value parameter. 
            This parameter is passed uninitialized.</param>
            <returns><c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> contains an element with the specified key; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.TryGetValue``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.UInt64,``0@)">
            <summary>
            Gets the value associated with the specified key.
            </summary>
            <param name="map">this map</param>
            <param name="key">The key of the value to get.</param>
            <param name="value">When this method returns, contains the value associated with the specified key, 
            if the key is found; otherwise, the default value for the type of the value parameter. 
            This parameter is passed uninitialized.</param>
            <returns><c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> contains an element with the specified key; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArrayMapExtensions.TryGetValue``1(Lucene.Net.Analysis.Util.CharArrayMap{``0},System.UInt16,``0@)">
            <summary>
            Gets the value associated with the specified key.
            </summary>
            <param name="map">this map</param>
            <param name="key">The key of the value to get.</param>
            <param name="value">When this method returns, contains the value associated with the specified key, 
            if the key is found; otherwise, the default value for the type of the value parameter. 
            This parameter is passed uninitialized.</param>
            <returns><c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArrayMap`1"/> contains an element with the specified key; otherwise, <c>false</c>.</returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.CharArraySet">
            <summary>
            A simple class that stores <see cref="T:System.String"/>s as <see cref="T:char[]"/>'s in a
            hash table.  Note that this is not a general purpose
            class.  For example, it cannot remove items from the
            set, nor does it resize its hash table to be smaller,
            etc.  It is designed to be quick to test if a <see cref="T:char[]"/>
            is in the set without the necessity of converting it
            to a <see cref="T:System.String"/> first.
            
            <a name="version"></a>
            <para>You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/>
            compatibility when creating <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>:
            <ul>
              <li> As of 3.1, supplementary characters are
                  properly lowercased.</li>
            </ul>
            Before 3.1 supplementary characters could not be
            lowercased correctly due to the lack of Unicode 4
            support in JDK 1.4. To use instances of
            <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> with the behavior before Lucene
            3.1 pass a <see cref="T:Lucene.Net.Util.LuceneVersion"/> to the constructors.
            </para>
            <para>
            <em>Please note:</em> This class implements <see cref="T:System.Collections.Generic.ISet`1"/> but
            does not behave like it should in all cases. The generic type is
            <see cref="T:System.String"/>, because you can add any object to it,
            that has a string representation (which is converted to a string). The add methods will use
            <see cref="M:System.Object.ToString"/> and store the result using a <see cref="T:char[]"/>
            buffer. The same behavior have the <see cref="M:Lucene.Net.Analysis.Util.CharArraySet.Contains(System.String)"/> methods.
            The <see cref="M:Lucene.Net.Analysis.Util.CharArraySet.GetEnumerator"/> returns an <see cref="T:IEnumerator{char[]}"/>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.#ctor(Lucene.Net.Util.LuceneVersion,System.Int32,System.Boolean)">
            <summary>
            Create set with enough capacity to hold <paramref name="startSize"/> terms
            </summary>
            <param name="matchVersion">
                     compatibility match version see <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> for details. </param>
            <param name="startSize">
                     the initial capacity </param>
            <param name="ignoreCase">
                     <c>false</c> if and only if the set should be case sensitive
                     otherwise <c>true</c>. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.#ctor(Lucene.Net.Util.LuceneVersion,System.Collections.Generic.ICollection{System.String},System.Boolean)">
            <summary>
            Creates a set from a collection of objects. 
            </summary>
            <param name="matchVersion">
                     compatibility match version see <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> for details. </param>
            <param name="c">
                     a collection whose elements to be placed into the set </param>
            <param name="ignoreCase">
                     <c>false</c> if and only if the set should be case sensitive
                     otherwise <c>true</c>. </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.#ctor(Lucene.Net.Analysis.Util.ICharArrayMap)">
            <summary>
            Create set from the specified map (internal only), used also by <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.Keys"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.Clear">
            <summary>
            Clears all entries in this set. This method is supported for reusing, but not <see cref="M:ICollection{string}.Remove(string)"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.Contains(System.Char[],System.Int32,System.Int32)">
            <summary>
            <c>true</c> if the <paramref name="length"/> chars of <paramref name="text"/> starting at <paramref name="offset"/>
            are in the set 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.Contains(System.Char[])">
            <summary>
            <c>true</c> if the <see cref="T:char[]"/>s 
            are in the set 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.Contains(J2N.Text.ICharSequence)">
            <summary>
            <c>true</c> if the <see cref="T:J2N.Text.ICharSequence"/> is in the set
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.Contains(System.String)">
            <summary>
            <c>true</c> if the <see cref="T:System.String"/> is in the set
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.Contains(System.Object)">
            <summary>
            <c>true</c> if the <see cref="M:System.Object.ToString"/> representation of <paramref name="o"/> is in the set
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.Add(System.Object)">
            <summary>
            Add the <see cref="M:System.Object.ToString"/> representation of <paramref name="o"/> into the set.
            The <see cref="M:System.Object.ToString"/> method is called after setting the thread to <see cref="P:System.Globalization.CultureInfo.InvariantCulture"/>.
            If the type of <paramref name="o"/> is a value type, it will be converted using the 
            <see cref="P:System.Globalization.CultureInfo.InvariantCulture"/>.
            </summary>
            <param name="o">A string-able object</param>
            <returns><c>true</c> if <paramref name="o"/> was added to the set; <c>false</c> if it already existed prior to this call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.Add(J2N.Text.ICharSequence)">
            <summary>
            Add this <see cref="T:J2N.Text.ICharSequence"/> into the set
            </summary>
            <returns><c>true</c> if <paramref name="text"/> was added to the set; <c>false</c> if it already existed prior to this call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.Add(System.String)">
            <summary>
            Add this <see cref="T:System.String"/> into the set
            </summary>
            <returns><c>true</c> if <paramref name="text"/> was added to the set; <c>false</c> if it already existed prior to this call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.Add(System.Char[])">
            <summary>
            Add this <see cref="T:char[]"/> directly to the set.
            If <c>ignoreCase</c> is true for this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>, the text array will be directly modified.
            The user should never modify this text array after calling this method.
            </summary>
            <returns><c>true</c> if <paramref name="text"/> was added to the set; <c>false</c> if it already existed prior to this call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.System#Collections#Generic#ICollection{System#String}#Add(System.String)">
            <summary>
            LUCENENET specific for supporting <see cref="T:System.Collections.Generic.ICollection`1"/>.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharArraySet.Count">
            <summary>
            Gets the number of elements contained in the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharArraySet.IsReadOnly">
            <summary>
            <c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> is read-only; otherwise <c>false</c>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.UnmodifiableSet(Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Returns an unmodifiable <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>. This allows to provide
            unmodifiable views of internal sets for "read-only" use.
            </summary>
            <param name="set">
                     a set for which the unmodifiable set is returned. </param>
            <returns> an new unmodifiable <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>. </returns>
            <exception cref="T:System.ArgumentNullException">
                      if the given set is <c>null</c>. </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.Copy``1(Lucene.Net.Util.LuceneVersion,System.Collections.Generic.ICollection{``0})">
            <summary>
            Returns a copy of the given set as a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>. If the given set
            is a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> the ignoreCase property will be preserved.
            <para>
            <b>Note:</b> If you intend to create a copy of another <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> where
            the <see cref="T:Lucene.Net.Util.LuceneVersion"/> of the source set differs from its copy
            <see cref="M:Lucene.Net.Analysis.Util.CharArraySet.#ctor(Lucene.Net.Util.LuceneVersion,System.Collections.Generic.ICollection{System.String},System.Boolean)"/> should be used instead.
            The <see cref="M:Lucene.Net.Analysis.Util.CharArraySet.Copy``1(Lucene.Net.Util.LuceneVersion,System.Collections.Generic.ICollection{``0})"/> will preserve the <see cref="T:Lucene.Net.Util.LuceneVersion"/> of the
            source set it is an instance of <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.
            </para>
            </summary>
            <param name="matchVersion">
                     compatibility match version. This argument will be ignored if the
                     given set is a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>. </param>
            <param name="set">
                     a set to copy </param>
            <returns> a copy of the given set as a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>. If the given set
                    is a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> the <see cref="F:Lucene.Net.Analysis.Util.CharArrayMap`1.ignoreCase"/> field as well as the
                    <see cref="P:Lucene.Net.Analysis.Util.CharArrayMap`1.MatchVersion"/> will be preserved. </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.GetEnumerator">
            <summary>
            Returns an <see cref="T:System.Collections.IEnumerator"/> for <see cref="T:char[]"/> instances in this set.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.ToString">
            <summary>
            Returns a string that represents the current object. (Inherited from <see cref="T:System.Object"/>.)
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.Equals(System.Object)">
            <summary>
            Compares the specified object with this set for equality. Returns <c>true</c> if the 
            given object is also a set, the two sets have the same size, and every member of the 
            given set is contained in this set. This ensures that the equals method works properly 
            across different implementations of the <see cref="T:ISet{string}"/> interface.
            <para/>
            This implementation first checks if the specified object is this set; if so it 
            returns <c>true</c>. Then, it checks if the specified object is a set whose 
            size is identical to the size of this set; if not, it returns <c>false</c>. If so, 
            it uses the enumerator of this set and the specified object to determine if all of the
            contained values are present (using <see cref="M:System.String.Equals(System.String)"/>).
            </summary>
            <param name="obj">object to be compared for equality with this set</param>
            <returns><c>true</c> if the specified object is equal to this set</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.GetHashCode">
            <summary>
            Returns the hash code value for this set. The hash code of a set 
            is defined to be the sum of the hash codes of the elements in the 
            set, where the hash code of a <c>null</c> element is defined to be zero. 
            This ensures that <c>s1.Equals(s2)</c> implies that 
            <c>s1.GetHashCode()==s2.GetHashCode()</c> for any two sets s1 and s2.
            This implementation iterates over the set, calling the GetHashCode() 
            method on each element in the set, and adding up the results.
            </summary>
            <returns>the hash code value for this set</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.CopyTo(System.String[],System.Int32)">
            <summary>
            Copies the entire <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to a one-dimensional <see cref="T:string[]"/> array, 
            starting at the specified index of the target array.
            </summary>
            <param name="array">The one-dimensional <see cref="T:string[]"/> Array that is the destination of the 
            elements copied from <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>. The Array must have zero-based indexing.</param>
            <param name="arrayIndex">The zero-based index in array at which copying begins.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.SetEquals(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Determines whether the current set and the specified collection contain the same elements.
            </summary>
            <param name="other">The collection to compare to the current set.</param>
            <returns><c>true</c> if the current set is equal to other; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.UnionWith(System.Collections.Generic.IEnumerable{System.Char[]})">
            <summary>
            Modifies the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to contain all elements that are present 
            in itself, the specified collection, or both.
            </summary>
            <param name="other">The collection whose elements should be merged into the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> changed as a result of the call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.UnionWith(System.Collections.Generic.IEnumerable{J2N.Text.ICharSequence})">
            <summary>
            Modifies the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to contain all elements that are present 
            in itself, the specified collection, or both.
            </summary>
            <param name="other">The collection whose elements should be merged into the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> changed as a result of the call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.UnionWith(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Modifies the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to contain all elements that are present 
            in itself, the specified collection, or both.
            </summary>
            <param name="other">The collection whose elements should be merged into the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.UnionWith``1(System.Collections.Generic.IEnumerable{``0})">
            <summary>
            Modifies the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to contain all elements that are present 
            in itself, the specified collection, or both.
            </summary>
            <param name="other">The collection whose elements should be merged into the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> changed as a result of the call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.IsSubsetOf(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Determines whether a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object is a subset of the specified collection.
            </summary>
            <param name="other">The collection to compare to the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object is a subset of <paramref name="other"/>; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.IsSubsetOf``1(System.Collections.Generic.IEnumerable{``0})">
            <summary>
            Determines whether a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object is a subset of the specified collection.
            </summary>
            <param name="other">The collection to compare to the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object is a subset of <paramref name="other"/>; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.IsSupersetOf(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Determines whether a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object is a superset of the specified collection.
            </summary>
            <param name="other">The collection to compare to the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object is a superset of <paramref name="other"/>; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.IsSupersetOf``1(System.Collections.Generic.IEnumerable{``0})">
            <summary>
            Determines whether a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object is a superset of the specified collection.
            </summary>
            <param name="other">The collection to compare to the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object is a superset of <paramref name="other"/>; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.IsProperSubsetOf(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Determines whether a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object is a proper subset of the specified collection.
            </summary>
            <param name="other">The collection to compare to the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object is a proper subset of <paramref name="other"/>; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.IsProperSubsetOf``1(System.Collections.Generic.IEnumerable{``0})">
            <summary>
            Determines whether a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object is a proper subset of the specified collection.
            </summary>
            <param name="other">The collection to compare to the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object is a proper subset of <paramref name="other"/>; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.IsProperSupersetOf(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Determines whether a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object is a proper superset of the specified collection.
            </summary>
            <param name="other">The collection to compare to the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object is a proper superset of <paramref name="other"/>; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.IsProperSupersetOf``1(System.Collections.Generic.IEnumerable{``0})">
            <summary>
            Determines whether a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object is a proper superset of the specified collection.
            </summary>
            <param name="other">The collection to compare to the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object is a proper superset of <paramref name="other"/>; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.Overlaps(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Determines whether the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object and a specified collection share common elements.
            </summary>
            <param name="other">The collection to compare to the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object.</param>
            <returns><c>true</c> if the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object and <paramref name="other"/> share at least one common element; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.Overlaps``1(System.Collections.Generic.IEnumerable{``0})">
            <summary>
            Determines whether the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object and a specified collection share common elements.
            </summary>
            <param name="other">The collection to compare to the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> object and <paramref name="other"/> share at least one common element; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.ContainsAll(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Returns <c>true</c> if this collection contains all of the elements
            in the specified collection.
            </summary>
            <param name="other">collection to be checked for containment in this collection</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> contains all of the elements in the specified collection; otherwise, <c>false</c>.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySet.ContainsAll``1(System.Collections.Generic.IEnumerable{``0})">
            <summary>
            Returns <c>true</c> if this collection contains all of the elements
            in the specified collection.
            </summary>
            <param name="other">collection to be checked for containment in this collection</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> contains all of the elements in the specified collection; otherwise, <c>false</c>.</returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.CharArraySetExtensions">
            <summary>
            LUCENENET specific extension methods for CharArraySet
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Add(Lucene.Net.Analysis.Util.CharArraySet,System.Boolean)">
            <summary>
            Add this <see cref="T:System.Boolean"/> into the set
            </summary>
            <returns><c>true</c> if <paramref name="text"/> was added to the set; <c>false</c> if it already existed prior to this call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Add(Lucene.Net.Analysis.Util.CharArraySet,System.Byte)">
            <summary>
            Add this <see cref="T:System.Byte"/> into the set
            </summary>
            <returns><c>true</c> if <paramref name="text"/> was added to the set; <c>false</c> if it already existed prior to this call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Add(Lucene.Net.Analysis.Util.CharArraySet,System.Char)">
            <summary>
            Add this <see cref="T:System.Char"/> into the set
            </summary>
            <returns><c>true</c> if <paramref name="text"/> was added to the set; <c>false</c> if it already existed prior to this call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Add(Lucene.Net.Analysis.Util.CharArraySet,System.Int32)">
            <summary>
            Add this <see cref="T:System.Int32"/> into the set
            </summary>
            <returns><c>true</c> if <paramref name="text"/> was added to the set; <c>false</c> if it already existed prior to this call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Add(Lucene.Net.Analysis.Util.CharArraySet,System.Int64)">
            <summary>
            Add this <see cref="T:System.Int64"/> into the set
            </summary>
            <returns><c>true</c> if <paramref name="text"/> was added to the set; <c>false</c> if it already existed prior to this call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Add(Lucene.Net.Analysis.Util.CharArraySet,System.SByte)">
            <summary>
            Add this <see cref="T:System.SByte"/> into the set
            </summary>
            <returns><c>true</c> if <paramref name="text"/> was added to the set; <c>false</c> if it already existed prior to this call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Add(Lucene.Net.Analysis.Util.CharArraySet,System.Int16)">
            <summary>
            Add this <see cref="T:System.Int16"/> into the set
            </summary>
            <returns><c>true</c> if <paramref name="text"/> was added to the set; <c>false</c> if it already existed prior to this call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Add(Lucene.Net.Analysis.Util.CharArraySet,System.UInt32)">
            <summary>
            Add this <see cref="T:System.UInt32"/> into the set
            </summary>
            <returns><c>true</c> if <paramref name="text"/> was added to the set; <c>false</c> if it already existed prior to this call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Add(Lucene.Net.Analysis.Util.CharArraySet,System.UInt64)">
            <summary>
            Add this <see cref="T:System.UInt64"/> into the set
            </summary>
            <returns><c>true</c> if <paramref name="text"/> was added to the set; <c>false</c> if it already existed prior to this call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Add(Lucene.Net.Analysis.Util.CharArraySet,System.UInt16)">
            <summary>
            Add this <see cref="T:System.UInt16"/> into the set
            </summary>
            <returns><c>true</c> if <paramref name="text"/> was added to the set; <c>false</c> if it already existed prior to this call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Contains(Lucene.Net.Analysis.Util.CharArraySet,System.Boolean)">
            <summary>
            <c>true</c> if the <see cref="T:System.Boolean"/> is in the set
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Contains(Lucene.Net.Analysis.Util.CharArraySet,System.Byte)">
            <summary>
            <c>true</c> if the <see cref="T:System.Byte"/> is in the set
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Contains(Lucene.Net.Analysis.Util.CharArraySet,System.Char)">
            <summary>
            <c>true</c> if the <see cref="T:System.Char"/> is in the set
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Contains(Lucene.Net.Analysis.Util.CharArraySet,System.Int32)">
            <summary>
            <c>true</c> if the <see cref="T:System.Int32"/> is in the set
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Contains(Lucene.Net.Analysis.Util.CharArraySet,System.Int64)">
            <summary>
            <c>true</c> if the <see cref="T:System.Int64"/> is in the set
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Contains(Lucene.Net.Analysis.Util.CharArraySet,System.SByte)">
            <summary>
            <c>true</c> if the <see cref="T:System.SByte"/> is in the set
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Contains(Lucene.Net.Analysis.Util.CharArraySet,System.Int16)">
            <summary>
            <c>true</c> if the <see cref="T:System.Int16"/> is in the set
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Contains(Lucene.Net.Analysis.Util.CharArraySet,System.UInt32)">
            <summary>
            <c>true</c> if the <see cref="T:System.UInt32"/> is in the set
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Contains(Lucene.Net.Analysis.Util.CharArraySet,System.UInt64)">
            <summary>
            <c>true</c> if the <see cref="T:System.UInt64"/> is in the set
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.Contains(Lucene.Net.Analysis.Util.CharArraySet,System.UInt16)">
            <summary>
            <c>true</c> if the <see cref="T:System.UInt16"/> is in the set
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.UnionWith(Lucene.Net.Analysis.Util.CharArraySet,System.Collections.Generic.IEnumerable{System.Byte})">
            <summary>
            Modifies the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to contain all elements that are present 
            in itself, the specified collection, or both.
            </summary>
            <param name="set">this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/></param>
            <param name="other">The collection whose elements should be merged into the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> changed as a result of the call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.UnionWith(Lucene.Net.Analysis.Util.CharArraySet,System.Collections.Generic.IEnumerable{System.Char})">
            <summary>
            Modifies the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to contain all elements that are present 
            in itself, the specified collection, or both.
            </summary>
            <param name="set">this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/></param>
            <param name="other">The collection whose elements should be merged into the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> changed as a result of the call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.UnionWith(Lucene.Net.Analysis.Util.CharArraySet,System.Collections.Generic.IEnumerable{System.Int32})">
            <summary>
            Modifies the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to contain all elements that are present 
            in itself, the specified collection, or both.
            </summary>
            <param name="set">this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/></param>
            <param name="other">The collection whose elements should be merged into the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> changed as a result of the call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.UnionWith(Lucene.Net.Analysis.Util.CharArraySet,System.Collections.Generic.IEnumerable{System.Int64})">
            <summary>
            Modifies the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to contain all elements that are present 
            in itself, the specified collection, or both.
            </summary>
            <param name="set">this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/></param>
            <param name="other">The collection whose elements should be merged into the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> changed as a result of the call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.UnionWith(Lucene.Net.Analysis.Util.CharArraySet,System.Collections.Generic.IEnumerable{System.SByte})">
            <summary>
            Modifies the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to contain all elements that are present 
            in itself, the specified collection, or both.
            </summary>
            <param name="set">this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/></param>
            <param name="other">The collection whose elements should be merged into the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> changed as a result of the call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.UnionWith(Lucene.Net.Analysis.Util.CharArraySet,System.Collections.Generic.IEnumerable{System.Int16})">
            <summary>
            Modifies the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to contain all elements that are present 
            in itself, the specified collection, or both.
            </summary>
            <param name="set">this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/></param>
            <param name="other">The collection whose elements should be merged into the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> changed as a result of the call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.UnionWith(Lucene.Net.Analysis.Util.CharArraySet,System.Collections.Generic.IEnumerable{System.UInt32})">
            <summary>
            Modifies the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to contain all elements that are present 
            in itself, the specified collection, or both.
            </summary>
            <param name="set">this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/></param>
            <param name="other">The collection whose elements should be merged into the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> changed as a result of the call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.UnionWith(Lucene.Net.Analysis.Util.CharArraySet,System.Collections.Generic.IEnumerable{System.UInt64})">
            <summary>
            Modifies the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to contain all elements that are present 
            in itself, the specified collection, or both.
            </summary>
            <param name="set">this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/></param>
            <param name="other">The collection whose elements should be merged into the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> changed as a result of the call</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharArraySetExtensions.UnionWith(Lucene.Net.Analysis.Util.CharArraySet,System.Collections.Generic.IEnumerable{System.UInt16})">
            <summary>
            Modifies the current <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to contain all elements that are present 
            in itself, the specified collection, or both.
            </summary>
            <param name="set">this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/></param>
            <param name="other">The collection whose elements should be merged into the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/>.</param>
            <returns><c>true</c> if this <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> changed as a result of the call</returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.CharFilterFactory">
            <summary>
            Abstract parent class for analysis factories that create <see cref="T:Lucene.Net.Analysis.CharFilter"/>
            instances.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharFilterFactory.ForName(System.String,System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            looks up a charfilter by name from the host project's dependent assemblies </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharFilterFactory.LookupClass(System.String)">
            <summary>
            looks up a charfilter class by name from the host project's dependent assemblies </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.CharFilterFactory.AvailableCharFilters">
            <summary>
            returns a list of all available charfilter names </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharFilterFactory.ReloadCharFilters">
            <summary>
            Reloads the factory list.
            Changes to the factories are visible after the method ends, all
            iterators (<see cref="P:Lucene.Net.Analysis.Util.CharFilterFactory.AvailableCharFilters"/>,...) stay consistent. 
            
            <para><b>NOTE:</b> Only new factories are added, existing ones are
            never removed or replaced.
            
            </para>
            <para><em>This method is expensive and should only be called for discovery
            of new factories on the given classpath/classloader!</em>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Initialize this factory via a set of key-value pairs.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharFilterFactory.Create(System.IO.TextReader)">
            <summary>
            Wraps the given <see cref="T:System.IO.TextReader"/> with a <see cref="T:Lucene.Net.Analysis.CharFilter"/>. </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.CharTokenizer">
            <summary>
            An abstract base class for simple, character-oriented tokenizers. 
            <para>
            You must specify the required <see cref="T:Lucene.Net.Util.LuceneVersion"/> compatibility
            when creating <see cref="T:Lucene.Net.Analysis.Util.CharTokenizer"/>:
            <list type="bullet">
                <item><description>As of 3.1, <see cref="T:Lucene.Net.Analysis.Util.CharTokenizer"/> uses an int based API to normalize and
                    detect token codepoints. See <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.IsTokenChar(System.Int32)"/> and
                    <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.Normalize(System.Int32)"/> for details.</description></item>
            </list>
            </para>
            <para>
            A new <see cref="T:Lucene.Net.Analysis.Util.CharTokenizer"/> API has been introduced with Lucene 3.1. This API
            moved from UTF-16 code units to UTF-32 codepoints to eventually add support
            for <a href=
            "http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Character.html#supplementary"
            >supplementary characters</a>. The old <i>char</i> based API has been
            deprecated and should be replaced with the <i>int</i> based methods
            <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.IsTokenChar(System.Int32)"/> and <see cref="M:Lucene.Net.Analysis.Util.CharTokenizer.Normalize(System.Int32)"/>.
            </para>
            <para>
            As of Lucene 3.1 each <see cref="T:Lucene.Net.Analysis.Util.CharTokenizer"/> - constructor expects a
            <see cref="T:Lucene.Net.Util.LuceneVersion"/> argument. Based on the given <see cref="T:Lucene.Net.Util.LuceneVersion"/> either the new
            API or a backwards compatibility layer is used at runtime. For
            <see cref="T:Lucene.Net.Util.LuceneVersion"/> &lt; 3.1 the backwards compatibility layer ensures correct
            behavior even for indexes build with previous versions of Lucene. If a
            <see cref="T:Lucene.Net.Util.LuceneVersion"/> >= 3.1 is used <see cref="T:Lucene.Net.Analysis.Util.CharTokenizer"/> requires the new API to
            be implemented by the instantiated class. Yet, the old <i>char</i> based API
            is not required anymore even if backwards compatibility must be preserved.
            <see cref="T:Lucene.Net.Analysis.Util.CharTokenizer"/> subclasses implementing the new API are fully backwards
            compatible if instantiated with <see cref="T:Lucene.Net.Util.LuceneVersion"/> &lt; 3.1.
            </para>
            <para>
            <strong>Note:</strong> If you use a subclass of <see cref="T:Lucene.Net.Analysis.Util.CharTokenizer"/> with <see cref="T:Lucene.Net.Util.LuceneVersion"/> >=
            3.1 on an index build with a version &lt; 3.1, created tokens might not be
            compatible with the terms in your index.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,System.IO.TextReader)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Util.CharTokenizer"/> instance
            </summary>
            <param name="matchVersion">
                     Lucene version to match </param>
            <param name="input">
                     the input to split up into tokens </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharTokenizer.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Util.CharTokenizer"/> instance
            </summary>
            <param name="matchVersion">
                     Lucene version to match </param>
            <param name="factory">
                     the attribute factory to use for this <see cref="T:Lucene.Net.Analysis.Tokenizer"/> </param>
            <param name="input">
                     the input to split up into tokens </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharTokenizer.Init(Lucene.Net.Util.LuceneVersion)">
            <summary>
            LUCENENET specific - Added in the .NET version to assist with setting the attributes
            from multiple constructors.
            </summary>
            <param name="matchVersion"></param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharTokenizer.IsTokenChar(System.Int32)">
            <summary>
            Returns true iff a codepoint should be included in a token. This tokenizer
            generates as tokens adjacent sequences of codepoints which satisfy this
            predicate. Codepoints for which this is false are used to define token
            boundaries and are not included in tokens.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.CharTokenizer.Normalize(System.Int32)">
            <summary>
            Called on each token character to normalize it before it is added to the
            token. The default implementation does nothing. Subclasses may use this to,
            e.g., lowercase tokens.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.ClasspathResourceLoader">
            <summary>
            Simple <see cref="T:Lucene.Net.Analysis.Util.IResourceLoader"/> that uses <see cref="M:System.Reflection.Assembly.GetManifestResourceStream(System.String)"/>
            and <see cref="M:System.Reflection.Assembly.GetType(System.String)"/> to open resources and
            <see cref="T:System.Type"/>s, respectively.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.ClasspathResourceLoader.#ctor(System.Type)">
            <summary>
            Creates an instance using the System.Assembly of the given class to load Resources and classes
            Resource paths must be absolute.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.ElisionFilter">
            <summary>
            Removes elisions from a <see cref="T:Lucene.Net.Analysis.TokenStream"/>. For example, "l'avion" (the plane) will be
            tokenized as "avion" (plane).
            <para/>
            <a href="http://fr.wikipedia.org/wiki/%C3%89lision">Elision in Wikipedia</a>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.ElisionFilter.#ctor(Lucene.Net.Analysis.TokenStream,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Constructs an elision filter with a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> of stop words </summary>
            <param name="input"> the source <see cref="T:Lucene.Net.Analysis.TokenStream"/> </param>
            <param name="articles"> a set of stopword articles </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.ElisionFilter.IncrementToken">
            <summary>
            Increments the <see cref="T:Lucene.Net.Analysis.TokenStream"/> with a <see cref="T:Lucene.Net.Analysis.TokenAttributes.CharTermAttribute"/> without elisioned start
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.ElisionFilterFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Util.ElisionFilter"/>.
            <code>
            &lt;fieldType name="text_elsn" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
                &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
                &lt;filter class="solr.ElisionFilterFactory" 
                  articles="stopwordarticles.txt" ignoreCase="true"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.ElisionFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Util.ElisionFilterFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.FilesystemResourceLoader">
            <summary>
            Simple <see cref="T:Lucene.Net.Analysis.Util.IResourceLoader"/> that opens resource files
            from the local file system, optionally resolving against
            a base directory.
            
            <para>This loader wraps a delegate <see cref="T:Lucene.Net.Analysis.Util.IResourceLoader"/>
            that is used to resolve all files, the current base directory
            does not contain. <see cref="M:Lucene.Net.Analysis.Util.FilesystemResourceLoader.NewInstance``1(System.String)"/> is always resolved
            against the delegate, as an <see cref="T:System.Assembly"/> is needed.
            
            </para>
            <para>You can chain several <see cref="T:Lucene.Net.Analysis.Util.FilesystemResourceLoader"/>s
            to allow lookup of files in more than one base directory.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.FilesystemResourceLoader.#ctor">
            <summary>
            Creates a resource loader that requires absolute filenames or relative to CWD
            to resolve resources. Files not found in file system and class lookups
            are delegated to context classloader.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.FilesystemResourceLoader.#ctor(System.IO.DirectoryInfo)">
            <summary>
            Creates a resource loader that resolves resources against the given
            base directory (may be <c>null</c> to refer to CWD).
            Files not found in file system and class lookups are delegated to context
            classloader.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.FilesystemResourceLoader.#ctor(System.IO.DirectoryInfo,Lucene.Net.Analysis.Util.IResourceLoader)">
            <summary>
            Creates a resource loader that resolves resources against the given
            base directory (may be <c>null</c> to refer to CWD).
            Files not found in file system and class lookups are delegated
            to the given delegate <see cref="T:Lucene.Net.Analysis.Util.IResourceLoader"/>.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.FilteringTokenFilter">
            <summary>
            Abstract base class for TokenFilters that may remove tokens.
            You have to implement <see cref="M:Lucene.Net.Analysis.Util.FilteringTokenFilter.Accept"/> and return a boolean if the current
            token should be preserved. <see cref="M:Lucene.Net.Analysis.Util.FilteringTokenFilter.IncrementToken"/> uses this method
            to decide if a token should be passed to the caller.
            <para>
            As of Lucene 4.4, an
            <see cref="T:System.ArgumentException"/> is thrown when trying to disable position
            increments when filtering terms.
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.FilteringTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,System.Boolean,Lucene.Net.Analysis.TokenStream)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Util.FilteringTokenFilter"/>. </summary>
            <param name="version">                  the <a href="#lucene_match_version">Lucene match version</a> </param>
            <param name="enablePositionIncrements"> whether to increment position increments when filtering out terms </param>
            <param name="input">                    the input to consume </param>
            @deprecated enablePositionIncrements=false is not supported anymore as of Lucene 4.4 
        </member>
        <member name="M:Lucene.Net.Analysis.Util.FilteringTokenFilter.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.TokenStream)">
            <summary>
            Create a new <see cref="T:Lucene.Net.Analysis.Util.FilteringTokenFilter"/>. </summary>
            <param name="version"> the Lucene match version </param>
            <param name="in">      the <see cref="T:Lucene.Net.Analysis.TokenStream"/> to consume </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.FilteringTokenFilter.Accept">
            <summary>
            Override this method and return if the current input token should be returned by <see cref="M:Lucene.Net.Analysis.Util.FilteringTokenFilter.IncrementToken"/>. </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.FilteringTokenFilter.SetEnablePositionIncrements(System.Boolean)">
            <summary>
            If <c>true</c>, this <see cref="T:Lucene.Net.Analysis.TokenFilter"/> will preserve
            positions of the incoming tokens (ie, accumulate and
            set position increments of the removed tokens).
            Generally, <c>true</c> is best as it does not
            lose information (positions of the original tokens)
            during indexing.
            
            <para/> When set, when a token is stopped
            (omitted), the position increment of the following
            token is incremented.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.IMultiTermAwareComponent">
            <summary>
            Add to any analysis factory component to allow returning an
            analysis component factory for use with partial terms in prefix queries,
            wildcard queries, range query endpoints, regex queries, etc.
            
            @lucene.experimental
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.IMultiTermAwareComponent.GetMultiTermComponent">
            <summary>
            Returns an analysis component to handle analysis if multi-term queries.
            The returned component must be a <see cref="T:Lucene.Net.Analysis.Util.TokenizerFactory"/>, <see cref="T:Lucene.Net.Analysis.Util.TokenFilterFactory"/> or <see cref="T:Lucene.Net.Analysis.Util.CharFilterFactory"/>.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.OpenStringBuilder">
            <summary>
            A StringBuilder that allows one to access the array.
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.IResourceLoader">
            <summary>
            Abstraction for loading resources (streams, files, and classes).
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.IResourceLoader.OpenResource(System.String)">
            <summary>
            Opens a named resource
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.IResourceLoader.FindType(System.String)">
            <summary>
            Finds class of the name
            <para/>
            NOTE: This was findClass() in Lucene
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.IResourceLoader.NewInstance``1(System.String)">
            <summary>
            Creates an instance of the name and expected type
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.IResourceLoaderAware">
            <summary>
            Interface for a component that needs to be initialized by
            an implementation of <see cref="T:Lucene.Net.Analysis.Util.IResourceLoader"/>.
            </summary>
            <seealso cref="T:Lucene.Net.Analysis.Util.IResourceLoader"/>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.IResourceLoaderAware.Inform(Lucene.Net.Analysis.Util.IResourceLoader)">
            <summary>
            Initializes this component with the provided <see cref="T:Lucene.Net.Analysis.Util.IResourceLoader"/>
            (used for loading types, embedded resources, files, etc).
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.RollingCharBuffer">
            <summary>
            Acts like a forever growing <see cref="T:char[]"/> as you read
            characters into it from the provided reader, but
            internally it uses a circular buffer to only hold the
            characters that haven't been freed yet.  This is like a
            PushbackReader, except you don't have to specify
            up-front the max size of the buffer, but you do have to
            periodically call <see cref="M:Lucene.Net.Analysis.Util.RollingCharBuffer.FreeBefore(System.Int32)"/>. 
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.RollingCharBuffer.Reset(System.IO.TextReader)">
            <summary>
            Clear array and switch to new reader. </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.RollingCharBuffer.Get(System.Int32)">
            <summary>
            Absolute position read.  NOTE: pos must not jump
            ahead by more than 1!  Ie, it's OK to read arbitarily
            far back (just not prior to the last <see cref="M:Lucene.Net.Analysis.Util.RollingCharBuffer.FreeBefore(System.Int32)"/>, 
            but NOT ok to read arbitrarily far
            ahead.  Returns -1 if you hit EOF.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.RollingCharBuffer.FreeBefore(System.Int32)">
            <summary>
            Call this to notify us that no chars before this
            absolute position are needed anymore. 
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.StemmerUtil">
            <summary>
            Some commonly-used stemming functions
            
            @lucene.internal
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.StemmerUtil.#ctor">
            <summary>
            no instance </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.StemmerUtil.StartsWith(System.Char[],System.Int32,System.String)">
            <summary>
            Returns true if the character array starts with the prefix.
            </summary>
            <param name="s"> Input Buffer </param>
            <param name="len"> length of input buffer </param>
            <param name="prefix"> Prefix string to test </param>
            <returns> <c>true</c> if <paramref name="s"/> starts with <paramref name="prefix"/> </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.StemmerUtil.EndsWith(System.Char[],System.Int32,System.String)">
            <summary>
            Returns true if the character array ends with the suffix.
            </summary>
            <param name="s"> Input Buffer </param>
            <param name="len"> length of input buffer </param>
            <param name="suffix"> Suffix string to test </param>
            <returns> <c>true</c> if <paramref name="s"/> ends with <paramref name="suffix"/> </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.StemmerUtil.EndsWith(System.Char[],System.Int32,System.Char[])">
            <summary>
            Returns true if the character array ends with the suffix.
            </summary>
            <param name="s"> Input Buffer </param>
            <param name="len"> length of input buffer </param>
            <param name="suffix"> Suffix string to test </param>
            <returns> <c>true</c> if <paramref name="s"/> ends with <paramref name="suffix"/> </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.StemmerUtil.Delete(System.Char[],System.Int32,System.Int32)">
            <summary>
            Delete a character in-place
            </summary>
            <param name="s"> Input Buffer </param>
            <param name="pos"> Position of character to delete </param>
            <param name="len"> length of input buffer </param>
            <returns> length of input buffer after deletion </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.StemmerUtil.DeleteN(System.Char[],System.Int32,System.Int32,System.Int32)">
            <summary>
            Delete n characters in-place
            </summary>
            <param name="s"> Input Buffer </param>
            <param name="pos"> Position of character to delete </param>
            <param name="len"> Length of input buffer </param>
            <param name="nChars"> number of characters to delete </param>
            <returns> length of input buffer after deletion </returns>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.StopwordAnalyzerBase">
            <summary>
            Base class for <see cref="T:Lucene.Net.Analysis.Analyzer"/>s that need to make use of stopword sets. 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Util.StopwordAnalyzerBase.m_stopwords">
            <summary>
            An immutable stopword set
            </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.StopwordAnalyzerBase.StopwordSet">
            <summary>
            Returns the analyzer's stopword set or an empty set if the analyzer has no
            stopwords
            </summary>
            <returns> the analyzer's stopword set or an empty set if the analyzer has no
                    stopwords </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.StopwordAnalyzerBase.#ctor(Lucene.Net.Util.LuceneVersion,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Creates a new instance initialized with the given stopword set
            </summary>
            <param name="version">
                     the Lucene version for cross version compatibility </param>
            <param name="stopwords">
                     the analyzer's stopword set </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.StopwordAnalyzerBase.#ctor(Lucene.Net.Util.LuceneVersion)">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Analyzer"/> with an empty stopword set
            </summary>
            <param name="version">
                     the Lucene version for cross version compatibility </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.StopwordAnalyzerBase.LoadStopwordSet(System.Boolean,System.Type,System.String,System.String)">
            <summary>
            Creates a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> from an embedded resource associated with a class. (See
            <see cref="M:System.Reflection.Assembly.GetManifestResourceStream(System.String)"/>).
            </summary>
            <param name="ignoreCase">
                     <c>true</c> if the set should ignore the case of the
                     stopwords, otherwise <c>false</c> </param>
            <param name="aClass">
                     a class that is associated with the given stopwordResource </param>
            <param name="resource">
                     name of the resource file associated with the given class </param>
            <param name="comment">
                     comment string to ignore in the stopword file </param>
            <returns> a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> containing the distinct stopwords from the given
                    file </returns>
            <exception cref="T:System.IO.IOException">
                      if loading the stopwords throws an <see cref="T:System.IO.IOException"/> </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.StopwordAnalyzerBase.LoadStopwordSet(System.IO.FileInfo,Lucene.Net.Util.LuceneVersion)">
            <summary>
            Creates a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> from a file.
            </summary>
            <param name="stopwords">
                     the stopwords file to load
            </param>
            <param name="matchVersion">
                     the Lucene version for cross version compatibility </param>
            <returns> a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> containing the distinct stopwords from the given
                    file </returns>
            <exception cref="T:System.IO.IOException">
                      if loading the stopwords throws an <see cref="T:System.IO.IOException"/> </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.StopwordAnalyzerBase.LoadStopwordSet(System.IO.TextReader,Lucene.Net.Util.LuceneVersion)">
            <summary>
            Creates a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> from a file.
            </summary>
            <param name="stopwords">
                     the stopwords reader to load
            </param>
            <param name="matchVersion">
                     the Lucene version for cross version compatibility </param>
            <returns> a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> containing the distinct stopwords from the given
                    reader </returns>
            <exception cref="T:System.IO.IOException">
                      if loading the stopwords throws an <see cref="T:System.IO.IOException"/> </exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.TokenFilterFactory">
            <summary>
            Abstract parent class for analysis factories that create <see cref="T:Lucene.Net.Analysis.TokenFilter"/>
            instances.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.TokenFilterFactory.ForName(System.String,System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            looks up a tokenfilter by name from the host project's referenced assemblies </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.TokenFilterFactory.LookupClass(System.String)">
            <summary>
            looks up a tokenfilter class by name from  the host project's referenced assemblies </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.TokenFilterFactory.AvailableTokenFilters">
            <summary>
            returns a list of all available tokenfilter names from the host project's referenced assemblies </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.TokenFilterFactory.ReloadTokenFilters">
            <summary>
            Reloads the factory list.
            Changes to the factories are visible after the method ends, all
            iterators (<see cref="P:Lucene.Net.Analysis.Util.TokenFilterFactory.AvailableTokenFilters"/>,...) stay consistent. 
            
            <para><b>NOTE:</b> Only new factories are added, existing ones are
            never removed or replaced.
            
            </para>
            <para><em>This method is expensive and should only be called for discovery
            of new factories on the given classpath/classloader!</em>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.TokenFilterFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Initialize this factory via a set of key-value pairs.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.TokenFilterFactory.Create(Lucene.Net.Analysis.TokenStream)">
            <summary>
            Transform the specified input <see cref="T:Lucene.Net.Analysis.TokenStream"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.TokenizerFactory">
            <summary>
            Abstract parent class for analysis factories that create <see cref="T:Lucene.Net.Analysis.Tokenizer"/>
            instances.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.TokenizerFactory.ForName(System.String,System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            looks up a tokenizer by name from the host project's referenced assemblies </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.TokenizerFactory.LookupClass(System.String)">
            <summary>
            looks up a tokenizer class by name from the host project's referenced assemblies </summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Util.TokenizerFactory.AvailableTokenizers">
            <summary>
            returns a list of all available tokenizer names from the host project's referenced assemblies </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.TokenizerFactory.ReloadTokenizers">
            <summary>
            Reloads the factory list.
            Changes to the factories are visible after the method ends, all
            iterators (<see cref="P:Lucene.Net.Analysis.Util.TokenizerFactory.AvailableTokenizers"/>,...) stay consistent. 
            
            <para><b>NOTE:</b> Only new factories are added, existing ones are
            never removed or replaced.
            
            </para>
            <para><em>This method is expensive and should only be called for discovery
            of new factories on the given classpath/classloader!</em>
            </para>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.TokenizerFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Initialize this factory via a set of key-value pairs.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.TokenizerFactory.Create(System.IO.TextReader)">
            <summary>
            Creates a <see cref="T:Lucene.Net.Analysis.TokenStream"/> of the specified input using the default attribute factory. </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.TokenizerFactory.Create(Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader)">
            <summary>
            Creates a <see cref="T:Lucene.Net.Analysis.TokenStream"/> of the specified input using the given <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Util.WordlistLoader">
            <summary>
            Loader for text files that represent a list of stopwords.
            <para/>
            <see cref="T:Lucene.Net.Util.IOUtils"/> to obtain <see cref="T:System.IO.TextReader"/> instances.
            @lucene.internal
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.WordlistLoader.#ctor">
            <summary>
            no instance </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.WordlistLoader.GetWordSet(System.IO.TextReader,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Reads lines from a <see cref="T:System.IO.TextReader"/> and adds every line as an entry to a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> (omitting
            leading and trailing whitespace). Every line of the <see cref="T:System.IO.TextReader"/> should contain only
            one word. The words need to be in lowercase if you make use of an
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> which uses <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/> (like <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/>).
            </summary>
            <param name="reader"> <see cref="T:System.IO.TextReader"/> containing the wordlist </param>
            <param name="result"> the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to fill with the readers words </param>
            <returns> the given <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> with the reader's words </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.WordlistLoader.GetWordSet(System.IO.TextReader,Lucene.Net.Util.LuceneVersion)">
            <summary>
            Reads lines from a <see cref="T:System.IO.TextReader"/> and adds every line as an entry to a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> (omitting
            leading and trailing whitespace). Every line of the <see cref="T:System.IO.TextReader"/> should contain only
            one word. The words need to be in lowercase if you make use of an
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> which uses <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/> (like <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/>).
            </summary>
            <param name="reader"> <see cref="T:System.IO.TextReader"/> containing the wordlist </param>
            <param name="matchVersion"> the <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <returns> A <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> with the reader's words </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.WordlistLoader.GetWordSet(System.IO.TextReader,System.String,Lucene.Net.Util.LuceneVersion)">
            <summary>
            Reads lines from a <see cref="T:System.IO.TextReader"/> and adds every non-comment line as an entry to a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> (omitting
            leading and trailing whitespace). Every line of the <see cref="T:System.IO.TextReader"/> should contain only
            one word. The words need to be in lowercase if you make use of an
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> which uses <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/> (like <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/>).
            </summary>
            <param name="reader"> <see cref="T:System.IO.TextReader"/> containing the wordlist </param>
            <param name="comment"> The string representing a comment. </param>
            <param name="matchVersion"> the <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <returns> A CharArraySet with the reader's words </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.WordlistLoader.GetWordSet(System.IO.TextReader,System.String,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Reads lines from a <see cref="T:System.IO.TextReader"/> and adds every non-comment line as an entry to a <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> (omitting
            leading and trailing whitespace). Every line of the <see cref="T:System.IO.TextReader"/> should contain only
            one word. The words need to be in lowercase if you make use of an
            <see cref="T:Lucene.Net.Analysis.Analyzer"/> which uses <see cref="T:Lucene.Net.Analysis.Core.LowerCaseFilter"/> (like <see cref="T:Lucene.Net.Analysis.Standard.StandardAnalyzer"/>).
            </summary>
            <param name="reader"> <see cref="T:System.IO.TextReader"/> containing the wordlist </param>
            <param name="comment"> The string representing a comment. </param>
            <param name="result"> the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to fill with the readers words </param>
            <returns> the given <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> with the reader's words </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.WordlistLoader.GetSnowballWordSet(System.IO.TextReader,Lucene.Net.Analysis.Util.CharArraySet)">
            <summary>
            Reads stopwords from a stopword list in Snowball format.
            <para>
            The snowball format is the following:
            <list type="bullet">
                <item><description>Lines may contain multiple words separated by whitespace.</description></item>
                <item><description>The comment character is the vertical line (&#124;).</description></item>
                <item><description>Lines may contain trailing comments.</description></item>
            </list>
            </para>
            </summary>
            <param name="reader"> <see cref="T:System.IO.TextReader"/> containing a Snowball stopword list </param>
            <param name="result"> the <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> to fill with the readers words </param>
            <returns> the given <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> with the reader's words </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.WordlistLoader.GetSnowballWordSet(System.IO.TextReader,Lucene.Net.Util.LuceneVersion)">
            <summary>
            Reads stopwords from a stopword list in Snowball format.
            <para>
            The snowball format is the following:
            <list type="bullet">
                <item><description>Lines may contain multiple words separated by whitespace.</description></item>
                <item><description>The comment character is the vertical line (&#124;).</description></item>
                <item><description>Lines may contain trailing comments.</description></item>
            </list>
            </para>
            </summary>
            <param name="reader"> <see cref="T:System.IO.TextReader"/> containing a Snowball stopword list </param>
            <param name="matchVersion"> the Lucene <see cref="T:Lucene.Net.Util.LuceneVersion"/> </param>
            <returns> A <see cref="T:Lucene.Net.Analysis.Util.CharArraySet"/> with the reader's words </returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.WordlistLoader.GetStemDict(System.IO.TextReader,Lucene.Net.Analysis.Util.CharArrayMap{System.String})">
            <summary>
            Reads a stem dictionary. Each line contains:
            <code>word<b>\t</b>stem</code>
            (i.e. two tab separated words)
            </summary>
            <returns> stem dictionary that overrules the stemming algorithm </returns>
            <exception cref="T:System.IO.IOException"> If there is a low-level I/O error. </exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Util.WordlistLoader.GetLines(System.IO.Stream,System.Text.Encoding)">
            <summary>
            Accesses a resource by name and returns the (non comment) lines containing
            data using the given character encoding.
            <para>
            A comment line is any line that starts with the character "#"
            </para>
            </summary>
            <returns> a list of non-blank non-comment lines with whitespace trimmed </returns>
            <exception cref="T:System.IO.IOException"> If there is a low-level I/O error. </exception>
        </member>
        <member name="T:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer">
            <summary>
            Extension of <see cref="T:Lucene.Net.Analysis.Standard.StandardTokenizer"/> that is aware of Wikipedia syntax.  It is based off of the
            Wikipedia tutorial available at http://en.wikipedia.org/wiki/Wikipedia:Tutorial, but it may not be complete.
            <para/>
            @lucene.experimental
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.TOKEN_TYPES">
            <summary>
            String token types that correspond to token type int constants </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.TOKENS_ONLY">
            <summary>
            Only output tokens
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.UNTOKENIZED_ONLY">
            <summary>
            Only output untokenized tokens, which are tokens that would normally be split into several tokens
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.BOTH">
            <summary>
            Output the both the untokenized token and the splits
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG">
            <summary>
            This flag is used to indicate that the produced "Token" would, if <see cref="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.TOKENS_ONLY"/> was used, produce multiple tokens.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.scanner">
            <summary>
            A private instance of the JFlex-constructed scanner
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.#ctor(System.IO.TextReader)">
            <summary>
            Creates a new instance of the <see cref="T:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer"/>. Attaches the
            <paramref name="input"/> to a newly created JFlex scanner.
            </summary>
            <param name="input"> The Input <see cref="T:System.IO.TextReader"/> </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.#ctor(System.IO.TextReader,System.Int32,System.Collections.Generic.ICollection{System.String})">
            <summary>
            Creates a new instance of the <see cref="T:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer"/>.  Attaches the
            <paramref name="input"/> to a the newly created JFlex scanner.
            </summary>
            <param name="input"> The input </param>
            <param name="tokenOutput"> One of <see cref="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.TOKENS_ONLY"/>, <see cref="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.UNTOKENIZED_ONLY"/>, <see cref="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.BOTH"/> </param>
            <param name="untokenizedTypes"> Untokenized types </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.#ctor(Lucene.Net.Util.AttributeSource.AttributeFactory,System.IO.TextReader,System.Int32,System.Collections.Generic.ICollection{System.String})">
            <summary>
            Creates a new instance of the <see cref="T:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer"/>.  Attaches the
            <paramref name="input"/> to a the newly created JFlex scanner. Uses the given <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/>.
            </summary>
            <param name="factory"> The <see cref="T:Lucene.Net.Util.AttributeSource.AttributeFactory"/> </param>
            <param name="input"> The input </param>
            <param name="tokenOutput"> One of <see cref="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.TOKENS_ONLY"/>, <see cref="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.UNTOKENIZED_ONLY"/>, <see cref="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.BOTH"/> </param>
            <param name="untokenizedTypes"> Untokenized types </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.IncrementToken">
            <summary>
            <see cref="M:Lucene.Net.Analysis.TokenStream.IncrementToken"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer.Reset">
            <summary>
            <see cref="M:Lucene.Net.Analysis.TokenStream.Reset"/>
            </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerFactory">
            <summary>
            Factory for <see cref="T:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizer"/>.
            <code>
            &lt;fieldType name="text_wiki" class="solr.TextField" positionIncrementGap="100"&gt;
              &lt;analyzer&gt;
                &lt;tokenizer class="solr.WikipediaTokenizerFactory"/&gt;
              &lt;/analyzer&gt;
            &lt;/fieldType&gt;</code>
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerFactory.#ctor(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Creates a new <see cref="T:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerFactory"/> </summary>
        </member>
        <member name="T:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl">
            <summary>
            JFlex-generated tokenizer that is aware of Wikipedia syntax.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.YYEOF">
            <summary>This character denotes the end of file</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.ZZ_BUFFERSIZE">
            <summary>initial size of the lookahead buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.YYINITIAL">
            <summary>lexical states</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.ZZ_LEXSTATE">
            <summary>
            ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
            ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
                at the beginning of a line
            l is of the form l = 2*k, k a non negative integer
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.ZZ_CMAP_PACKED">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.ZZ_CMAP">
            <summary>
            Translates characters to character classes
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.ZZ_ACTION">
            <summary>
            Translates DFA states to action switch labels.
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.ZZ_ROWMAP">
            <summary>
            Translates a state to a row index in the transition table
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.ZZ_TRANS">
            <summary>
            The transition table of the DFA
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.ZZ_ATTRIBUTE">
            <summary>
            ZZ_ATTRIBUTE[aState] contains the attributes of state <c>aState</c>
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.zzReader">
            <summary>the input device</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.zzState">
            <summary>the current state of the DFA</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.zzLexicalState">
            <summary>the current lexical state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.zzBuffer">
            <summary>
            this buffer contains the current text to be matched and is
            the source of the YyText string 
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.zzMarkedPos">
            <summary>the textposition at the last accepting state</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.zzCurrentPos">
            <summary>the current text position in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.zzStartRead">
            <summary>startRead marks the beginning of the YyText string in the buffer</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.zzEndRead">
            <summary>
            endRead marks the last character in the buffer, that has been read
            from input
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.yyline">
            <summary>number of newlines encountered up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.yychar">
            <summary>the number of characters up to the start of the matched text</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.yycolumn">
            <summary>
            the number of characters from the last newline up to the start of the
            matched text
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.zzAtBOL">
            <summary>
            zzAtBOL == true &lt;=&gt; the scanner is currently at the beginning of a line
            </summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.zzAtEOF">
            <summary>zzAtEOF == true &lt;=&gt; the scanner is at the EOF</summary>
        </member>
        <member name="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.zzEOFDone">
            <summary>denotes if the user-EOF-code has already been executed</summary>
        </member>
        <member name="P:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.NumWikiTokensSeen">
            <summary>
            Returns the number of tokens seen inside a category or link, etc.
            </summary>
            <returns>the number of tokens seen inside the context of wiki syntax.</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.GetText(Lucene.Net.Analysis.TokenAttributes.ICharTermAttribute)">
            <summary>
            Fills Lucene token with the current token text.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.#ctor(System.IO.TextReader)">
            <summary>
            Creates a new scanner
            </summary>
            <param name="in">the TextReader to read input from.</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.ZzUnpackCMap(System.String)">
            <summary>
            Unpacks the compressed character translation table.
            </summary>
            <param name="packed">the packed character translation table</param>
            <returns>the unpacked character translation table</returns>
        </member>
        <member name="M:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.ZzRefill">
            <summary>
            Refills the input buffer.
            </summary>
            <returns><c>false</c>, iff there was new input.</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="M:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.YyClose">
            <summary>
            Disposes the input stream.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.YyReset(System.IO.TextReader)">
            <summary>
            Resets the scanner to read from a new input stream.
            Does not close the old reader.
            <para/>
            All internal variables are reset, the old input stream 
            <b>cannot</b> be reused (internal buffer is discarded and lost).
            Lexical state is set to <see cref="F:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.YYINITIAL"/>.
            <para/>
            Internal scan buffer is resized down to its initial length, if it has grown.
            </summary>
            <param name="reader">the new input stream </param>
        </member>
        <member name="P:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.YyState">
            <summary>
            Returns the current lexical state.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.YyBegin(System.Int32)">
            <summary>
            Enters a new lexical state
            </summary>
            <param name="newState">the new lexical state</param>
        </member>
        <member name="P:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.YyText">
            <summary>
            Returns the text matched by the current regular expression.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.YyCharAt(System.Int32)">
            <summary>
            Returns the character at position <paramref name="pos"/> from the 
            matched text.
            <para/>
            It is equivalent to YyText[pos], but faster
            </summary>
            <param name="pos">
            the position of the character to fetch. 
            A value from 0 to YyLength-1.
            </param>
            <returns>the character at position pos</returns>
        </member>
        <member name="P:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.YyLength">
            <summary>
            Returns the length of the matched text region.
            </summary>
        </member>
        <member name="M:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.ZzScanError(System.Int32)">
            <summary>
            Reports an error that occured while scanning.
            <para/>
            In a wellformed scanner (no or only correct usage of 
            YyPushBack(int) and a match-all fallback rule) this method 
            will only be called with things that "Can't Possibly Happen".
            If this method is called, something is seriously wrong
            (e.g. a JFlex bug producing a faulty scanner etc.).
            <para/>
            Usual syntax/scanner level error handling should be done
            in error fallback rules.
            </summary>
            <param name="errorCode">the code of the errormessage to display</param>
        </member>
        <member name="M:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.YyPushBack(System.Int32)">
            <summary>
            Pushes the specified amount of characters back into the input stream.
            <para/>
            They will be read again by then next call of the scanning method
            </summary>
            <param name="number">
            the number of characters to be read again.
            This number must not be greater than YyLength!
            </param>
        </member>
        <member name="M:Lucene.Net.Analysis.Wikipedia.WikipediaTokenizerImpl.GetNextToken">
            <summary>
            Resumes scanning until the next regular expression is matched,
            the end of input is encountered or an I/O-Error occurs.
            </summary>
            <returns>the next token</returns>
            <exception cref="T:System.IO.IOException">if any I/O-Error occurs</exception>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Among">
            <summary>
            This is the rev 502 of the Snowball SVN trunk,
            but modified:
            made abstract and introduced abstract method stem to avoid expensive reflection in filter class.
            refactored StringBuffers to StringBuilder
            uses char[] as buffer instead of StringBuffer/StringBuilder
            eq_s,eq_s_b,insert,replace_s take CharSequence like eq_v and eq_v_b
            reflection calls (Lovins, etc) use EMPTY_ARGS/EMPTY_PARAMS
            </summary>
        </member>
        <member name="P:Lucene.Net.Tartarus.Snowball.Among.Length">
            <summary>search string</summary>
        </member>
        <member name="P:Lucene.Net.Tartarus.Snowball.Among.S">
            <summary>search string</summary>
        </member>
        <member name="P:Lucene.Net.Tartarus.Snowball.Among.SubstringIndex">
            <summary>index to longest matching substring</summary>
        </member>
        <member name="P:Lucene.Net.Tartarus.Snowball.Among.Result">
            <summary>result of the lookup</summary>
        </member>
        <member name="P:Lucene.Net.Tartarus.Snowball.Among.Method">
            <summary>method to use if substring matches</summary>
        </member>
        <member name="P:Lucene.Net.Tartarus.Snowball.Among.MethodObject">
            <summary>object to invoke method on</summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.ArmenianStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.BasqueStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.CatalanStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.DanishStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.DutchStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.EnglishStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.FinnishStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.FrenchStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.German2Stemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.GermanStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.HungarianStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.IrishStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.ItalianStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.KpStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.LovinsStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.NorwegianStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.PorterStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.PortugueseStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.RomanianStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.RussianStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.SpanishStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.SwedishStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.Ext.TurkishStemmer">
            <summary>
            This class was automatically generated by a Snowball to Java compiler
            It implements the stemming algorithm defined by a snowball script.
            </summary>
        </member>
        <member name="T:Lucene.Net.Tartarus.Snowball.SnowballProgram">
            <summary>
            This is the rev 502 of the Snowball SVN trunk,
            but modified:
            made abstract and introduced abstract method stem to avoid expensive reflection in filter class.
            refactored StringBuffers to StringBuilder
            uses char[] as buffer instead of StringBuffer/StringBuilder
            eq_s,eq_s_b,insert,replace_s take CharSequence like eq_v and eq_v_b
            reflection calls (Lovins, etc) use EMPTY_ARGS/EMPTY_PARAMS
            </summary>
        </member>
        <member name="M:Lucene.Net.Tartarus.Snowball.SnowballProgram.SetCurrent(System.String)">
            <summary>
            Set the current string.
            </summary>
        </member>
        <member name="P:Lucene.Net.Tartarus.Snowball.SnowballProgram.Current">
            <summary>
            Get the current string.
            </summary>
        </member>
        <member name="M:Lucene.Net.Tartarus.Snowball.SnowballProgram.SetCurrent(System.Char[],System.Int32)">
            <summary>
            Set the current string.
            </summary>
            <param name="text">character array containing input</param>
            <param name="length">valid length of text.</param>
        </member>
        <member name="P:Lucene.Net.Tartarus.Snowball.SnowballProgram.CurrentBuffer">
            <summary>
            Get the current buffer containing the stem.
            <para/>
            NOTE: this may be a reference to a different character array than the
            one originally provided with setCurrent, in the exceptional case that 
            stemming produced a longer intermediate or result string.
            <para/>
            It is necessary to use <see cref="P:Lucene.Net.Tartarus.Snowball.SnowballProgram.CurrentBufferLength"/> to determine
            the valid length of the returned buffer. For example, many words are
            stemmed simply by subtracting from the length to remove suffixes.
            </summary>
            <seealso cref="P:Lucene.Net.Tartarus.Snowball.SnowballProgram.CurrentBufferLength"/>
        </member>
        <member name="P:Lucene.Net.Tartarus.Snowball.SnowballProgram.CurrentBufferLength">
            <summary>
            Get the valid length of the character array in <seealso cref="P:Lucene.Net.Tartarus.Snowball.SnowballProgram.CurrentBuffer"/>
            </summary>
        </member>
        <member name="M:Lucene.Net.Tartarus.Snowball.SnowballProgram.ReplaceS(System.Int32,System.Int32,System.String)">
            <summary>
            to replace chars between <paramref name="c_bra"/> and <paramref name="c_ket"/> in current by the
            chars in <paramref name="s"/>.
            </summary>
        </member>
        <member name="M:Lucene.Net.Tartarus.Snowball.SnowballProgram.SliceTo(System.Text.StringBuilder)">
            <summary>
            Copy the slice into the supplied <see cref="T:System.Text.StringBuilder"/>
            </summary>
        </member>
    </members>
</doc>
